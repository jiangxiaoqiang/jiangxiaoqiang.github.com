<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[sonar安装]]></title>
      <url>%2F2017%2F01%2F10%2Fsonar-using%2F</url>
      <content type="text"><![CDATA[配置数据库登录MySQL数据库： 1mysql -u root -p 创建sonar数据库： 1create database sonar default charset=utf8; 配置 sonar 用户： 1234create user 'sonar' identified by 'sonar123'; grant all on sonar.* to 'sonar'@'%' identified by 'sonar123';grant all on sonar.* to 'sonar'@'localhost' identified by 'sonar123'; flush privileges; 配置sonar编辑 ${SONAR_HOME}/conf/sonar.properties： 12345678910111213# User credentials.# Permissions to create tables, indices and triggers must be granted to JDBC user.# The schema must be created first.sonar.jdbc.username=sonarsonar.jdbc.password=sonar123#----- Embedded Database (default)# H2 embedded database server listening port, defaults to 9092#sonar.embeddedDatabase.port=9092#----- MySQL 5.6 or greater# Only InnoDB storage engine is supported (not myISAM).# Only the bundled driver is supported. It can not be changed.sonar.jdbc.url=jdbc:mysql://localhost:3306/sonar?useUnicode=true&amp;characterEncoding=utf8&amp;rewriteBatchedStatements=true&amp;useConfigs=maxPerformance 配置完毕后，启动sonar： 1./sonar.sh start &amp; 启动成功后，访问地址：http://localhost:9000即可看到sonarqube的主界面。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[npm使用]]></title>
      <url>%2F2017%2F01%2F10%2Fnpm-using%2F</url>
      <content type="text"><![CDATA[基础npm 允许在package.json文件里面，使用scripts字段定义脚本命令。例如可以在Package.js里面定义如下脚本： 123456789101112131415161718&#123; "name": "xiaoqiang", "version": "0.0.1", "private": true, "devDependencies": &#123; "babel-core": "6.18.2" &#125;, "dependencies": &#123; "antd": "2.6.0" &#125;, "scripts": &#123; "site-dev": "./node_modules/gulp/bin/gulp.js", "site-watch": "./node_modules/gulp/bin/gulp.js watch", "dev": "webpack --watch --progress --colors --display-error-details --config webpack/dev.config.js", "build": "webpack --verbose --display-error-details --config webpack/dev.config.js &amp;&amp; ./node_modules/gulp/bin/gulp.js", "prod": "webpack --progress --display-error-details --config webpack/prod.config.js &amp;&amp; ./node_modules/gulp/bin/gulp.js" &#125;&#125; 运行脚本： 123456# 安装依赖，如果我们的项目依赖了很多package，一个一个地安装那将是个体力活。# 可以将项目依赖的包都在package.json这个文件里声明，然后一行命令搞定npm install# 运行脚本里的site-dev,相当于执行npm gulp.jsnpm run site-devnpm run dev gulp是一个基于流的构建系统(The streaming build system)，暂时还不理解到底是个什么鬼东西。Webpack是一款用户打包前端模块的工具。主要是用来打包在浏览器端使用的javascript的。同时也能转换、捆绑、打包其他的静态资源，包括css、image、font file、template等。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ID设计]]></title>
      <url>%2F2017%2F01%2F10%2Fid-design%2F</url>
      <content type="text"><![CDATA[UUID(GUID) 全球唯一 存储大 索引慢 不美观 ObjectIdObjectId是MongoDB的一种ID生成机制。 存储大 索引慢 不美观 特定集群内唯一 可以逆向推算出数据对应的(插入时间，插入的机器)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Nginx使用]]></title>
      <url>%2F2017%2F01%2F09%2Fnginx-using%2F</url>
      <content type="text"><![CDATA[安装在Ubuntu下，输入如下命令安装： 1sudo apt install nginx -y 安装完毕后配置文件在/etc/nginx/config.d。 常用命令12# 重启Nginxsudo /etc/init.d/nginx restart location每个 url 请求都会对应的一个服务，nginx 进行处理转发或者是本地的一个文件路径，或者是其他服务器的一个服务路径。而这个路径的匹配是通过 location 来进行的。我们可以将 server 当做对应一个域名进行的配置，而 location 是在一个域名下对更精细的路径进行配置。 1234location /api&#123; proxy_pass http://dn5:28080; proxy_redirect off;&#125; 将前缀为api的请求发送到地址：http://dn5:28080.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux Shell脚本调试]]></title>
      <url>%2F2017%2F01%2F08%2Flinux-shell-debug%2F</url>
      <content type="text"><![CDATA[在Ubuntu 14.04下安装Gradle之后,提示如下错误: 123456# gradleERROR: JAVA_HOME is set to an invalid directory: /usr/lib/jvm/default-javaPlease set the JAVA_HOME variable in your environment to match thelocation of your Java installation. 打开Gradle脚本: 1vim /usr/bin/gradle 修改第70行为: 1export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_101 再次运行Gradle命令,问题已经修复.在解决的过程中,可以是用命令调试Gradle脚本: 1bash -x /usr/bin/gradle 可以看到脚本运行过程中输出的变量. 参考资料: Gradle finds wrong JAVA_HOME even though it’s correctly set 如何调试BASH脚本]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用命令行编译运行Java]]></title>
      <url>%2F2017%2F01%2F08%2Fjava-command-compile%2F</url>
      <content type="text"><![CDATA[在旧的电脑上打开IDE还是比较慢的，有时只需要运行一段很小的代码片段(Snippet),没有必要搬出IDE,所以就直接使用命令来编译,逼格高,速度快. 直接新建一个Java文件,名字叫xiaoqiang.java,输入简单的内容: 12345public class xiaoqiang&#123; public static void main(String args[])&#123; System.out.println("My name is xiaoqiang."); &#125; &#125; 保存后,输入如下命令编译: 12# Java编译器(Compiler)javac xiaoqiang.java 编译完毕后,会生成一个xiaoqiang.class文件.输入如下命令运行程序: 1java xiaoqiang 输出为:My name is xiaoqiang.以后就可以直接写代码片段了,查看效果就可以简单的用命令搞定即可.研究一些代码片段非常方便.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[SQL注入(SQL Injection)]]></title>
      <url>%2F2017%2F01%2F06%2Fsql-injection%2F</url>
      <content type="text"><![CDATA[MyBatis预防SQL注入在MyBatis中，”${xxx}”这样格式的参数会直接参与sql编译，从而不能避免注入攻击。但涉及到动态表名和列名时，只能使用“${xxx}”这样的参数格式，所以，这样的参数需要我们在代码中手工进行处理来防止注入。所以尽量用#{}这种方式传参数，如果用到了${}方式要手动过滤sql注入。 1str.replaceAll(".*([';]+|(--)+).*","")]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[aria2使用]]></title>
      <url>%2F2017%2F01%2F05%2Faria2-using%2F</url>
      <content type="text"><![CDATA[忽然发现uGet在Mac下又玩不转了，还是命令行包打天下啊，Mac支持aria2下载，目前看来在各种平台上都能转起来的就aria了。 aria2最简单的下载,aria2后面跟上链接的地址即可: 1aria2c http://ftp.jaist.ac.jp/pub/Linux/Fedora/releases/25/Workstation/x86_64/iso/Fedora-Workstation-Live-x86_64-25-1.3.iso 会将下载的文件保存到当前目录下。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Graphviz使用]]></title>
      <url>%2F2017%2F01%2F05%2Fgraphviz-using%2F</url>
      <content type="text"><![CDATA[安装1234# Fedora下安装dnf install -y graphviz#Ubuntu下安装sudo apt install -y graphviz 生成文件： 1dot -Tjpg -Gdpi=1024 maven-lifecycle.dot -o maven-lifecycle.jpg]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java rest总结]]></title>
      <url>%2F2017%2F01%2F04%2Fjava-rest-summarize%2F</url>
      <content type="text"><![CDATA[以下是开发Rest接口时遇到的一些问题。 HTTP信息转换@ResponseBody 注解能够对客户端发过来的对象进行自动转换。 12345678@RequestMapping(value = "savejson",method = RequestMethod.POST,headers = "content-Type=application/json")@ApiOperation(value = "必须且只能传一个参数")public void saveXzss(@RequestBody String summaryXzssJson) throws IOException &#123; ObjectMapper mapper = new ObjectMapper(); SummaryXzss summaryXzss = mapper.readValue(summaryXzssJson, new TypeReference&lt;SummaryXzss&gt;() &#123; &#125;); xzssService.saveXzss(summaryXzss);&#125; 实现自动转换，必须满足以下两个条件： 请求的 Content-Type 头信息必须是 application/json； Jackson Json 库必须包含在应用程序的类路径下。 在接收客户端HTTP请求的消息时，如果客户端将请求的Json放到消息体中，那么服务端可以直接取实体，或者取相应的Json进行反序列化。2种方式都能够达到要求，一般是直接取实体，省去了反序列化的步骤，由框架进行反序列化。 发送Post请求发送Post请求时，需要指定Content-Type和Accept请求头。 12345678curl -H "APPID:hlb11529c136998cb6" -H "TIMESTAMP:2016-12-19 16:58:02" -H "ECHOSTR:sdsaasf" -H "TOKEN:14d45648c62a746ae9dd9b90c03c50893061222d" -H "Accept:application/json" -H "Accept:application/json" -H "Content-Type:application/json" -X POST -d '&#123;"id":1&#125;' http://localhost:28080/api/xzss/savejson 否则会提示415错误： 12345678&#123; &quot;timestamp&quot;: &quot;2017-01-04T03:28:26.404+0000&quot;, &quot;status&quot;: 415, &quot;error&quot;: &quot;Unsupported Media Type&quot;, &quot;exception&quot;: &quot;org.springframework.web.HttpMediaTypeNotSupportedException&quot;, &quot;message&quot;: &quot;Content type &apos;application/x-www-form-urlencoded&apos; not supported&quot;, &quot;path&quot;: &quot;/api/xzss/savejson&quot;&#125; The Content-Type header is used by @RequestBody to determine what format the data being sent from the client in the request is. The accept header is used by @ResponseBody to determine what format to sent the data back to the client in the response. That’s why you need both headers.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python入门]]></title>
      <url>%2F2017%2F01%2F01%2Fpython-introduce%2F</url>
      <content type="text"><![CDATA[目录树结构清晰的项目需要有清晰的目录结构。 —-pydolphin |—–dolphin |—–lib |—–doc |—–scripts |—–tests]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[你需要一台Mac]]></title>
      <url>%2F2017%2F01%2F01%2Fyou-need-a-mac%2F</url>
      <content type="text"><![CDATA[年初，给自己换了一台MacBook Air。一大早即去专卖店，从进店到提走电脑，时间不到5分钟，一千多美金就花出去了。心里暗自嘀咕，要是挣钱有这么快就好了。选购的是最低配版的，配置如下： 13.3inch屏幕 8GB内存 Intel Core i5 128GB SSD 屏幕分辨率1440*900(本人觉得不够细腻，选购时对分辨率要求可以再高些) 工作人员说此款电脑是几年前的，性价比不高，128的SSD不够用。不过即使最低配置，应付一般工作是绝对足够的了，一没有玩游戏的嗜好，也不会用它存储电影，而且一般情况下存储容量大多数都是闲置的。还有它的待机时间也是让人非常惊叹的，理论上可以达到10个小时。这样即使忘记带电源线也可以处理不少工作，应付一整天。 由于它的轻便，在咖啡馆或者途中也可处理手头的工作。如果是女生的话，推荐高配的11.6inch的Mac，便携性更优秀，出差或者旅行即可随身携带。如果不是长时间使用电脑，小屏比大屏更加合适。 迫不及待用了一个下午，因为有使用*NIX系列系统的经验，除了注册Apple ID花了一些时间之外，其他还算问题不大。Mac最直接的感受就是：快，仿佛又回到了Windows XP的时代。电脑可以跟上自己的速度，不用再苦苦等待了。使用Mac和Windows有一些不同，比如它的Command键，Option键，完全不知道什么情况下使用，习惯了Windows的用户还是需要一些时间去适应 Mac OS X操作系统。而Mac OS X是由BSD系列衍生出来的，而BSD属于*NIX系列，了解操作系统的历史就会知道。平时也有使用Ubuntu和Fedora，但是拿到Mac还是不能快速找到节奏，快捷键什么的都是不一样的。不过这些都不是大问题，这些在使用了几个小时后就已经熟练了，使用熟练后，才会感觉到它对效率的提升。 如果平时使用Windows电脑经常遇到卡，电脑得好久才能反应过来，那么强烈建议换一台设备，Mac应是首选。至少经常卡顿的那台电脑不要应用于平时的工作中，不是由于它不够可靠，而是经常卡顿，对工作的思路和积极性会的影响不是一点半点。当用过Mac后，再也不想摸Windows了。 虽然Mac相对来说价格偏高，但是当使用之后，一定会觉得物有所值，贵是有贵的道理的。所以想要入手Mac的朋友，放下你的Windows电脑，立即去买Mac，不再犹豫，在Windows下能够做的事情，在Mac下一样可以做到并且会更快速。特别是做程序开发的朋友，如果是微软阵营，买Mac可能不是很必要，其他非微软阵营的，这件事请是值得考虑的。当然，如果是要玩游戏的话，Windows还是最佳的的选择。不管是办公还是装逼，找不到任何理由不使用Mac，我想我再也不想用Windows了。 走，Mac带回家！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mac使用]]></title>
      <url>%2F2017%2F01%2F01%2Fmac-using%2F</url>
      <content type="text"><![CDATA[安装常用软件brew是Mac下一款包管理器，默认是没有安装的。先安装包管理工具，其他软件可以通过包管理工具方便的安装。在终端中输入如下命令安装brew。 1/usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)" 安装完毕brew后，可以通过brew安装其他软件： 1234567891011121314# 安装Lanternbrew cask install Caskroom/cask/lantern# 安装Google Chromebrew cask install Caskroom/cask/google-chrome# 安装Javabrew cask install java# 安装PyCharmbrew cask install pycharm# 安装wgetbrew cask install wget# 安装gradlebrew install gradle# 列出已经安装的软件名字brew list 有些命令安装软件是brew，而有些命令安装软件是brew cask。brew是从下载源码解压然后./configure &amp;&amp; make install，同时会包含相关依存库。并自动配置好各种环境变量，而且易于卸载。而brew cask是已经编译好了的应用包(.dmg[Apple Disk Image]/.pkg)，仅仅是下载解压，放在统一的目录中(/opt/homebrew-cask/Caskroom)，省掉了自己去下载、解压、拖拽（安装）等步骤，同样，卸载相当容易与干净。这个对一般用户来说会比较方便，包含很多在AppStore里没有的常用软件。 brew默认的安装路径在/usr/local/bin下。 设置root打开终端，键入命令sudo passwd root,然后提示输入当前登录用户密码，通过以后，提示输入两遍root的密码。这样就设置好root帐号密码了，可以用root来登录MacOS了。打开终端，输入命令su root，输入密码即可． 显示隐藏的文件夹根目录有些文件夹默认是隐藏的，但是有时候配置的时候需要查看隐藏的目录,要显示隐藏的目录，运行如下命令即可． 12defaults write com.apple.finder AppleShowAllFiles -bool trueKillAll Finder 前往根目录，在Finder里按快捷键Ctrl + Alt + G．输入根目录符号/即可，可以看到原来没有显示的隐藏目录． 设置Java环境变量在当前用户的根目录下新建.bash_profiile隐藏文件。文件中添加如下内容： 123export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_112.jdk/Contents/Home #jdk安装路径 export PATH=$JAVA_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 常用快捷键 快捷键(Short Keys) 作用 ⌃ Control Control(⌃) + Command(⌘) + F(Full Screen) 终端进入(退出)全屏模式，其他程序也可以使用快捷键进行最大话(最适化) Command + Shift + { 在终端(Terminal)中向左切换Tab页 Command + Shift + } 在终端(Terminal)中向右切换Tab页 Command + Space 打开Spotlight，快速搜索应用 Command + left 在Google Chrome中后退 Command + Enter 在新的标签页里打开链接 Command + Tab +Q 在按Command + Tab出现运行程序列表后，按住Command键不松手，再按下Q键即可彻底退出程序 Command + L(Location) 在Google Chrome中，可以快读定位到浏览器的链接处 Ctrl + Up 到程序Tab页概览视图 Ctrl + Down 从概览视图的当前页，视图最适化 Ctrl + Left/Right 不同程序视图之间切换 Command + Tab + Option 先按住Command + Tab，切换到需要最大化的程序,接着松开Tab，Command键不松，按住Option键，松开Command即可 Command + O/Down 在Finder中打开文件 Ctrl + Space 输入法切换(用苹果的输入法即可) Command + Option + Shift + Esc 强制关闭当前活动的程序 Command + Option + Esc 打开强制关闭程序对话框]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[spring-boot-app-management]]></title>
      <url>%2F2016%2F12%2F31%2Fspring-boot-app-management%2F</url>
      <content type="text"><![CDATA[123456789@ComponentScan@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication app = new SpringApplication(Application.class); app.addListeners(new ApplicationPidFileWriter("app.pid")); app.run(args); &#125;&#125; when we already have our PID file we need bash script providing standard operations like stop, start, restart and status checking. Below you can find simple script solving that challenge.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[我的背包]]></title>
      <url>%2F2016%2F12%2F31%2Fbackpack%2F</url>
      <content type="text"><![CDATA[是时候换一个高逼格的背包了，包里装电脑、书、笔记本，可以随时翻一翻，想想还蛮惬意的。上一个背包是大二去学校的时候老爸在镇上买的，结实耐用，到如今已经5年有余了。对背包的几点要求： 要有逼格，不能太土，太Low，不能让背包和我一样 结实、耐用，持久是必须的 可以放一些小物件(数据线、U盘、充电器、水杯) 价格不能太贵(500以内)，毕竟现在还挣扎在温饱线上 作为一个下流码农，包就如同剑客的剑鞘，电脑如同剑一样，包在人在，包亡人亡，虽然是玩笑话，也不禁热血沸腾。经过最终的比对，选择了City Compact Backpack，官网上是100美金左右。浏览了一下介绍的页面，不得不称赞，网页的字体非常漂亮。某宝上下单，748RMB。另外一款黑色的要便宜100多，在金钱面前总是那么容易丧失原则。最后还是选择了逼格满满的石楠黑，卖家声称石楠黑是新型环保面料，防水的效果也很好。天天吸霾的我，终于找到机会为祖国的环保事业贡献自己的一份力量了，默默的选择了石楠黑。 City Compact Backpack是Incase旗下的产品。Incase创立于1997年美国加州，是专门以Apple产品为基础，用精品水准生产Apple周边产品的一个品牌。与 Apple 相同，Incase品牌风格主打的是高级简约的路线。 买回来后看了一下它的铭牌，是采用一种叫做Ecoya的材料，这种材料在生产上色过程中采用了节水和低二氧化碳排放的工艺，既能保持原先的耐用性，也比传统材料更环保。不过还是跟自己预想的环保材料有差异，本以为是那种可以自然降解对环境无害的材料。不管只是商家的噱头也好，还是真正的减少了能量的消耗和碳的排放，也算是进步吧。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java中synchronized、reentrantlock、lock区别]]></title>
      <url>%2F2016%2F12%2F30%2Fsynchronized-reentrantlock-lock%2F</url>
      <content type="text"><![CDATA[synchronizedreentrantlock与目前的 synchronized 实现相比，争用下的 ReentrantLock 实现更具可伸缩性。（在未来的 JVM 版本中，synchronized 的争用性能很有可能会获得提高。）这意味着当许多线程都在争用同一个锁时，使用 ReentrantLock 的总体开支通常要比 synchronized 少得多。eentrantLock 构造器的一个参数是 boolean 值，它允许您选择想要一个 公平（fair）锁，还是一个 不公平（unfair）锁。公平锁使线程按照请求锁的顺序依次获得锁；而不公平锁则允许讨价还价，在这种情况下，线程有时可以比先请求锁的其他线程先得到锁。为什么我们不让所有的锁都公平呢？毕竟，公平是好事，不公平是不好的，不是吗？（当孩子们想要一个决定时，总会叫嚷“这不公平”。我们认为公平非常重要，孩子们也知道。）在现实中，公平保证了锁是非常健壮的锁，有很大的性能成本。要确保公平所需要的记帐（bookkeeping）和同步，就意味着被争夺的公平锁要比不公平锁的吞吐率更低。作为默认设置，应当把公平设置为 false ，除非公平对您的算法至关重要，需要严格按照线程排队的顺序对其进行服务。 lockLock 和 synchronized 有一点明显的区别 —— lock 必须在 finally 块中释放。否则，如果受保护的代码将抛出异常，锁就有可能永远得不到释放！这一点区别看起来可能没什么，但是实际上，它极为重要。忘记在 finally 块中释放锁，可能会在程序中留下一个定时炸弹，当有一天炸弹爆炸时，您要花费很大力气才有找到源头在哪。而使用同步，JVM 将确保锁会获得自动释放。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ElasticSearch使用]]></title>
      <url>%2F2016%2F12%2F30%2Felasticsearch-using%2F</url>
      <content type="text"><![CDATA[什么是索引(Index)索引只是一个用来指向一个或多个分片(shards)的“逻辑命名空间(logical namespace)”.一个分片(shard)是一个最小级别“工作单元(worker unit)”,它只是保存了索引中所有数据的一部分。分片就是一个Lucene实例，并且它本身就是一个完整的搜索引擎。我们的文档存储在分片中，并且在分片中被索引，但是我们的应用程序不会直接与它们通信，取而代之的是，直接与索引通信。当索引创建完成的时候，主分片的数量就固定了，但是复制分片的数量可以随时调整。Cluster包含多个node，Indices不应该理解成动词索引，Indices可理解成关系数据库中的databases，Indices可包含多个Index，Index对应关系数据库中的database，它是用来存储相关文档的。Elasticsearch与关系数据的类比对应关系可以做如下理解： 123Relational DB ⇒ Databases ⇒ Tables ⇒ Rows ⇒ ColumnsElasticsearch ⇒ Indices ⇒ Types ⇒ Documents ⇒ Fields 什么是分片(Shard)分片(Shard)是Elasticsearch在集群中分发数据的关键。文档存储在分片中，然后分片分配到集群中的节点上。当集群扩容或缩小，lasticsearch将会自动在你的节点间迁移分片，以使集群保持平衡。分片可以是主分片(primary shard)或者是复制分片(replica shard)。索引中的每个文档属于一个单独的主分片，所以主分片的数量决定了索引最多能存储多少数据。理论上主分片能存储的数据大小是没有限制的，限制取决于你实际的使用情况。分片的最大容量完全取决于你的使用状况：硬件存储的大小、文档的大小和复杂度、如何索引和查询你的文档，以及你期望的响应时间。复制分片只是主分片的一个副本，它可以防止硬件故障导致的数据丢失，同时可以提供读请求，比如搜索或者从别的shard取回文档。 创建索引(Index)如下curl命令，创建一个名字为jiangxiaoqiang的索引(Index)： 123456789curl -XPUT 'localhost:9200/jiangxiaoqiang?pretty' -d'&#123; "settings" : &#123; "index" : &#123; "number_of_shards" : 3, "number_of_replicas" : 2 &#125; &#125;&#125;' url里面的pretty是pretty format的含义，指代参数的json是良好格式化的。 创建TypeElasticSearch中每个文档必须有一个类型定义。这里的类型相当于数据库当中的表，类型定义了字段映射（类似数据库表结构），这样一来，每个索引可以包含多种文档类型，而每种文档类型定义一种映射关系。 123456789101112curl -XPUT 'localhost:9200/jiangxiaoqiang/typejxq/_mapping' -d'&#123; "typejxq": &#123; "properties": &#123; "name": &#123; "type": "string" &#125;, "desc": &#123; "type": "string" &#125; &#125; &#125;&#125;' 添加数据1234curl -XPUT 'localhost:9200/jiangxiaoqiang/typejxq/1' -d'&#123; "name":"jiangxiaoqiang", "desc":"dfaewgrehrehr"&#125;' 查询数据12345678910111213141516171819curl -XPOST 'localhost:9200/jiangxiaoqiang/_search' -d'&#123; "query": &#123; "bool": &#123; "must": [ &#123; "prefix": &#123; "name": "jiang" &#125; &#125; ], "must_not": [], "should": [] &#125; &#125;, "from": 0, "size": 10, "sort": [], "aggs": &#123;&#125;&#125;' 参考资料： 在ElasticSearch中，集群(Cluster),节点(Node),分片(Shard),Indices(索引),replicas(备份)之间是什么关系？ Elastic Stack and Product Documentation]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java中的时间]]></title>
      <url>%2F2016%2F12%2F28%2Fjava-date%2F</url>
      <content type="text"><![CDATA[如果单纯以Date类型定义时间字段，定义时间的格式： 123@ApiModelProperty(value = "数据更新时间戳")@DateTimeFormat(pattern = "yyyy-MM-dd HH:mm:ss")private LocalDateTime sjc; 那么返回的时间值如下： 1&quot;jdrq&quot;:&quot;2016-06-26T00:23:15.000+0000&quot; 如果想返回类似2016-06-26 00:00:00格式的时间，那么需要定义日期类型为LocalDateTime。返回的时间里，日期和时间中始终有一个大写T，原来是LocalDateTime的toString方法在转化时添加了一个T字符： 123456789101112131415161718192021//-----------------------------------------------------------------------/** * Outputs this date-time as a &#123;@code String&#125;, such as &#123;@code 2007-12-03T10:15:30&#125;. * &lt;p&gt; * The output will be one of the following ISO-8601 formats: * &lt;ul&gt; * &lt;li&gt;&#123;@code uuuu-MM-dd'T'HH:mm&#125;&lt;/li&gt; * &lt;li&gt;&#123;@code uuuu-MM-dd'T'HH:mm:ss&#125;&lt;/li&gt; * &lt;li&gt;&#123;@code uuuu-MM-dd'T'HH:mm:ss.SSS&#125;&lt;/li&gt; * &lt;li&gt;&#123;@code uuuu-MM-dd'T'HH:mm:ss.SSSSSS&#125;&lt;/li&gt; * &lt;li&gt;&#123;@code uuuu-MM-dd'T'HH:mm:ss.SSSSSSSSS&#125;&lt;/li&gt; * &lt;/ul&gt; * The format used will be the shortest that outputs the full value of * the time where the omitted parts are implied to be zero. * * @return a string representation of this date-time, not null */@Overridepublic String toString() &#123; return date.toString() + 'T' + time.toString();&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Ubuntu使用Latex]]></title>
      <url>%2F2016%2F12%2F28%2Fubuntu-using-latex%2F</url>
      <content type="text"><![CDATA[LaTeX（ LATEX，音译“拉泰赫”）是一种基于TeX的排版系统，由美国计算机学家莱斯利·兰伯特（Leslie Lamport）在20世纪80年代初期开发。 安装在Ubuntu 16.04中，输入如下命令安装TeX Live： 1sudo apt install texlive-full -y]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ElasticSearch入门]]></title>
      <url>%2F2016%2F12%2F27%2Felasticsearch-introduce%2F</url>
      <content type="text"><![CDATA[简介ElasticSearch是一个基于Lucene开发的搜索服务器，具有分布式多用户的能力，ElasticSearch是用Java开发的开源项目，基于Restful Web接口，能够达到实时搜索、稳定、可靠、快速、高性能、安装使用方便，同时它的横向扩展能力非常强，不需要重启服务。ElasticSearch是一个非常好用的实时分布式搜索和分析引擎，可以帮助我们快速的处理大规模数据，也可以用于全文检索，结构化搜索以及分析等。目前很多网站都在使用ElasticSearch进行全文检索，例如：GitHub、StackOverflow、Wiki等。ElasticSearch式建立在全文检索引擎Lucene基础上的，而Lucene是最先进、高效的开元搜索引擎框架，但是Lucene只是一个框架，要充分利用它的功能，需要很高的学习成本，而ElasticSearch使用Lucene作为内部引擎，在其基础上封装了功能强大的Restful API，让开发人员可以在不需要了解背后复杂的逻辑，即可实现比较高效的搜索。 倒排索引(Inverted Index)关系型数据库为了提高查询效率会添加索引，比如MySQL就是B-Tree索引(B+树)，还有文件系统也是采用B+树。搜索引擎的基础数据结构倒排索引(Inverted Index)。在平时，会经常使用各种各样的索引，如根据链接，可以找到链接里的具体文本，这就是索引。反过来，如果，如果我们能根据具体文本，找到文本存在的具体链接，这就是倒排索引，可简单理解为从文本到链接的映射。平时在使用Google、百度时，就是根据具体文本去找链接，这就是以倒排索引为基础的。ElasticSearch也是使用的是倒排索引(Inverted Index)，也是为了提高查询速度。 应用案例 GitHub searches 20TB of data using Elasticsearch, including 1.3 billion files and 130 billion lines of code. Foursquare:”实时搜索5千万地理位置信息” SoundCloud使用ElasticSearch为1.8亿用户提供即时而精准的音乐搜索服务 Elasticsearch使Fog Creek可以在400亿行代码中进行一个月3千万次的查询 特点 Open Source（开源） Apache Lucene（基于 Lucene） Schema Free(模式自由) Document Oriented(面向文档型的设计) Real Time Data &amp; Analytics（实时索引数据） Distributed（分布式） High Availability（高可靠性） 其他特性：RESTful API；JSON format；multi-tenancy；full text search；conflict management；per-operation persistence 基本概念Gateway：代表ElasticSearch索引快照的存储方式，ElasticSearch默认是先把索引存放到内存中，当内存满了时再持久化(Persist)到本地硬盘。gateway对索引快照进行存储，当这个ElasticSearch集群关闭再重新启动时就会从gateway中读取索引备份数据。ElasticSearch支持多种类型的gateway，有本地文件系统（默认），分布式文件系统，Hadoop的HDFS和Amazon的s3云存储服务。 安装输入如下命令下载ElasticSearch： 1curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.1.1.tar.gz -L参数表示–location，因为有些下载链接有重定向操作，L操作告诉curl如果返回的是重定向30X，那么就直接再次发起请求，请求重定向到的网址。O参数表示output，–remote-name，表示写入到本地磁盘的文件名称和远程文件名称(remote name)一样。下载完毕后，将文件拷贝到需要安装的目录下： 1mv elasticsearch-5.1.1.tar.gz /opt/local/tools/ 将文件解压： 1tar -xzvf elasticsearch-5.1.1.tar.gz 启动ElasticSearch: 1./elasticsearch -d 输入如下命令判断是否安装成功： 1curl http://localhost:9200/?pretty 返回内容为： 12345678910111213&#123; &quot;name&quot; : &quot;eS0_ZDL&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;DU2WJNtbRmOHXF9HFFs5LQ&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;5.1.1&quot;, &quot;build_hash&quot; : &quot;5395e21&quot;, &quot;build_date&quot; : &quot;2016-12-06T12:36:15.409Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.3.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 安装插件Head插件ElasticSearch从版本5+即不支持site plugin(site plugins are not supported). Run elasticsearch-head as a standalone server。安装Grunt： 12sudo npm install -g grunt-clisudo npm install grunt --save-dev Grunt安装完毕后，接着安装elasticsearch-head： 1234git clone git://github.com/mobz/elasticsearch-head.gitcd elasticsearch-headnpm installgrunt server 安装完毕后，访问地址：http://localhost:9100/。由于有同源策略，暂时还无法连接。 1XMLHttpRequest cannot load http://localhost:9200/_cluster/state. No &apos;Access-Control-Allow-Origin&apos; header is present on the requested resource. Origin &apos;http://localhost:9100&apos; is therefore not allowed access. 在ElasticSearch配置文件中(/opt/local/tools/elasticsearch-5.1.1/config/elasticsearch.yml)添加如下配置： 12http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; 最后成功连接集群，如下图所示： Elasticsearch 简介 elasticsearch-the-definitive-guide-clinton-gormley-zachary-tong]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Gradle使用]]></title>
      <url>%2F2016%2F12%2F27%2Fgradle-using%2F</url>
      <content type="text"><![CDATA[使用以下配置文件对Gradle的构建进行配置： Gradle构建脚本（build.gradle）指定了一个项目和它的任务。 Gradle属性文件（gradle.properties）用来配置构建属性。 Gradle设置文件（gradle.settings）对于只有一个项目的构建而言是可选的，如果我们的构建中包含多于一个项目，那么它就是必须的，因为它描述了哪一个项目参与构建。每一个多项目的构建都必须在项目结构的根目录中加入一个设置文件。 基础在Ubuntu 16.04 LTS中安装Gradle: 1sudo apt install gradle -y 如下列出了Gradle常用的命令： 123456789101112131415161718# 查看所有任务gradle taskgradle task --all# 查看所有项目gradle projects# 清楚app目录下的build文件夹gradle clean# 检查依赖并编译打包 （正式和测试）gradle build#编译并打Debug包gradle assembleDebug#编译并打Release包gradle assembleRelease 定义一个依赖通常需要三个元素： group:创建该library的组织名，通常也会是包名， name :是该library的唯一标识 version:该library的版本号 一个简单的打包命令： 1gradle -p cc-web-boot bootRun -p参数指定项目(project directory)的目录，如果不指定就是当前目录(-p, –project-dir,Specifies the start directory for Gradle. Defaults to current directory)。bootRun指代的是当前项目下的任务名称。 插件(Plugin)插件就是 Gradle 的扩展，简而言之就是为你添加一些非常有用的默认配置。Gradle 自带了很多插件，并且你也可以很容易的编写和分享自己的插件。Java plugin 作为其中之一，为你提供了诸如编译，测试，打包等一些功能。插件配置示例： 123apply plugin: &apos;java&apos;apply plugin: &apos;propdeps&apos;apply plugin: &apos;org.springframework.boot&apos; WrapperWrapper主要是考虑在没有安装Gradle的电脑上使用Gradle命令。当执行gradlew(Gradle Wrapper)命令时，首先会检查电脑是否安装了Gradle，如果没有安装，会自动从gradle repository下载安装。需要查看Gradle Wrapper帮助，在命令行中输入如下命令： 1gradle Wrapper --help 会打印出所有Gradle Wrapper. Gradle界面(GUI)在终端中输入如下命令可以查看Gradle的GUI： 1gradle --gui]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Restful API文档编写]]></title>
      <url>%2F2016%2F12%2F27%2Frest-api-doc%2F</url>
      <content type="text"><![CDATA[在写Restful(Representational state transfer)接口时，文档是必不可少的，文档采用Springfox自动生成。Swagger是用来描述和文档化RESTful API的一个项目。Swagger Spec是一套规范，定义了该如何去描述一个RESTful API。类似的项目还有RAML、API Blueprint。 根据Swagger Spec来描述RESTful API的文件称之为Swagger specification file，它使用JSON来表述，也支持作为JSON支持的YAML(YAML Ain’t Markup Language)。swagger-core是一个Java的实现，现在支持JAX-RS。swagger-annotation定义了一套注解给用户用来描述API。spring-fox也是一个Java的实现，它支持Spring MVC， 它也支持swagger-annotation定义的部分注解。 对接口文档的要求有如下几点： 要实时更新，代码变动，参数变动后，文档要相应更新 需要显示请求/返回数据类型、请求/返回数据示例 支持泛型返回值接口的返回值是通过泛型来动态定义的，Springfox生成接口文档时，由于未指定具体的返回类型，所以无法生成返回实体对应的注释。可以通过手动定义返回实体，手动在接口返回时指定的方式来解决此问题。例如，返回的数据定义成泛型： 12345678910111213@lombok.Data@io.swagger.annotations.ApiModel(value = "ApiResult", description = "Api返回结果")public class ApiResult&lt;T&gt;&#123; @ApiModelProperty(value = "错误码") private int errCode = 0; @ApiModelProperty(value = "错误消息") private String errMsg = ""; @ApiModelProperty(value = "返回数据") private T data = null;&#125; 手动定义一个具体的实体消除泛型： 123public class SingleBlacklistResult extends AbstractApiResult &#123; private SummaryBlacklist data;&#125; SummaryBlacklist就是编译时具体的泛型类型。在接口中指定返回类型： 1234@GetMapping @ApiOperation(value = "必须且只能传一个参数", response = BlackListApiResult.class) public ApiResult getBlackList(@RequestParam(required = false) @ApiParam("主体名（模糊）") String xdr&#125; 在接口文档页面即可看到返回的具体的实体了，如下图所示。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用Rest-Assured做Restful接口自动化测试]]></title>
      <url>%2F2016%2F12%2F26%2Frestful-api-auto-test%2F</url>
      <content type="text"><![CDATA[使用Python脚本测试安装pycurl包： 1pip install pycurl 安装时可能会出现如下问题： 1Command &quot;python setup.py egg_info&quot; failed with error code 1 in /tmp/pip-build-jzTgwZ/pycurl 使用如下命令安装依赖包： 1apt-get install libcurl4-openssl-dev 使用rest-assured框架测试rest-assured框架用于测试REST方式的接口，使用JSON schema验证返回体，大大的简化了测试代码。esting and validation of REST services in Java is harder than in dynamic languages such as Ruby and Groovy. REST Assured brings the simplicity of using these languages into the Java domain. 引入依赖包在Gradle中，引入rest-assured框架的依赖包和TestNG的依赖包。 123456789101112project(&apos;:cc-api&apos;) &#123; apply from: ccCommonBuildScript description = &apos;credit-system-api&apos; dependencies &#123; compile &apos;io.rest-assured:json-path:3.0.1&apos; compile &apos;org.testng:testng:6.8.17&apos;//TestNG依赖包 testCompile &apos;io.rest-assured:rest-assured:3.0.1&apos; compile(&apos;com.jayway.restassured:rest-assured:2.3.1&apos;)//rest-assured框架依赖包 compile project(&apos;:cc-business&apos;) &#125;&#125; 添加测试方法测试方法如下： 1234567891011121314151617import common.TestPublicVariable;import org.testng.annotations.Test;import static io.restassured.RestAssured.given;import static org.hamcrest.Matchers.*;public class ApiXysjControllerTest &#123; @Test public void testCountTotal() throws Exception &#123; given().header(TestPublicVariable.APPID, TestPublicVariable.APPID_VALUE) .header("TIMESTAMP", "2016-12-19 16:58:02") .header("ECHOSTR", "sdsaasf") .header("TOKEN", "14d45648c62a746ae9dd9b90c03c50893061222d") .get("http://192.168.32.105:28080/api/xysj/counttotal") .then() .body("errCode", equalTo(0)); &#125;&#125; 上述方法在请求头中加入认证信息，get中为请求的地址，根据返回的error code为0判断服务端成功处理了此次请求。测试结果如下图：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[meld使用]]></title>
      <url>%2F2016%2F12%2F25%2Fmeld-using%2F</url>
      <content type="text"><![CDATA[简介Meld is a visual diff and merge tool targeted at developers. Meld helps you compare files, directories, and version controlled projects. It provides two- and three-way comparison of both files and directories, and has support for many popular version control systems. Meld helps you review code changes and understand patches. It might even help you to figure out what is going on in that merge you keep avoiding. 安装与配置输入如下命令安装： 1234#在Fedora 24中安装dnf install meld -y#在Ubuntu 16.04中安装sudo apt install meld -y 配置： 12#配置meld为默认的合并工具(Merge Tool)git config --global merge.tool meld 使用合并工具： 12#使用merge tool合并git mergetool 可以直接键入meld命令打开meld的GUI。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[uget使用]]></title>
      <url>%2F2016%2F12%2F25%2Fuget-using%2F</url>
      <content type="text"><![CDATA[uGet是一款Linux下的下载工具，uGet is the #1 Open Source Download Manager app.uGet is a multi-platform app for Linux, BSD, Android &amp; Windows (XP - 8)。uGet下载最大的体会比较稳定，直观的感受是，用Google Chrome（中途可能变为速度为0的状态）和Wget(可能失去下载连接)无法下载的文件，使用uGet可以下载。 安装在Ubuntu 16.04中，输入如下命令安装uGet。 1sudo apt install uget -y 打开uGet即可使用，如下图所示： 插件(Plugin)uGet可以作为aria2的UI，aria2下载工具特点： 支持http/https/ftp/bt协议 支持metalink3.0，metalink是一种可以将不同协议下载的同一文件集合到一起以达到最大下载稳定性和速度 支持分段下载和续传 支持通过http代理的ftp下载 可以作为守护进程(Deamon)运行 在Ubuntu 16.04中，输入如下命令安装aria2： 1sudo apt install aria2 -y 安装完毕后在uGet中(uget→编辑(Edit)→设置(Settings))进行设置即可，如下图所示：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Lombok精简Java代码]]></title>
      <url>%2F2016%2F12%2F25%2Flombok%2F</url>
      <content type="text"><![CDATA[简介lombok提供了简单的注解的形式来帮助我们简化消除一些必须有但显得很臃肿的java代码,特别是相对于POJO(Plain Ordinary Java Object)。安装Lombok Plugin和引入Jar包之后方可使用。 安装安装完毕lombok插件之后，添加如下包： 12345&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.6&lt;/version&gt;&lt;/dependency&gt; 常用的 lombok 注解： @Data：注解在类上；提供类所有属性的 getting 和 setting 方法，此外还提供了equals、canEqual、hashCode、toString 方法 @Setter：注解在属性上；为属性提供 setting 方法 @Getter：注解在属性上；为属性提供 getting 方法 @Log4j：注解在类上；为类提供一个 属性名为log 的 log4j 日志对象 @NoArgsConstructor：注解在类上；为类提供一个无参的构造方法 @AllArgsConstructor：注解在类上；为类提供一个全参的构造方法 如果不使用lombok注解，代码是这样： 1234567891011121314151617181920public class Person &#123; private String id; private Logger log = Logger.getLogger(Person.class); public Person() &#123; &#125; public Person(String id) &#123; this.id = id; &#125; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125;&#125; 使用lombok注解，代码是这样： 1234567@Data@Log4j@NoArgsConstructor@AllArgsConstructorpublic class Person &#123; private String id;&#125; 一旦POJO字段较多时精简的代码就很明显了，而且使用Lombok的代码更加干净、易读，添加@Data注解之后，按Ctrl + O(Outline)可以看到生成了get和set方法。 @EqualsAndHashCode@EqualsAndHashCode注解实现equals()方法和hashCode()方法。hashcode是用于散列数据的快速存取，如利用HashSet/HashMap/Hashtable类来存储数据时，都是根据存储对象的hashcode值来进行判断是否相同的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring框架搭建]]></title>
      <url>%2F2016%2F12%2F25%2Fspring%2F</url>
      <content type="text"><![CDATA[Spring的不足： 配置太多 常见问题Could not open ServletContext resource [/WEB-INF/applicationContext.xml]ContextLoaderListener has its own context which is shared by all servlets and filters. By default it will search /WEB-INF/applicationContext.xml。 1234&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/somewhere-else/root-context.xml&lt;/param-value&gt;&lt;/context-param&gt; Missing artifact org.aspectj:aspectjweaver:jar:1.8.0.M1According to a reported issue at springsource, aspectjweaver is “basically identical to AspectJ 1.7” except that it has early support for Java 8.As I don’t need Java 8 support, I basically added a compile dependency to the latest release version of aspectweaver: 12345&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.7.4&lt;/version&gt;&lt;/dependency&gt; This ensures that the 1.7.4 is used instead of the milestone release, and is an acceptable workaround for me, for the time being. Exception java.lang.ClassNotFoundException: org.apache.commons.dbcp.BasicDataSource在POM.xml中引入jar包。 12345&lt;dependency&gt; &lt;groupId&gt;commons-dbcp&lt;/groupId&gt; &lt;artifactId&gt;commons-dbcp&lt;/artifactId&gt; &lt;version&gt;1.2.2&lt;/version&gt;&lt;/dependency&gt; java.lang.NoClassDefFoundError: org/apache/ibatis/session/SqlSessionFactory引入jar包。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.ibatis&lt;/groupId&gt; &lt;artifactId&gt;ibatis-core&lt;/artifactId&gt; &lt;version&gt;3.0&lt;/version&gt;&lt;/dependency&gt;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring MVC-DispatcherServlet处理请求过程]]></title>
      <url>%2F2016%2F12%2F25%2Fspringmvc-dispatcherservlet%2F</url>
      <content type="text"><![CDATA[Spring MVC基于模型-视图-控制器(Model-View-Controller,MVC)模式，它能够帮助我们建立灵活和松耦合的Web程序.Spring MVC中用户请求如下图所示： 请求发出(HTTP Resquest)一个请求携带信息进入Spring MVC程序时，第一站是Spring的DispatcherServlet(在web.xml中配置)，它是接收所有用户请求的前端控制器Servlet。DispatcherServlet对请求URL进行解析，得到请求资源标识符（URI）。然后根据该URI，调用HandlerMapping获得该Handler配置的所有相关的对象（包括Handler对象以及Handler对象对应的拦截器），最后以HandlerExecutionChain对象的形式返回；DispatcherServlet的任务是将请求转发给Spring MVC控制器(Controller)，控制器是用于处理用户请求的Spring Bean。DispatcherServlet 根据获得的Handler，选择一个合适的HandlerAdapter。（如果成功获得HandlerAdapter后，此时将开始执行拦截器的preHandler()方法） 处理器映射(Handler Mapping)一般程序中会有不止一个控制器，那么DispatcherServlet需要知道将请求发送给哪个控制器，所以DispatcherServlet将会查询处理器映射(handler mapping)来确定请求的下一站，处理器映射会根据用户请求的URL来决定是哪个控制器。提取Request中的模型数据，填充Handler入参，开始执行Handler（Controller)。 在填充Handler的入参过程中，根据你的配置，Spring将帮你做一些额外的工作： HttpMessageConveter： 将请求消息（如Json、xml等数据）转换成一个对象，将对象转换为指定的响应信息 数据转换：对请求消息进行数据转换。如String转换成Integer、Double等 数据根式化：对请求消息进行数据格式化。 如将字符串转换成格式化数字或格式化日期等 数据验证： 验证数据的有效性（长度、格式等），验证结果存储到BindingResult或Error中 控制器(Controller)DispatcherServlet选择了控制器之后，就会将请求发送给该控制器并等待控制器处理用户请求。控制器在完成了逻辑处理后，通常会返回处理结果并将这些结果在浏览器上显示，这些信息在Spring MVC中成为模型(Model)。Handler执行完成后，向DispatcherServlet 返回一个ModelAndView对象。 模型以及逻辑视图名称(Model)控制器仅仅返回模型信息往往是不够的，需要对模型信息进行格式化，生成用户友好的方式如html进行显示。所以模型信息将会被发送给一个视图(View，例如jsp视图。事实上，控制器同时产生了模型以及视图名称，将这些信息发送回DispatcherServlet。 视图解析器(View Parser)DispatcherServlet收到控制器的视图名称并不直接表示某个特定的JSP，这个视图名称仅仅是个逻辑值，DispatcherServlet为了找到真正的视图，会使用视图解析器(view resolver)将视图名称匹配成一个具体的视图。根据返回的ModelAndView，选择一个适合的ViewResolver（必须是已经注册到Spring容器中的ViewResolver)返回给DispatcherServlet。 视图(View)目前为止，DispatcherServlet知道了具体由哪个视图来显示模型信息，那么它就会将模型信息交付给视图，请求的任务到这里就完成了。ViewResolver 结合Model和View，来渲染视图。 响应(HTTP Response)视图渲染模型信息并输出，该输出最后会传递给用户端，展示给用户查看。从以上步骤可以看到，Spring MVC要处理很多过程，但是大部分过程是Spring框架内部处理的，事实上，我们可以十分方便利用Spring MVC框架的编写功能强大的Web应用程序，下一章我们将搭建一个基础的Spring MVC实例程序。 原始地址： Spring MVC入门-DispatcherServlet处理请求过程]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Windows下用Nexus搭建Maven私服]]></title>
      <url>%2F2016%2F12%2F25%2Fmaven-private-service%2F</url>
      <content type="text"><![CDATA[Nexus简介使用Maven的中央仓库存在如下问题： Maven自己的中央库访问速度非常慢，外加GFW，基本没法用。 有些jar包由于版权原因，maven中央仓库没有，比如oracle JDBC驱动。另外也会有一些项目中用到的比较老的开源jar包， 中央仓库也没有。这种情况我们需要把jar包手动上传到私服。 公司自己开发的jar包并不开源，不能上传到maven中央仓库，只能部署到私服上面。 Nexus 是Maven仓库管理器，如果你使用Maven，你可以从Maven中央仓库下载所需要的构件（artifact），但这通常不是一个好的做法，你应该在本地架设一个Maven仓库服务器，在代理远程仓库的同时维护本地仓库，以节省带宽和时间，Nexus就可以满足这样的需要。此外，他还提供了强大的仓库管理功能，构件搜索功能，它基于REST，友好的UI是一个extjs的REST客户端，它占用较少的内存，基于简单文件系统而非数据库。这些优点使其日趋成为最流行的Maven仓库管理器。下载Nexus（nexus-3.0.1-01-win64.exe），安装完毕后访问本地Maven私服主页。 配置单个项目安装完毕后需要登录，默认的用户名密码是：admin/admin123。登录之后才会显示设置图标，才能添加repositories。将本地Maven私服路径配置到项目的pom.xml中即可。如下代码片段所示。 1234567891011121314151617181920212223242526&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;my-nexus-repository&lt;/name&gt; &lt;url&gt;http://192.168.1.102:8081/repository/maven-public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;my-nexus-repository&lt;/name&gt; &lt;url&gt;http://192.168.1.102:8081/repository/maven-public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt;&lt;/pluginRepositories&gt; 如果在本地私服没有的jar包，会自动从中心服务器下载。至此，最简单的Maven私服搭建完毕。 配置全局应用在Maven的settings.xml中配置profile元素，这样就能让本机所有的Maven项目都使用自己的Maven私服。 1234567891011121314151617181920212223242526&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;central&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus&lt;/name&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt;&lt;/profiles&gt;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java Web开发--log4j]]></title>
      <url>%2F2016%2F12%2F25%2Flog4j%2F</url>
      <content type="text"><![CDATA[isDebugEnabled在输出日志时，判断输出级别： 123if(logger.isDebugEnabled()) &#123; logger.debug("通用处理，信息为：" + JSON.toJSONString(vehicleLocationData));&#125; 当输出级别是debug，即需要进行日志信息输出时，加不加这句if判断，在效率上几乎没有差别；当输出级别高于debug，即不需要进行日志信息输出时： ①假如debug方法中的参数比较简单时（比如直接就是写好的字符串），加不加这句if判断，在效率上也几乎没有什么差别； ②假如debug方法中的参数比较复杂时（比如还要使用别的函数进行计算、或者还要进行字符串的拼接等等,如上代码片段所示，输出的内容需要序列化），在前面就加上这句if判断，会让效率提高（否则，开始大动干戈做了很多事情（比如字符串的拼接，序列化），后来才发现不需要进行输出日志信息），白白浪费了CPU资源，影响程序的运行效率。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Websocket发送消息长度设置]]></title>
      <url>%2F2016%2F12%2F25%2Fwebsocket-sending-config%2F</url>
      <content type="text"><![CDATA[项目中用到WebSocket技术，发送文本字节比较多时出现异常，立即断开了连接。项目在Tomcat上运行，Tomcat默认的文本大小为8192。 1If the application does not define a MessageHandler.Partial for incoming text messages, any incoming text messages must be buffered so the entire message can be delivered in a single call to the registered MessageHandler.Whole for text messages. The default buffer size for text messages is 8192 bytes. This may be changed for a web application by setting the servlet context initialization parameter org.apache.tomcat.websocket.textBufferSize to the desired value in bytes. 在项目Tomcat的web.xml(D:\Source\zwnewplatform\javasoftware\runtime\apache-tomcat-8.0.36\apache-tomcat-8.0.36\conf\web.xml)中添加如下配置（注意单位为byte）。 12345678&lt;context-param&gt; &lt;param-name&gt;org.apache.tomcat.websocket.textBufferSize&lt;/param-name&gt; &lt;param-value&gt;327680&lt;/param-value&gt;&lt;/context-param&gt;&lt;context-param&gt; &lt;param-name&gt;org.apache.tomcat.websocket.binaryBufferSize&lt;/param-name&gt; &lt;param-value&gt;327680&lt;/param-value&gt;&lt;/context-param&gt; 如果是Eclipse，则直接在Eclipse中配置，如下图所示：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[vim使用]]></title>
      <url>%2F2016%2F12%2F24%2Fvim-using%2F</url>
      <content type="text"><![CDATA[title: vim使用 替換（substitute）1:[range]s/pattern/string/[c,e,g,i] 命令 作用 range 指的是範圍，1,7 指從第一行至第七行，1,$ 指從第一行至最後一行，也就是整篇文章，也可以 % 代表。還記得嗎？ % 是目前編輯的文章，# 是前一次編輯的文章。 pattern 就是要被替換掉的字串，可以用 regexp 來表示。 tring 將pattern 由 string 所取代。 c confirm，每次替換前會詢問。 e 不顯示 error。 g globe，不詢問，整行替換。 i ignore 不分大小寫。 g 大概都是要加的，否則只會替換每一行的第一個符合字串。可以合起來用，如 cgi，表示不分大小寫，整行替換，替換前要詢問是否替換。 常用命令 命令 作用 shift + g(G) 跳转到文件末尾 :行号 在命令行模式下，冒号后输入数字，即可跳转到指定行中 yy 拷贝当前行 P/p(paste) 粘贴 $ 到一行的行尾 0 到一行的行首 在命令行模式下，输入斜杠，输入需要搜索的单词。匹配的单词会自动高亮，按n继续搜索下一个匹配结果，按#搜索上一个匹配结果。/pattern 向下搜索 n继续搜索下一个?pattern 向上搜索 #继续搜索上一个]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Ubuntu常用软件一键安装]]></title>
      <url>%2F2016%2F12%2F24%2Fubuntu-install-software-batch%2F</url>
      <content type="text"><![CDATA[进公司时同事帮助安装Ubuntu操作系统时，使用脚本安装软件，非常之快。2小时之内，包括操作系统和开发环境皆搭建完毕。或许我们都有搭建开发环境的经历，半天能够搞定还算是比较顺利的。中间还不能出什么幺蛾子，要是再遇到点奇奇怪怪的问题，耗费的时间更长了。这里也见识到了Linux的高效和方便。 仔细观察，他是许多软件用几行简单的命令皆搞定了，所以我想编写一个简单的Bash脚本，争取一个脚本搞定大部分软件安装工作。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux压缩包管理]]></title>
      <url>%2F2016%2F12%2F24%2Flinux-compress-package-management%2F</url>
      <content type="text"><![CDATA[tar.bz2解压tar.bz2的命令如下： 1tar -jxvf xx.tar.bz2 tar.gz1tar -xzvf example.tar.gz rar在Ubuntu中处理rar(WinRAR Compressed Archive)文件类型，安装相应包： 1sudo apt install -y unrar 输入如下命令解压缩： 1unrar x -r simple.rar]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[初识React]]></title>
      <url>%2F2016%2F12%2F23%2Freact-using%2F</url>
      <content type="text"><![CDATA[React 起源于 Facebook 的内部项目，因为该公司对市场上所有 JavaScript MVC 框架，都不满意，就决定自己写一套，用来架设Instagram的网站。 React 的设计思想极其独特，属于革命性创新，性能出众，代码逻辑却非常简单。从最早的UI引擎变成了一整套前后端通吃的 Web App 解决方案。衍生的 React Native 项目，目标更是宏伟，希望用写 Web App 的方式去写 Native App。如果能够实现，整个互联网行业都会被颠覆，因为同一组人只需要写一次 UI ，就能同时运行在服务器、浏览器和手机。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[wget使用]]></title>
      <url>%2F2016%2F12%2F23%2Fwget-using%2F</url>
      <content type="text"><![CDATA[GNU Wget is a free software package for retrieving files using HTTP, HTTPS and FTP, the most widely-used Internet protocols. Wget是直接在命令行中使用的，可以很容易的以脚本的方式调用。 下载文件 1wget url 下载文件(断点续传) 1wget -c url 抓取整站： 12#抓取整站wget -r -p -np -k http://www.xxx.com]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[我的疑问]]></title>
      <url>%2F2016%2F12%2F22%2Fquestion%2F</url>
      <content type="text"><![CDATA[在接口返回的Json中，在Eclipse调试查看返回的Json带小数点，使用curl请求不带小数点 循环引用(Cycle Reference)对象的序列化问题 Spring MVC中数据转对象的过程 MyBatis中使用#时，如何将参数也显示的显示在日志中 Java中注解实现原理 Swagger中如何添加请求头]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据库切分]]></title>
      <url>%2F2016%2F12%2F22%2Fdatabase-sharding%2F</url>
      <content type="text"><![CDATA[垂直切分(Vertical Sharding)Sharding的基本思想就要把一个数据库切分成多个部分放到不同的数据库(server)上，从而缓解单一数据库的性能问题。不太严格的讲，对于海量数据的数据库，如果是因为表多而数据多，这时候适合使用垂直切分，即把关系紧密（比如同一模块）的表切分出来放在一个server上。如果表并不多，但每张表的数据非常多，这时候适合水平切分，即把表的数据按某种规则（比如按ID散列）切分到多个数据库(server)上。当然，现实中更多是这两种情况混杂在一起，这时候需要根据实际情况做出选择，也可能会综合使用垂直与水平切分，从而将原有数据库切分成类似矩阵一样可以无限扩充的数据库(server)阵列。 垂直切分的最大特点就是规则简单，实施也更为方便，尤其适合各业务之间的耦合度非常低，相互影响很小，业务逻辑非常清晰的系统。在这种系统中，可以很容易做到将不同业务模块所使用的表分拆到不同的数据库中。根据不同的表来进行拆分，对应用程序的影响也更小，拆分规则也会比较简单清晰。 水平切分(Herizonal Sharding)水平切分于垂直切分相比，相对来说稍微复杂一些。因为要将同一个表中的不同数据拆分到不同的数据库中，对于应用程序来说，拆分规则本身就较根据表名来拆分更为复杂，后期的数据维护也会更为复杂一些。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java泛型实现原理]]></title>
      <url>%2F2016%2F12%2F21%2Fjava-generate-implement%2F</url>
      <content type="text"><![CDATA[泛型编程是一种通过参数化的方式将数据处理与数据类型解耦的技术，通过对数据类型施加约束（比如Java中的有界类型）来保证数据处理的正确性，又称参数类型或参数多态性。泛型最著名的应用就是容器，C++的STL、Java的Collection Framework。 不同的语言在实现泛型时采用的方式不同，C++的模板会在编译时根据参数类型的不同生成不同的代码，而Java的泛型是一种违反型，编译为字节码时参数类型会在代码中被擦除，单独记录在Class文件的attributes域内，而在使用泛型处做类型检查与类型转换。假设参数类型的占位符为T，擦除规则如下： 泛型擦除后变为Obecjt &lt;? extends A&gt;擦除后变为A &lt;？ super A&gt;擦除后变为Object 上述擦除规则叫做保留上界。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Ubuntu使用]]></title>
      <url>%2F2016%2F12%2F20%2Fubuntu-using%2F</url>
      <content type="text"><![CDATA[使用的Ubuntu版本为14.04 LTS和16.04 LTS版本。 软件安装在Ubuntu下，常用的软件安装一条命令即搞定,跟上常用的软件名称即可，真的是一条命令安装所有软件,非常方便： 123# 安装常用软件(16.04 LTS及以后使用apt命令)sudo apt install -y wget curl aria2 keepass2 putty vim tree unzip gitopenssh-server uget maven gradle shutter nginx mysql-server nodejs 字体在Ubuntu下一些漂亮的字体记录。 字体名字 说明 Latin Modern Mono 10 Regular 目前终端采用的这种字体 小技巧 长按Windows键会出现全局快捷键页面 设置截屏快捷键在安装好了Shutter之后，每次截取屏幕都需要打开Shutter，点击Selection按钮。其实可以配置截屏的快捷键，就像QQ的Ctrl + Alt + A一样。如下图所示： 设置Terminal默认路径一般情况下，在终端里有一个最高频的使用目录，希望打开终端时默认切换到此目录下。在Ubuntu里可以在Home目录下的bashrc~/.bashrc脚本里添加如下代码即可： 123if [ -d ~/document/blogs/jiangxiaoqiang/xiaoqiang-blog-source ];then cd ~/document/blogs/jiangxiaoqiang/xiaoqiang-blog-sourcefi 代码的含义是，如果当前目录不在~/document/blogs/jiangxiaoqiang/xiaoqiang-blog-source目录下，那么就切换到此目录。下一次打开终端时默认路径即为：~/document/blogs/jiangxiaoqiang/xiaoqiang-blog-source 快捷键 快捷键 作用 Ctrl + Alt + + 放大终端 Ctrl + - 缩小终端 Ctrl + Window + Up 最大化窗口，注意按下时按顺序按下相应键 Ctrl + Window + Down 最小化窗口，注意按下时按顺序按下相应键 Alt + F1 调出侧边栏(如果有设置自动隐藏的话) Ctrl + Window + D(Desktop) 显示桌面快捷键 Alt + F7 激活窗口移动功能 Alt(长按) 激活程序的菜单，即可使用程序的Alt快捷键组合了 Alt(短按) 激活本应用程序的搜索菜单，即可在当前应用程序中搜索了 Alt + E 在Google Chrome浏览器中，可以打开浏览器右侧的菜单 Window + 1 打开Home目录(还可以用Window组合键打开其他目录)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MyBatis使用]]></title>
      <url>%2F2016%2F12%2F20%2Fmybatis-using%2F</url>
      <content type="text"><![CDATA[Java的实体属性中，属性赋值时是区分大小写的，比如adog不能成功映射到aDog 获取新增数据的ID有时在添加一条记录后，需要获取添加记录的ID，进行其他业务逻辑。那么只需要在MyBatis中进行如下配置即可： 12345678&lt;!-- 新增--&gt;&lt;insert id="saveXzss" useGeneratedKeys="true" keyProperty="id" parameterType="system.data.model.CreditDissent"&gt; INSERT INTO TS_B_XYSS( APPLAYER_NAME) VALUES( #&#123;applayerName&#125;);&lt;/insert&gt; useGeneratedKeys=&quot;true&quot;表示开启返回自增ID。keyProperty=&quot;id&quot;表示返回主键的名字。在业务代码中接收： 12workflowMapper.create(autoGenerateFlow);int id=creditDissent.getId(); Mapper文件中遍历集合参数在遍历List时，直接使用如下的语句即可： 12345select * from table&lt;where&gt; id in &lt;foreach collection="ids" item="item" index="index"open="(" separator="," close=")"&gt;#&#123;item&#125;&lt;/foreach&gt;&lt;/where&gt; 如果是遍历List，可以采用如下写法： 12345&lt;select id=**"countByXdrShxym" **resultType=**"java.util.HashMap"**&gt; select XDR_SHXYM, count(*) AS total from TS_B_BLACKLIST where XDR_SHXYM in &lt;foreach item=**"item" **index=**"index" **collection=**"list" **open=**"('" **separator=**"','" **close=**"')"**&gt;$&#123;item&#125;&lt;/foreach&gt; group by XDR_SHXYM&lt;/select&gt; 其中open和close需要单独加上单引号，链接分隔符(separator)也需要添加引号。另外遍历的item会出现解析出来是？的情况，将字符#替换为$，不过有SQL注入的风险。Mybatis如果采用#{xxx}的形式设置参数，Mybatis会进行sql注入的过滤。如果采用的是${xxx}，Mybatis不会进行sql注入过滤，而是直接将参入的内容输出为sql语句。 特殊符号优势想在SQL中添加特殊符号，比如在字符串变量中添加一个单引号，如下语句所示。 123456789&lt;select id="findList" parameterType="map" resultMap="SummaryXzxk"&gt; SELECT * FROM TS_B_XZXK &lt;trim prefix="WHERE" prefixOverrides="AND|OR"&gt; &lt;if test = "xdr != null"&gt; AND XDR like CONCAT('%', $&#123;xdr&#125;, '%') &lt;/if&gt; &lt;/trim&gt;&lt;/select&gt; 在变量xdr上添加单引号，可以写成如下语句： 123456789&lt;select&gt; SELECT * FROM TS_B_XZXK &lt;trim prefix="WHERE" prefixOverrides="AND|OR"&gt; &lt;if test = "xdr != null"&gt; AND XDR like CONCAT('%', &amp;apos;$&#123;xdr&#125;&amp;apos;, '%') &lt;/if&gt; &lt;/trim&gt;&lt;/select&gt; 传入参数ＭyBatis的传入参数可以是各种Java的基本数据类型：包含int,String,Date等。基本数据类型作为传参，只能传入一个。通过#{参数名}即可获取传入的值，复杂数据类型：包含Java实体类、Map。通过#{属性名}或#{map的KeyName}即可获取传入的值，但是如果想传入一个collection时，可以使用mapper配置文件中的foreach语句。 MyBatis中${}与#{}的区别当使用#{parameterName}引入参数的时候，Mybatis会把这个参数认为是一个字符串，在拼接SQL的时候其实首先是一个问号（？），然后查询的时候，将参数引入到问号（？）之中。比如 select * from emp where name = #{name}，这样的一个SQL，解析以后是select * from emp where name = ?，由于是#{name}的方式引入，那么就将问号（？）替换成#{name}的值，比如传进一个字符串”yedward”，那么最终的查询SQL是select * from emp where name = &#39;yedward&#39;。可以理解为#可以进行预编译，进行类型匹配，而$不进行数据类型匹配。 当使用${parameterName}引入参数的时候，Mybatis会将这个参数直接拼到SQL中去，就没有上面那种问号（？）。 #将传入的数据都当成一个字符串，会对自动传入的数据加一个双引号。如：order by #user_id#，如果传入的值是111,那么解析成sql时的值为order by “111”, 如果传入的值是id，则解析成的sql为order by “id” $将传入的数据直接显示生成在sql中。如：order by $user_id$，如果传入的值是111,那么解析成sql时的值为order by user_id, 如果传入的值是id，则解析成的sql为order by id. #方式能够很大程度防止sql注入。 $方式无法防止Sql注入。 $方式一般用于传入数据库对象，例如传入表名. 注：MyBatis排序时使用order by 动态参数时用$而不是# 有时你只是想直接在SQL语句中插入一个不改变的字符串。比如，像ORDER BY，你可以这样来使用： ORDER BY ${columnName} 这里MyBatis不会修改或转义字符串。 总结：写一句SQL-例如：select * from user_role where user_code = &quot;100&quot;;这句话而言，需要写成 select * from ${tableName} where user_code = #{userCode}所以，$符是直接拼成sql的 ，#符则会以字符串的形式 与sql进行拼接。在直接拼接规则的时候可以用$，规则已经有了，只是传递参数进去，那么可以用#。 _parameter参数错误： 1There is no getter for property named &apos;moduleCode&apos; in &apos;class java.lang.String 将映射语句由： 12345&lt;select id="findByUUID" parameterType="java.lang.String" resultMap="Corporation"&gt; SELECT * FROM TS_F_CORPORATION WHERE ID = &amp;apos;$&#123;id&#125;&amp;apos;; &lt;/select&gt; 改为： 12345&lt;select id="findByUUID" parameterType="java.lang.String" resultMap="Corporation"&gt; SELECT * FROM TS_F_CORPORATION WHERE ID = &amp;apos;$&#123;_parameter&#125;&amp;apos;; &lt;/select&gt; 分页(Page)拦截器实现分页(Implement by Interceptor)由于不同的数据库厂商所提供的分页不同，例如ORACLE是子查询实现，MySQL是limit语句实现，所以在Mybatis中，默认的实现是基于逻辑分页(Logical Page)的。但是Mybatis支持拦截器(Interceptor),所以，我们可以根据不同的数据库，定制自己的数据库物理分页(Physical Page)逻辑。改变mybatis内部的分页行为，理论上只要把最终要执行的sql转变成对应的分页语句就行了。 Mybatis-PageHelper]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux安装输入法]]></title>
      <url>%2F2016%2F12%2F19%2Flinux-install-input-method%2F</url>
      <content type="text"><![CDATA[Ubuntu安装搜狗中文输入法]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[git使用]]></title>
      <url>%2F2016%2F12%2F19%2Fgit-using%2F</url>
      <content type="text"><![CDATA[Git是目前世界上最先进的分布式版本控制系统，没有之一。Git有什么特点？简单来说就是：高端大气上档次！逼格那是相当高，装逼必备神器。当你说用Git管理源码的时候，逼格瞬间高好几个档次。最近项目中在用，结合以前使用的经验，简单的总结记录在此。 常用命令列出当前的Git配置： 1git config --list 12#列出分支git branch 查看提交历史： 1git log --graph --all Git保存用户名和密码： 1git config credential.helper store Git查看Url： 1git remote -v 添加(Add)在使用Git的过程中，使用命令： 12# 添加当前目录的所有文件到暂存区git add . 没有添加某一部分子目录的文件，让我有点郁闷，最后切换到那个目录下手动添加修改后(Modified)的文件。按理说应该是添加当前目录下所有的文件和子文件夹啊，百思不得其解。原来是由于子目录下的远程库的地址与当前库不一致所导致,子目录下还是next主题的URL,而不是当前库的URL,将子目录下的远程库的地址修改为当前库的地址即可: 1git remote set-url --add origin https://github.com/jiangxiaoqiang/xiaoqiang-blog-source.git 其中,origin是远程库的名字.添加了新地址后，原来的地址还存在，可使用git config --list命令查看，同时，使用如下命令删除原来的旧的远程地址： 1git remote set-url --delete origin https://github.com/iissnan/hexo-theme-next 发起pull request新建(check out)分支v1_xiaoqiang： 1git checkout -b v1_xiaoqiang 参数 含义 -b create and checkout a new branch -B create/reset and checkout a branch 将新的分之push到origin分支中： 1git push origin v1_xiaoqiang 到GitLab页面里创建一个pull request即可。 关于Commit的建议 one thing one commit:在提交commit的时候尽量保证这个commit只做一件事情，比如实现某个功能或者修改了配置文件。 easy to read:清楚的表达这个commit做了什么。 cherry-pick:cherry-pick将Commit从一个分之拷贝到另一个分支，如果每个Commit包含的特性太多，那么就不能完美的pick出想要的特性。 code review:易于别人做code review。 123456789# 50-character subject line## 72-character wrapped longer description. This should answer:## * Why was this change necessary?# * How does it address the problem?# * Are there any side effects?## Include a link to the ticket, if any. 常见的修改类型如下： feat (feature) fix (bug fix) docs (documentation) style (formating, missing semi colons, …) refactor test (when adding missing tests) chore (maintain) 更新(update)采用git pull时，提示如下： 123456789101112hldev@hldev-100:~/hldata/backend/credit-system$ git pullUsername for &apos;http://dn6&apos;: xiaoqiang.jiangPassword for &apos;http://xiaoqiang.jiang@dn6&apos;:There is no tracking information for the current branch.Please specify which branch you want to merge with.See git-pull(1) for details. git pull &lt;remote&gt; &lt;branch&gt;If you wish to set tracking information for this branch you can do so with: git branch --set-upstream-to=origin/&lt;branch&gt; v1_xiaoqiang 指定本地dev分支与远程origin/dev分支的链接，根据提示，设置dev和origin/dev的链接： 1git branch --set-upstream-to origin/v1 v1_xiaoqiang 合并(merge)下面是使用过的一个合并步骤,切换到v1分支，v1分支是开发的主分支： 1git checkout v1 将远程分支更新获取并合并到v1分支： 1git pull origin v1 查看当前分支： 1git branch 假如当前分支为名称：v1_xiaoqiang,将v1分支合并到当前分支： 1234#切换到v1_xiaoqiang分支git checkout v1_xiaoqiang#将v1分支合并到当前分支（v1_xiaoqiang）git merge v1 关闭文件对比(合并)工具后，辅助文件都会自动删除，但同时会生成一个test.txt.orig的文件，orig是original的缩写，内容是解决冲突前的冲突现场。默认该.orig文件可能不会自动删除，需要手动删除。 撤销合并使用git show命令查看父编号(parent number): 1git show s868dfa3e5267578eeec73947f334320740885f56 显示的内容如下： 123456commit 868dfa3e5267578eeec73947f334320740885f56Merge: 0995c73 6d3ef06Author: jiangxiaoqiang &lt;jiangtingqiang@gmail.com&gt;Date: Fri Jan 6 09:30:15 2017 +0800 Merge branch &apos;v1&apos; of http://dn6/backend/credit-system into v1 The first one is the first parent, the second one is the second parent。取消合并： 1git revert -m 1 HEAD 1就是1，表示0995c73对应的父来源，2表示6d3ef06对应的父来源。要撤销的那条merge线的编号,HEAD表示merge前的版本号。 Pull操作git pull的作用是取回远程主机某个分支的更新，再与本地指定分之合并。 1git pull &lt;远程主机名&gt; &lt;远程分支名&gt;：&lt;本地分支名&gt; 比如取回origin主机的next分支，与本地的master分支合并，写成下面这样： 1git pull origin next:master 远程分支与当前分支合并，冒号后的内容可以省略： 1git pull origin next 等同于先做fetch，再做merge。 12git fetch origingit merge origin/next 储藏(Stash)“‘储藏”“可以获取你工作目录的中间状态——也就是你修改过的被追踪的文件和暂存的变更——并将它保存到一个未完结变更的堆栈中，随时可以重新应用。往堆栈推送一个新的储藏，只要运行： 1git stash 查看储藏： 1git stash list 输出的内容如下： 12hldev@hldev-100:~/hldata/backend/credit-system$ git stash liststash@&#123;0&#125;: WIP on v1_xiaoqiang: ac284e4 refactor:根据属性名获取属性值优化 其中WIP代表:Work In Progress，应用储藏： 1git stash apply Your branch and ‘origin/master’ have divergedIf you absolutely sure this is your case then you can force Git to push your changes: 12#force push changesgit push origin master -f Changes not staged for commit1git stash Please, commit your changes or stash them before you can merge出现这个问题的原因是其他人修改了文件并提交到版本库中去了，而你本地也修改了xxx.php，这时候你进行git pull操作就好出现冲突了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring Boot特点]]></title>
      <url>%2F2016%2F12%2F19%2Fspring-boot%2F</url>
      <content type="text"><![CDATA[Spring Boot特点2013年12月12日，Spring发布了4.0版本。Spring的4.0版本可以说是一个重大的更新，其全面支持Java8，并且对Groovy语言也有良好的支持。另外引入了非常多的新项目，比如Spring boot，Spring Cloud，Spring WebSocket等。Spring由于其繁琐的配置，一度被人成为“配置地狱”，各种XML、Annotation配置，让人眼花缭乱，而且如果出错了也很难找出原因。Spring Boot项目就是为了解决配置繁琐的问题，最大化的实现convention over configuration(约定大于配置)。熟悉Ruby On Rails（ROR框架的程序员都知道，借助于ROR的脚手架工具只需简单的几步即可建立起一个Web应用程序。而Spring Boot就相当于Java平台上的ROR。 Spring Boot的特性有以下几条： 创建独立Spring应用程序 嵌入式Tomcat，Jetty容器，无需部署WAR包 简化Maven及Gradle配置 尽可能的自动化配置Spring 直接植入产品环境下的实用功能，比如度量指标、健康检查及扩展配置等 无需代码生成及XML配置 Spring Boot是这几年微服务概念流行后，Spring开发的一套快速开发Spring应用的框架。它本身并不提供Spring框架的核心特性以及扩展功能，只是用于快速、敏捷地开发新一代基于Spring框架的应用程序。也就是说，它并不是用来替代Spring的解决方案，而是和Spring框架紧密结合用于提升Spring开发者体验的工具。同时它集成了大量常用的第三方库配置（例如Jackson, JDBC, Mongo, Redis, Mail等等），Spring Boot应用中这些第三方库几乎可以零配置的开箱即用（out-of-the-box），大部分的Spring Boot应用都只需要非常少量的配置代码，开发者能够更加专注于业务逻辑。 @EnableAutoConfiguration@EnableAutoConfiguration这个注解告诉Spring Boot根据添加的jar依赖猜测你想如何配置Spring。spring-boot-starter-web添加了Tomcat和Spring MVC，所以auto-configuration将假定你正在开发一个web应用并相应地对Spring进行默认设置。 参考资料： spring -boot和spring-mvc是两个平行的框架么? 值得使用的Spring Boot]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java concurrent包]]></title>
      <url>%2F2016%2F12%2F18%2Fjava-concurrent-package%2F</url>
      <content type="text"><![CDATA[简介 java.util.concurrent 包含许多线程安全、测试良好、高性能的并发构建块。不客气地说，创建 java.util.concurrent 的目的就是要实现 Collection 框架对数据结构所执行的并发操作。通过提供一组可靠的、高性能并发构建块，开发人员可以提高并发类的线程安全、可伸缩性、性能、可读性和可靠性。 通常所说的concurrent包基本有3个package组成 java.util.concurrent：提供大部分关于并发的接口和类，如BlockingQueue,Callable,ConcurrentHashMap,ExecutorService, Semaphore等 java.util.concurrent.atomic：提供所有原子操作的类， 如AtomicInteger, AtomicLong等； java.util.concurrent.locks:提供锁相关的类, 如Lock, ReentrantLock, ReadWriteLock, Condition等； 参考文章： Java concurrent包介绍及使用 java.util.concurrent介绍]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java并发编程：Callable、Future和FutureTask]]></title>
      <url>%2F2016%2F12%2F18%2Fjava-concurrent-program%2F</url>
      <content type="text"><![CDATA[创建线程的2种方式，一种是直接继承Thread，另外一种就是实现Runnable接口。这2种方式都有一个缺陷就是：在执行完任务之后无法获取执行结果。如果需要获取执行结果，就必须通过共享变量或者使用线程通信的方式来达到效果，这样使用起来就比较麻烦。而自从Java 1.5开始，就提供了Callable和Future，通过它们可以在任务执行完毕之后得到任务执行结果。 Callable与RunnableFuture多线程开发中有几个痛点： 主线程如何正确的关闭异步线程？ 主线程怎么知道异步线程是否执行完成？ Future提供了三种功能： 1）判断任务是否完成； 2）能够中断任务； 3）能够获取任务执行结果。 FutureTask参考文章： Java - 使用Future模式进行多线程编程 Java线程(七)：Callable和Future]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hexo使用总结]]></title>
      <url>%2F2016%2F12%2F18%2Fhexo-using%2F</url>
      <content type="text"><![CDATA[Hexo是一个简洁、高效的博客框架。使用Markdown编写文章，快速渲染成博客静态网页。 安装安装Hexo： 1npm install hexo --save 安装hexo-cli: 1npm install hexo-cli -g 仅仅安装了Hexo还不可以在浏览器中查看效果，还需要安装Hexo Server模块： 1npm install hexo-server --save 安装之后就可以使用hexo server命令了。在Ubuntu下安装会出现获取文件失败的错误，可能需要多次尝试。提示：WARN No layout: index.html时，一般是由于主题文件不存在导致，安装相应的主题即可。切换到源码目录下： 1git clone https://github.com/iissnan/hexo-theme-next themes/next 访问http://localhost:4000页面空白，提示can not get /，此时需要在源码目录下运行一下npm install命令即可。 安装搜索模块安装搜索模块可自定义站内搜索，在源码目录下执行如下命令。 1npm install hexo-generator-searchdb --save 在使用搜索模块时，有时会出现如下错误： 12345678910111213141516171819202122232425hldev@hldev-100:~/summerize/xiaoqiang-blog-source$ hexo sERROR Plugin load failed: hexo-generator-searchdbError: Cannot find module &apos;../highlight_alias.json&apos; at Function.Module._resolveFilename (module.js:455:15) at Function.Module._load (module.js:403:25) at Module.require (module.js:483:17) at require (internal/module.js:20:19) at Object.&lt;anonymous&gt; (/home/hldev/summerize/xiaoqiang-blog-source/node_modules/hexo-generator-searchdb/node_modules/hexo-util/lib/highlight.js:6:13) at Module._compile (module.js:556:32) at Object.Module._extensions..js (module.js:565:10) at Module.load (module.js:473:32) at tryModuleLoad (module.js:432:12) at Function.Module._load (module.js:424:3) at Module.require (module.js:483:17) at require (internal/module.js:20:19) at Object.&lt;anonymous&gt; (/home/hldev/summerize/xiaoqiang-blog-source/node_modules/hexo-generator-searchdb/node_modules/hexo-util/lib/index.js:8:21) at Module._compile (module.js:556:32) at Object.Module._extensions..js (module.js:565:10) at Module.load (module.js:473:32) at tryModuleLoad (module.js:432:12) at Function.Module._load (module.js:424:3) at Module.require (module.js:483:17) at require (internal/module.js:20:19) at Object.&lt;anonymous&gt; (/home/hldev/summerize/xiaoqiang-blog-source/node_modules/hexo-generator-searchdb/lib/generator.js:4:12) at Module._compile (module.js:556:32) 解决此问题，切换到node_modules目录下，删除hexo-generator-searchdb模块，重新安装hexo-generator-searchdb模块即可。 安装Next主题next主题是Hexo下一款非常简洁美观的主题，切换到themes目录下，执行如下命令： 1git clone https://github.com/iissnan/hexo-theme-next themes/next 设置菜单可以自己添加相应菜单，可以在一级菜单上添加一个书(Book)或者其他模块。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ArrayList线程安全]]></title>
      <url>%2F2016%2F12%2F17%2Farraylist-thread-safe%2F</url>
      <content type="text"><![CDATA[Using Collections.synchronizedList() method Using thread-safe variant of ArrayList: CopyOnWriteArrayList CopyOnWriteArrayListCopyOnWriteArrayList是java.util.concurrent包中的一个List的实现类。CopyOnWrite的意思是在写时拷贝，也就是如果需要对CopyOnWriteArrayList的内容进行改变，首先会拷贝一份新的List并且在新的List上进行修改，最后将原List的引用指向新的List。使用CopyOnWriteArrayList可以线程安全地遍历，因为如果另外一个线程在遍历的时候修改List的话，实际上会拷贝出一个新的List上修改，而不影响当前正在被遍历的List。 How do I make my ArrayList Thread-Safe? Another approach to problem in Java?]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java异常分类]]></title>
      <url>%2F2016%2F12%2F17%2Fjava-exception%2F</url>
      <content type="text"><![CDATA[Java异常关系 Throwable是所有异常的根，java.lang.Throwable Error是错误，java.lang.Error Exception是异常，java.lang.Exception Java异常的分类如下图所示： 图形绘制(graphviz)源码： 12345678910111213digraph&#123;size=&quot;8,8&quot;; edge[fontname=&quot;FangSong&quot;]; node[shape=&quot;Mrecord&quot;,fontname=&quot;FangSong&quot;,size=&quot;20,20&quot;,fontsize=12,color=&quot;skyblue&quot;,style=&quot;filled&quot;] Throwable -&gt; Error; Throwable -&gt; Exception; Exception -&gt; RuntimeException; Exception -&gt; CheckedException; Error -&gt; VirtualMachineError; Error -&gt; AWTError; VirtualMachineError -&gt; StackOverFlowError; VirtualMachineError -&gt; OutOfMemoryError;&#125; 编译命令： 1dot -Tjpg -Gdpi=1024 java-exception.dot -o java-exception.jpg RuntimeException是那些可能在 Java 虚拟机正常运行期间抛出的异常的超类。可能在执行方法期间抛出但未被捕获的RuntimeException的任何子类都无需在throws子句中进行声明。除了runtimeException以外的异常，都属于checkedException，它们都在java.lang库内部定义。Java编译器要求程序必须捕获或声明抛出这种异常。一个方法必须通过throws语句在方法的声明部分说明它可能抛出但并未捕获的所有checkedException。 Java.lang.ClassNotFoundException Java.lang.CloneNotSupportedException Java.lang.IllegalAccessException Java.lang.InterruptedException Java.lang.NoSuchFieldException Java.lang.NoSuchMetodException]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Fedora常用软件集合]]></title>
      <url>%2F2016%2F12%2F16%2Ffedora-software%2F</url>
      <content type="text"><![CDATA[使用Fedora 24有一段时间了，这里列出了平时总结的在Fedora上可以使用的一些软件，完成平时的任务。如果您也要尝试使用Fedora，或许可以节省不少时间。列出的软件大多数是跨平台(Crossplatform)的，可以运行在其他Linux发行版上，以及Mac OS和Windows上。 网络(Network)geary(邮件客户端)Geary is an email application built around conversations, for the GNOME 3 desktop. It allows you to read, find and send email with a straightforward, modern interface. Conversations allow you to read a complete discussion without having to find and click from message to message. httrack网页下载与缓存。 wget1wget -c http://dl.zeroturnaround.com/idea/jr-ide-intellij-6.5.0_13-16.zip -c参数表示断点续传。–continue： Continue getting a partially-downloaded file. This is useful when you want to finish up a download started by a previous instance of Wget, or by another program. uGetuGet is the #1 Open Source Download Manager app.uGet is a multi-platform app for Linux, BSD, Android &amp; Windows (XP - 8) 编程开发（Development）PyCharm(EDU)PyCharm的教育版是开源免费的，学习Python。 NodeJS本博客就是基于NodeJS生成，虽然不是很懂NodeJS，但是感觉很牛逼的样子。 nmap端口扫描。 Fiddler for LinuxApache TomcatOpenVPNStarDicthtophtop是实时显示当前系统运行情况的，不同于ps、pstree、pidof、vmsta等这些快照工具，快照工具只能显示命令执行前一秒系统的情况。htop 对关键信息实行高亮显示，对于我们用肉眼查看进程信息的情况相当友好。 列名 含义 PID 进程id PPID 父进程id RUSER Real user name UID 进程所有者的用户id TTY 启动进程的终端名。不是从终端启动的进程则显示为 ? PR 优先级 NI nice值。负值表示高优先级，正值表示低优先级 P 最后使用的CPU，仅在多CPU环境下有意义 %CPU 上次更新到现在的CPU时间占用百分比 VIRT(Virtual) 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES TIME 进程使用的CPU时间总计，单位秒 SHR(Share) 共享内存大小，单位kb TIME+ 进程使用的CPU时间总计，单位1/100秒 %MEM 进程使用的物理内存百分比 SWAP 进程使用的虚拟内存中，被换出的大小，单位kb S(Status) 进程状态（D=不可中断的睡眠状态，R=运行，S=睡眠，T=跟踪/停止，Z=僵尸进程） RES 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA CODE 可执行代码占用的物理内存大小，单位kb DATA 可执行代码以外的部分(数据段+栈)占用的物理内存大小，单位kb nFLT 页面错误次数 nDRT 最后一次写入到现在，被修改过的页面数 WCHAN 若该进程在睡眠，则显示睡眠中的系统函数名 Flags 任务标志，参考 sched.h 默认情况下仅显示比较重要的 PID、USER、PR、NI、VIRT、RES、SHR、S、%CPU、%MEM、TIME+、COMMAND 列。可以通过下面的快捷键来更改显示内容 上图说明计算机有8个内核，8GB的内存，8GB的交换空间(Swap Space)。 Visual Studio Code跨平台文本编辑器。 EclipseJava开发IDE。 7zip7-Zip is open source software. Most of the source code is under the GNU LGPL license. The unRAR code is under a mixed license: GNU LGPL + unRAR restrictions.7-Zip works in Windows 10 / 8 / 7 / Vista / XP / 2012 / 2008 / 2003 / 2000 / NT. There is a port of the command line version to Linux/Unix.安装7zip用于解压.7z结尾的文件： 1dnf install -y p7zip-plugins 使用如下命令解压文件： 17z e Netty权威指南\ PDF完整版带目录书签.7z 输出的结果如下： 1234567891011121314151617181920217-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,8 CPUs Intel(R) Core(TM) i7-3632QM CPU @ 2.20GHz (306A9),ASM,AES-NI)Scanning the drive for archives:1 file, 61246559 bytes (59 MiB)Extracting archive: Netty权威指南 PDF完整版带目录书签.7z--Path = Netty权威指南 PDF完整版带目录书签.7zType = 7zPhysical Size = 61246559Headers Size = 306Method = LZMA2:24Solid = +Blocks = 1Everything is Ok Files: 3Size: 86833121Compressed: 61246559 影音娱乐(Media&amp;Entertainment)VLC Media PlayerElectronic-Wechat基于微信网页版微信，结合Electron开发的基于Linux的微信客户端。 ShutterLinux下的截图工具。 Intellij IdeaLinux下Java开发必备工具。 FileZillaSlackWiresharkTransmissionLinux下torrent下载工具。 DelugeLinux下下载工具，支持磁力链接(Magnic Link)下载。 MPlayerGraphizPuttyGoogle ChromeGoogle Chrome浏览器的好处之一就是，只要登陆Google的账号后，它会把你的搜索记录、插件、Cookie等同步到云端，这样回家之后可以接着处理工作上遗留的问题。比如在办公室研究某个问题到一半，回家打开浏览器搜索关键字会快速出现办公时研究的关键字，相当方便。还有你的历史浏览记录，在不同的设备上使用即时同步，不同设备（PC、Laptop）、平台（Windows和Fedora）使用习惯无缝对接。 FireFoxGoogle EarthLantern如果平时需要用Google搜索一些资料，Lantern是一款必不可少的工具。 xx-net如果平时需要用Google搜索一些资料，xx-net是一款必不可少的工具。 AtomaMuleTexStudioNutstore跨平台的云同步软件。 Haroopad跨平台Markdown编辑器。 工具(Tools)KeePassKeePass是一款密码管理软件，我的密码有100多个。平时没法全部都记住的，所以只需要记住一个KeePass密码即可。KeePass可以同时安装在Windows、Linux上。 albertAlbert is a desktop agnostic launcher, inspired by the ease of use of OSX’ Alfred launcher. Its goals are usability and beauty, performance and extensability. It is written in C++ and based on the Qt framework.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[zookeeper总结]]></title>
      <url>%2F2016%2F12%2F16%2Fzookeeper-using%2F</url>
      <content type="text"><![CDATA[Zookeeper工作原理Zookeeper的核心是原子广播（Atomic Broadcast），这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab(Zookeeper Atomic Broadcast)协议。Zab协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。 为了保证事务的顺序一致性，zookeeper采用了递增的事务id号（zxid）来标识事务。所有的提议（proposal）都在被提出的时候加上了zxid(ZooKeeper transaction id)。实现中zxid是一个64位的数字，它高32位是epoch用来标识leader关系是否改变，每次一个leader被选出来，它都会有一个新的epoch，标识当前属于那个leader的统治时期。低32位用于递增计数。 每个Server在工作过程中有三种状态： LOOKING：当前Server不知道leader是谁，正在搜寻 LEADING：当前Server即为选举出来的leader FOLLOWING：leader已经选举出来，当前Server与之同步 参考文章： 简要分析ZooKeeper基本原理及安装部署]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Maven使用]]></title>
      <url>%2F2016%2F12%2F15%2Fmaven-using%2F</url>
      <content type="text"><![CDATA[Maven生命周期1process-resources, compile, process-test-resources, test-compile, test and package Maven发布包]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring JPA]]></title>
      <url>%2F2016%2F12%2F14%2Fspring-jpa%2F</url>
      <content type="text"><![CDATA[JPA(Java Persistant API)。Spring Data JPA, part of the larger Spring Data family, makes it easy to easily implement JPA based repositories. This module deals with enhanced support for JPA based data access layers. It makes it easier to build Spring-powered applications that use data access technologies.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring注入方式]]></title>
      <url>%2F2016%2F12%2F14%2Fspring-injection-type%2F</url>
      <content type="text"><![CDATA[Set注入这是最简单的注入方式，假设有一个SpringAction，类中需要实例化一个SpringDao对象，那么就可以定义一个private的SpringDao成员变量，然后创建SpringDao的set方法（这是ioc的注入入口）。 构造器注入这种方式的注入是指带有参数的构造函数注入，看下面的例子，我创建了两个成员变量SpringDao和User，但是并未设置对象的set方法，所以就不能支持第一种注入方式，这里的注入方式是在SpringAction的构造函数中注入，也就是说在创建SpringAction对象时要将SpringDao和User两个参数值传进来。 静态工厂的方法注入实例工厂的方法注入Spring四种依赖注入方式]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring MVC数据绑定流程]]></title>
      <url>%2F2016%2F12%2F14%2Fspring-mvc-databinding%2F</url>
      <content type="text"><![CDATA[SpringMVC主框架将ServletRequest对象及目标方法的入参实例传递给WebDataBinderFactory实例，以创建DataBinder实例对象。DataBinder调用装配在SpringMVC上下文中的ConversionService组件进行数据类型转换、数据格式化工作。将Servlet中的请求信息填充到入参对象中。调用Validator组件对已经绑定了请求消息的入参对象进行数据合法性校验，并最终生成数据绑定结果。 到DispatcherServlet客户端的请求到达服务端，首先到DispatcherServlet(org.springframework.web.servlet)的doService方法中。DispatcherServlet通过HandlerMapping获得HandlerExecutionChain，然后获得HandlerAdapter。请求方法参数的处理、响应返回值的处理，分别是HandlerMethodArgumentResolver和HandlerMethodReturnValueHandler，这两个接口都是Spring3.1版本之后加入的。 执行请求1234567891011121314public Object invokeForRequest(NativeWebRequest request, ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception &#123; Object[] args = getMethodArgumentValues(request, mavContainer, providedArgs); if (logger.isTraceEnabled()) &#123; logger.trace("Invoking '" + ClassUtils.getQualifiedMethodName(getMethod(), getBeanType()) + "' with arguments " + Arrays.toString(args)); &#125; Object returnValue = doInvoke(args); if (logger.isTraceEnabled()) &#123; logger.trace("Method [" + ClassUtils.getQualifiedMethodName(getMethod(), getBeanType()) + "] returned [" + returnValue + "]"); &#125; return returnValue;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring MVC返回Json]]></title>
      <url>%2F2016%2F12%2F14%2Fspring-mvc-return-json%2F</url>
      <content type="text"><![CDATA[Spring MVC返回 json字符串的方式有很多种方法: 直接 PrintWriter 输出 使用 JSP 视图 使用Spring内置的支持 PrintWriter输出123456789//返回给前台一个字符串 @RequestMapping(params = "method=getJson1") public void getJson(@RequestParam("userid") String userid,@RequestHeader("Accept-Encoding") String encoding,HttpServletRequest request,PrintWriter printWriter) &#123; System.out.println("通过注解在参数中取值 "+userid); System.out.println("通过@RequestHeader获得的encoding "+encoding); printWriter.write("&#123;key,1&#125;"); printWriter.flush(); printWriter.close(); &#125; @ResponseBody通过@ResponseBody直接返回对象,Spring MVC会自动把对象转化成Json,需要其他配置支持。@ResponseBody这个注解就是使用消息转换机制，最终通过json的转换器转换成json数据的。SpringMVC使用消息转换器实现请求报文和对象、对象和响应报文之间的自动转换。可以使用@RequestBody和@ResponseBody两个注解，分别完成请求报文到对象和对象到响应报文的转换，底层这种灵活的消息转换机制，就是Spring3.x中新引入的HttpMessageConverter即消息转换器机制。 1、开启 2、Jackson library 对应的jar必须加入到工程中 3、方法的返回值必须添加 @ResponseBody 12345678//把返回结果解析成json串返回到前台 @RequestMapping(params = "method=json") public @ResponseBody User passValue(HttpServletRequest request) &#123; User user = new User(); user.setUser("aaaa"); user.setPass("asfd"); return user; &#125; 注意：在使用@ResponseBody 返回json的时候，方法参数中一定不能他添加 PrintWriter printWriter。从流中，只能读取到原始的字符串报文，同样，我们往输出流中，也只能写原始的字符。而在java世界中，处理业务逻辑，都是以一个个有业务意义的对象为处理维度的，那么在报文到达SpringMVC和从SpringMVC出去，都存在一个字符串到java对象的阻抗问题。这一过程，不可能由开发者手工转换。我们知道，在Struts2中，采用了OGNL（Object-Graph Navigation Language）来应对这个问题，而在SpringMVC中，它是HttpMessageConverter机制。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Interview面试题]]></title>
      <url>%2F2016%2F12%2F14%2Finterview-question%2F</url>
      <content type="text"><![CDATA[平时并未注意许多细节和实质性的东西，许多原理的方面非常欠缺，面试后总结如下。 Java中如何确保不再使用的资源被正确的关闭在jdk1.6之前，应该把close()放在finally块中，以确保资源的正确释放。如果使用jdk1.7以上的版本，推荐使用try-with-resources语句。 类的构造顺序12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * @Author: jiangxiaoqiang * Created by 12/17/16 on 8:26 PM. */public class ClassConstructSquence &#123; &#123; System.out.println(&quot;Class ClassConstructSquence code block&quot;); &#125; static &#123; System.out.println(&quot;class ClassConstructSquence static code block&quot;); &#125; public ClassConstructSquence()&#123; this(null); System.out.println(&quot;class ClassConstructSquence non-param code block&quot;); &#125; public ClassConstructSquence(String a)&#123; System.out.println(&quot;class ClassConstructSquence param code block&quot;); &#125;&#125;public class ClassConstructSquenceB extends ClassConstructSquence &#123; &#123; System.out.println(&quot;Class ClassConstructSquenceB code block&quot;); &#125; static &#123; System.out.println(&quot;class ClassConstructSquenceB static code block&quot;); &#125; public ClassConstructSquenceB() &#123; this(null); System.out.println(&quot;class ClassConstructSquenceB non-param code block&quot;); &#125; public ClassConstructSquenceB(String a) &#123; System.out.println(&quot;class ClassConstructSquenceB param code block&quot;); &#125; public static void main(String[] args) &#123; ClassConstructSquenceB b = new ClassConstructSquenceB(); &#125;&#125; 输出结果如下： 12345678class ClassConstructSquence static code blockclass ClassConstructSquenceB static code blockClass ClassConstructSquence code blockclass ClassConstructSquence param code blockclass ClassConstructSquence non-param code blockClass ClassConstructSquenceB code blockclass ClassConstructSquenceB param code blockclass ClassConstructSquenceB non-param code block 可以总结Java类中的构造顺序如下： 12345678910父类--静态变量父类--静态初始化块子类--静态变量子类--静态初始化块父类--变量父类--初始化块父类--构造器子类--变量子类--初始化块子类--构造器 运算符优先级1234567public class Boy &#123; public static void main(String[] args) &#123; int i = 12; //输出为-120 System.out.print(i += i -= i *= i); &#125;&#125; 计算过程参考这里： Why the output is -120? #### synchronized是对类的当前实例进行加锁，防止其他线程同时访问该类的该实例的所有synchronized块，注意这里是“类的当前实例”，类的两个不同实例就没有这种约束了。那么static synchronized恰好就是要控制类的所有实例的访问了，static synchronized是限制线程同时访问jvm中该类的所有实例同时访问对应的代码快。实际上，在类中某方法或某代码块中有synchronized，那么在生成一个该类实例后，该类也就有一个监视快，放置线程并发访问改实例synchronized保护快，而static synchronized则是所有该类的实例公用一个监视快了，也就是两个的区别了,也就是synchronized相当于this.synchronized，而static synchronized相当于Something.synchronized。 synchronized针对同一个实例不能访问，针对不同的实例可以同时访问。static synchronized针对所有的实例均不能同时访问。synchronized本来就是修饰方法的，后来引申出synchronized修饰代码块，只是为了可以更精确的控制冲突限制的访问区域，使得表现更加高效率。synchronized方法只能锁定现阶段的对象，而synchronized区块可以锁定指定的对象，指定的对象直接跟在synchronized()括号之后。此外，synchronized关键字是不能继承的，也就是说，基类的方法synchronized f(){} 在继承类中并不自动是synchronized f(){}，而是变成了f(){}。继承类需要你显式的指定它的某个方法为synchronized方法。还有synchronized不能被继承，继承时子类的覆盖方法必须显示定义成synchronized。 除了方法前用synchronized关键字，synchronized关键字还可以用于方法中的某个区块中，表示只对这个区块的资源实行互斥访问。用法是: synchronized(object){/区块/}，它的作用域是object对象。当一个线程执行时，将object对象锁住，另一个线程就不能执行对应的块。synchronized方法实际上等同于用一个synchronized块包住方法中的所有语句，然后在synchronized块的括号中传入this关键字。当然，如果是静态方法，需要锁定的则是class对象。可能一个方法中只有几行代码会涉及到线程同步问题，所以synchronized块比synchronized方法更加细粒度地控制了多个线程的访问，只有synchronized块中的内容不能同时被多个线程所访问，方法中的其他语句仍然可以同时被多个线程所访问（包括synchronized块之前的和之后的）。 Java值传递12345678910111213141516public class JavaPramPassing &#123; String str = new String("good"); char[] ch = &#123;'a', 'b', 'c'&#125;; public static void main(String[] args) &#123; JavaPramPassing javaPramPassing = new JavaPramPassing(); javaPramPassing.change(javaPramPassing.str,javaPramPassing.ch); System.out.println(javaPramPassing.str);//good System.out.println(javaPramPassing.ch);//gbc &#125; public void change(String str, char[] ch) &#123; str = "test ok"; ch[0] = 'g'; &#125;&#125; Java参数，不管是原始类型还是引用类型，传递的都是副本(有另外一种说法是传值，但是说传副本更好理解吧，传值通常是相对传址而言)。要特殊考虑String，以及Integer、Double等几个基本类型包装类，它们都是immutable类型，因为没有提供自身修改的函数，每次操作都是新生成一个对象，所以要特殊对待，可以认为是和基本数据类型相似，传值(Pass by Value)操作。 如果参数类型是原始类型，那么传过来的就是这个参数的一个副本，也就是这个原始参数的值，这个跟之前所谈的传值是一样的。如果在函数中改变了副本的 值不会改变原始的值. 如果参数类型是引用类型，那么传过来的就是这个引用参数的副本，这个副本存放的是参数的地址。如果在函数中没有改变这个副本的地址，而是改变了地址中的 值，那么在函数内的改变会影响到传入的参数。如果在函数中改变了副本的地址，如new一个，那么副本就指向了一个新的地址，此时传入的参数还是指向原来的 地址，所以不会改变参数的值。 volatile关键字作用在Java内存模型中，有Main Memory，每个线程也有自己的Memory (例如寄存器)。为了性能，一个线程会在自己的Memory中保持要访问的变量的副本。这样就会出现同一个变量在某个瞬间，在一个线程的Memory中的值可能与另外一个线程Memory中的值，或者Main Memory中的值不一致的情况。 一个变量声明为volatile，就意味着这个变量是随时会被其他线程修改的，因此不能将它cache在线程memory中。Volatile 变量具有 synchronized 的可见性特性，但是不具备原子性。这就是说线程能够自动发现 volatile 变量的最新值。出于简易性或可伸缩性的考虑，倾向于使用 volatile 变量而不是锁。当使用 volatile 变量而非锁时，某些习惯用法（idiom）更加易于编码和阅读。此外，volatile 变量不会像锁那样造成线程阻塞，因此也很少造成可伸缩性问题。在某些情况下，如果读操作远远大于写操作，volatile 变量还可以提供优于锁的性能优势。 状态标志（Status Flags） 一次性安全发布（One-time Safe Publication） 在缺乏同步的情况下，可能会遇到某个对象引用的更新值（由另一个线程写入）和该对象状态的旧值同时存在。这就是造成著名的双重检查锁定（double-checked-locking）问题的根源，其中对象引用在没有同步的情况下进行读操作，产生的问题是您可能会看到一个更新的引用，但是仍然会通过该引用看到不完全构造的对象。 独立观察（independent observation） 安全使用 volatile 的另一种简单模式是：定期 “发布” 观察结果供程序内部使用。【例如】假设有一种环境传感器能够感觉环境温度。一个后台线程可能会每隔几秒读取一次该传感器，并更新包含当前文档的 volatile 变量。然后，其他线程可以读取这个变量，从而随时能够看到最新的温度值。使用该模式的另一种应用程序就是收集程序的统计信息。 “volatile bean” 模式 开销较低的“读－写锁”策略]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[抽象类和接口区别]]></title>
      <url>%2F2016%2F12%2F14%2Fabstract-and-interface-diff%2F</url>
      <content type="text"><![CDATA[区别 参数 抽象类 接口 默认的方法实现 它可以有默认的方法实现 接口完全是抽象的。它根本不存在方法的实现 实现 子类使用extends关键字来继承抽象类。如果子类不是抽象类的话，它需要提供抽象类中所有声明的方法的实现。 子类使用关键字implements来实现接口。它需要提供接口中所有声明的方法的实现 构造器 抽象类可以有构造器 接口不能有构造器 与正常Java类的区别 除了你不能实例化抽象类之外，它和普通Java类没有任何区别 接口是完全不同的类型 访问修饰符 抽象方法可以有public、protected和default这些修饰符 接口方法默认修饰符是public。你不可以使用其它修饰符。 main方法 抽象方法可以有main方法并且我们可以运行它 接口没有main方法，因此我们不能运行它。 多继承 抽象方法可以继承一个类和实现多个接口 接口只可以继承一个或多个其它接口 速度 它比接口速度要快 接口是稍微有点慢的，因为它需要时间去寻找在类中实现的方法。 添加新方法 如果你往抽象类中添加新的方法，你可以给它提供默认的实现。因此你不需要改变你现在的代码。 如果你往接口中添加方法，那么你必须改变实现该接口的类。 什么时候使用抽象类和接口如果你拥有一些方法并且想让它们中的一些有默认实现，那么使用抽象类吧。如果你想实现多重继承，那么你必须使用接口。由于Java不支持多继承，子类不能够继承多个类，但可以实现多个接口。因此你就可以使用接口来解决它。如果基本功能在不断改变，那么就需要使用抽象类。如果不断改变基本功能并且使用接口，那么就需要改变所有实现了该接口的类。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[算法]]></title>
      <url>%2F2016%2F12%2F14%2Falgorithm%2F</url>
      <content type="text"><![CDATA[汉诺塔算法12345678910111213static void hannoi(int n, String from, String buffer, String to) &#123; if (n == 1) &#123; System.out.println("Move disk " + n + " from " + from + " to " + to); &#125; else &#123; hannoi(n - 1, from, to, buffer); System.out.println("Move disk " + n + " from " + from + " to " + to); hannoi(n - 1, buffer, from, to); &#125;&#125;public static void main(String[] args) &#123; hannoi(3, "A", "B", "C");&#125; 快速排序快速排序（英语：Quicksort），又称划分交换排序（partition-exchange sort），一种排序算法，最早由东尼·霍尔提出。在平均状况下，排序n个项目要Ο(n log n)次比较。在最坏状况下则需要Ο(n2)次比较，但这种状况并不常见。事实上，快速排序通常明显比其他Ο(n log n)算法更快，因为它的内部循环（inner loop）可以在大部分的架构上很有效率地被实现出来。 快速排序使用分治法（Divide and conquer）策略来把一个序列（list）分为两个子序列（sub-lists）。步骤为： 从数列中挑出一个元素，称为”基准”（pivot） 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。递归的最底部情形，是数列的大小是零或一，也就是永远都已经被排序好了。虽然一直递归下去，但是这个算法总会结束，因为在每次的迭代（iteration）中，它至少会把一个元素摆到它最后的位置去。 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class QuickSort &#123; static int[] arr; private static void swap(int x, int y) &#123; int temp = arr[x]; arr[x] = arr[y]; arr[y] = temp; &#125; private static void quick_sort_recursive(int start, int end) &#123; if (start &gt;= end) return; int mid = arr[end]; int left = start, right = end - 1; while (left &lt; right) &#123; while (arr[left] &lt; mid &amp;&amp; left &lt; right) left++; while (arr[right] &gt;= mid &amp;&amp; left &lt; right) right--; swap(left, right); &#125; if (arr[left] &gt;= arr[end]) &#123; swap(left, end); &#125; else &#123; left++; &#125; quick_sort_recursive(start, left - 1); quick_sort_recursive(left + 1, end); &#125; public static void sort(int[] arrin) &#123; arr = arrin; quick_sort_recursive(0, arr.length - 1); &#125; public static void main(String[] args) &#123; int[] array = &#123;1, 5, 3, 2&#125;; sort(array); for (int i = 0; i &lt; array.length; i++) &#123; int sortedElement = array[i]; System.out.println(sortedElement); &#125; &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java内存模型(Memory Model)]]></title>
      <url>%2F2016%2F12%2F14%2Fjava-memory-model%2F</url>
      <content type="text"><![CDATA[JVM管理的内存区域(Memory Area)包括以下几个区域： 栈区(Stack Area)栈分为Java虚拟机栈和本地方法栈 Java虚拟机栈(Java Virtual Machine Stack)重点是Java虚拟机栈，它是线程私有的，生命周期与线程相同。每个方法执行都会创建一个栈帧，用于存放局部变量表，操作栈，动态链接，方法出口等。每个方法从被调用，直到被执行完。对应着一个栈帧在虚拟机中从入栈到出栈的过程。通常说的栈就是指局部变量表部分，存放编译期间可知的8种基本数据类型，及对象引用和指令地址。局部变量表是在编译期间完成分配，当进入一个方法时，这个栈中的局部变量分配内存大小是确定的。会有两种异常StackOverFlowError和 OutOfMemoneyError。当线程请求栈深度大于虚拟机所允许的深度就会抛出StackOverFlowError错误；虚拟机栈动态扩展，当扩展无法申请到足够的内存空间时候，抛出OutOfMemoneyError。 本地方法栈(Local Methond Stack)本地方法栈 为虚拟机使用到本地方法服务（native） 堆区(Heap Area) 堆被所有线程共享区域，在虚拟机启动时创建，唯一目的存放对象实例。 堆区是gc的主要区域，通常情况下分为两个区块年轻代和年老代。更细一点年轻代又分为Eden区最要放新创建对象，From survivor 和 To survivor 保存gc后幸存下的对象，默认情况下各自占比 8:1:1。不过很多文章介绍分为3个区块，把方法区算着为永久代。这大概是基于Hotspot虚拟机划分， 然后比如IBM j9就不存在永久代概论。不管怎么分区，都是存放对象实例。会有异 常OutOfMemoneyError 方法区(Method Area)被所有线程共享区域，用于存放已被虚拟机加载的类信息，常量，静态变量等数据。被Java虚拟机描述为堆的一个逻辑部分。习惯是也叫它永久代（permanment generation），垃圾回收很少光顾这个区域，不过也是需要回收的，主要针对常量池回收，类型卸载。常量池用于存放编译期生成的各种字节码和符号引用，常量池具有一定的动态性，里面可以存放编译期生成的常量；运行期间的常量也可以添加进入常量池中，比如string的intern()方法。 程序计数器当前线程所执行的行号指示器。通过改变计数器的值来确定下一条指令，比如循环，分支，跳转，异常处理，线程恢复等都是依赖计数器来完成。Java虚拟机多线程是通过线程轮流切换并分配处理器执行时间的方式实现的。为了线程切换能恢复到正确的位置，每条线程都需要一个独立的程序计数器，所以它是线程私有的。唯一一块Java虚拟机没有规定任何OutofMemoryError的区块。jvm分区大致就这个块，具体里面还有很多细节，及其各个模块工作的算法都很复杂，这里只是对分区进行简单介绍，掌握一些基本的知识点。 参考文章： JVM内存模型及分区]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Netty原理]]></title>
      <url>%2F2016%2F12%2F13%2Fnetty-theory%2F</url>
      <content type="text"><![CDATA[Netty是一个高性能 事件驱动的异步的非堵塞的IO(NIO)框架，用于建立TCP等底层的连接，基于Netty可以建立高性能的Http服务器。支持HTTP、 WebSocket 、Protobuf、 Binary TCP |和UDP，Netty已经被很多高性能项目作为其Socket底层基础，如HornetQ Infinispan Vert.xPlay Framework Finangle和 Cassandra。其竞争对手是：Apache MINA和 Grizzly。 作为当前最流行的NIO框架，Netty在互联网领域、大数据分布式计算领域、游戏行业、通信行业等获得了广泛的应用，一些业界著名的开源组件也基于Netty的NIO框架构建。 异步非阻塞通信在IO编程过程中，当需要同时处理多个客户端接入请求时，可以利用多线程或者IO多路复用技术进行处理。IO多路复用技术通过把多个IO的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。与传统的多线程/多进程模型比，I/O多路复用的最大优势是系统开销小，系统不需要创建新的额外进程或者线程，也不需要维护这些进程和线程的运行，降低了系统的维护工作量，节省了系统资源。 JDK1.4提供了对非阻塞IO（NIO）的支持，JDK1.5_update10版本使用epoll替代了传统的select/poll，极大的提升了NIO通信的性能。 零拷贝Netty的“零拷贝”主要体现在如下三个方面： 1) Netty的接收和发送ByteBuffer采用DIRECT BUFFERS，使用堆外直接内存进行Socket读写，不需要进行字节缓冲区的二次拷贝。如果使用传统的堆内存（HEAP BUFFERS）进行Socket读写，JVM会将堆内存Buffer拷贝一份到直接内存中，然后才写入Socket中。相比于堆外直接内存，消息在发送过程中多了一次缓冲区的内存拷贝。 2) Netty提供了组合Buffer对象，可以聚合多个ByteBuffer对象，用户可以像操作一个Buffer那样方便的对组合Buffer进行操作，避免了传统通过内存拷贝的方式将几个小Buffer合并成一个大的Buffer。 3) Netty的文件传输采用了transferTo方法，它可以直接将文件缓冲区的数据发送到目标Channel，避免了传统通过循环write方式导致的内存拷贝问题。 内存池高效的Reactor线程模型无锁化的串行设计理念高效的并发编程参考文章： Netty系列之Netty高性能之道]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis与Memcached区别]]></title>
      <url>%2F2016%2F12%2F13%2Fredis-memcached-diff%2F</url>
      <content type="text"><![CDATA[传统MySQL+ Memcached架构遇到的问题实际MySQL是适合进行海量数据存储的，通过Memcached将热点数据加载到cache，加速访问，很多公司都曾经使用过这样的架构，但随着业务数据量的不断增加，和访问量的持续增长，我们遇到了很多问题： MySQL需要不断进行拆库拆表，Memcached也需不断跟着扩容，扩容和维护工作占据大量开发时间。 Memcached与MySQL数据库数据一致性问题。 Memcached数据命中率低或down机，大量访问直接穿透到DB，MySQL无法支撑。 跨机房cache同步问题。 众多NoSQL百花齐放，如何选择 最近几年，业界不断涌现出很多各种各样的NoSQL产品（MongoDB、Cassandra、CouchDB、Redis、Hadoop HBase、MemcacheDB），那么如何才能正确地使用好这些产品，最大化地发挥其长处，是我们需要深入研究和思考的问题，实际归根结底最重要的是了解这些产品的定位，并且了解到每款产品的tradeoffs，在实际应用中做到扬长避短，总体上这些NoSQL主要用于解决以下几种问题。 少量数据存储，高速读写访问。此类产品通过数据全部in-momery 的方式来保证高速访问，同时提供数据落地的功能，实际这正是Redis最主要的适用场景。 海量数据存储，分布式系统支持，数据一致性保证，方便的集群节点添加/删除。 这方面最具代表性的是dynamo和bigtable 2篇论文所阐述的思路。前者是一个完全无中心的设计，节点之间通过gossip方式传递集群信息，数据保证最终一致性，后者是一个中心化的方案设计，通过类似一个分布式锁服务来保证强一致性,数据写入先写内存和redo log，然后定期compat归并到磁盘上，将随机写优化为顺序写，提高写入性能。 Schema free，auto-sharding等。比如目前常见的一些文档数据库都是支持schema-free的，直接存储json格式数据，并且支持auto-sharding等功能，比如MongoDB。 面对这些不同类型的NoSQL产品,我们需要根据我们的业务场景选择最合适的产品。 Redis适用场景，如何正确的使用 前面已经分析过，Redis最适合所有数据in-momory的场景，虽然Redis也提供持久化功能，但实际更多的是一个disk-backed的功能，跟传统意义上的持久化有比较大的差别，那么可能大家就会有疑问，似乎Redis更像一个加强版的Memcached，那么何时使用Memcached,何时使用Redis呢? 如果简单地比较Redis与Memcached的区别，大多数都会得到以下观点： Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Redis支持数据的备份，即master-slave模式的数据备份。 Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。 抛开这些，可以深入到Redis内部构造去观察更加本质的区别，理解Redis的设计。 在Redis中，并不是所有的数据都一直存储在内存中的。这是和Memcached相比一个最大的区别。Redis只会缓存所有的 key的信息，如果Redis发现内存的使用量超过了某一个阀值，将触发swap的操作，Redis根据“swappability = age*log(size_in_memory)”计 算出哪些key对应的value需要swap到磁盘。然后再将这些key对应的value持久化到磁盘中，同时在内存中清除。这种特性使得Redis可以 保持超过其机器本身内存大小的数据。当然，机器本身的内存必须要能够保持所有的key，毕竟这些数据是不会进行swap操作的。同时由于Redis将内存 中的数据swap到磁盘中的时候，提供服务的主线程和进行swap操作的子线程会共享这部分内存，所以如果更新需要swap的数据，Redis将阻塞这个 操作，直到子线程完成swap操作后才可以进行修改。 使用Redis特有内存模型前后的情况对比： 12345VM off: 300k keys, 4096 bytes values: 1.3G usedVM on: 300k keys, 4096 bytes values: 73M usedVM off: 1 million keys, 256 bytes values: 430.12M usedVM on: 1 million keys, 256 bytes values: 160.09M usedVM on: 1 million keys, values as large as you want, still: 160.09M used 当 从Redis中读取数据的时候，如果读取的key对应的value不在内存中，那么Redis就需要从swap文件中加载相应数据，然后再返回给请求方。 这里就存在一个I/O线程池的问题。在默认的情况下，Redis会出现阻塞，即完成所有的swap文件加载后才会相应。这种策略在客户端的数量较小，进行 批量操作的时候比较合适。但是如果将Redis应用在一个大型的网站应用程序中，这显然是无法满足大并发的情况的。所以Redis运行我们设置I/O线程 池的大小，对需要从swap文件中加载相应数据的读取请求进行并发操作，减少阻塞的时间。 如果希望在海量数据的环境中使用好Redis，我相信理解Redis的内存设计和阻塞的情况是不可缺少的。 补充的知识点：memcached和redis的比较1 网络IO模型 Memcached是多线程，非阻塞IO复用的网络模型，分为监听主线程和worker子线程，监听线程监听网络连接，接受请求后，将连接描述字pipe 传递给worker线程，进行读写IO, 网络层使用libevent封装的事件库，多线程模型可以发挥多核作用，但是引入了cache coherency和锁的问题，比如，Memcached最常用的stats 命令，实际Memcached所有操作都要对这个全局变量加锁，进行计数等工作，带来了性能损耗。 (Memcached网络IO模型) Redis使用单线程的IO复用模型，自己封装了一个简单的AeEvent事件处理框架，主要实现了epoll、kqueue和select，对于单纯只有IO操作来说，单线程可以将速度优势发挥到最大，但是Redis也提供了一些简单的计算功能，比如排序、聚合等，对于这些操作，单线程模型实际会严重影响整体吞吐量，CPU计算过程中，整个IO调度都是被阻塞住的。 2.内存管理方面 Memcached使用预分配的内存池的方式，使用slab和大小不同的chunk来管理内存，Item根据大小选择合适的chunk存储，内存池的方式可以省去申请/释放内存的开销，并且能减小内存碎片产生，但这种方式也会带来一定程度上的空间浪费，并且在内存仍然有很大空间时，新的数据也可能会被剔除，原因可以参考Timyang的文章：http://timyang.net/data/Memcached-lru-evictions/ Redis使用现场申请内存的方式来存储数据，并且很少使用free-list等方式来优化内存分配，会在一定程度上存在内存碎片，Redis跟据存储命令参数，会把带过期时间的数据单独存放在一起，并把它们称为临时数据，非临时数据是永远不会被剔除的，即便物理内存不够，导致swap也不会剔除任何非临时数据(但会尝试剔除部分临时数据)，这点上Redis更适合作为存储而不是cache。 3.数据一致性问题 Memcached提供了cas命令，可以保证多个并发访问操作同一份数据的一致性问题。 Redis没有提供cas 命令，并不能保证这点，不过Redis提供了事务的功能，可以保证一串 命令的原子性，中间不会被任何操作打断。 4.存储方式及其它方面 Memcached基本只支持简单的key-value存储，不支持枚举，不支持持久化和复制等功能 Redis除key/value之外，还支持list,set,sorted set,hash等众多数据结构，提供了KEYS 进行枚举操作，但不能在线上使用，如果需要枚举线上数据，Redis提供了工具可以直接扫描其dump文件，枚举出所有数据，Redis还同时提供了持久化和复制等功能。 5.关于不同语言的客户端支持 在不同语言的客户端方面，Memcached和Redis都有丰富的第三方客户端可供选择，不过因为Memcached发展的时间更久一些，目前看在客户端支持方面，Memcached的很多客户端更加成熟稳定，而Redis由于其协议本身就比Memcached复杂，加上作者不断增加新的功能等，对应第三方客户端跟进速度可能会赶不上，有时可能需要自己在第三方客户端基础上做些修改才能更好的使用。 根据以上比较不难看出，当我们不希望数据被踢出，或者需要除key/value之外的更多数据类型时，或者需要落地功能时，使用Redis比使用Memcached更合适。 关于Redis的一些周边功能 Redis除了作为存储之外还提供了一些其它方面的功能，比如聚合计算、pubsub、scripting等，对于此类功能需要了解其实现原理，清楚地了解到它的局限性后，才能正确的使用，比如pubsub功能，这个实际是没有任何持久化支持的，消费方连接闪断或重连之间过来的消息是会全部丢失的，又比如聚合计算和scripting等功能受Redis单线程模型所限，是不可能达到很高的吞吐量的，需要谨慎使用。 总的来说Redis作者是一位非常勤奋的开发者，可以经常看到作者在尝试着各种不同的新鲜想法和思路，针对这些方面的功能就要求我们需要深入了解后再使用。 总结： Redis使用最佳方式是全部数据in-memory。 Redis更多场景是作为Memcached的替代者来使用。 当需要除key/value之外的更多数据类型支持时，使用Redis更合适。 当存储的数据不能被剔除时，使用Redis更合适。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis集群(Redis Cluster)]]></title>
      <url>%2F2016%2F12%2F13%2Fredis-cluster%2F</url>
      <content type="text"><![CDATA[在非官方集群解决方案中，物理上把数据“分片”（sharding）存储在多个Redis实例，一般情况下，每一“片”是一个Redis实例。 客户端分片代理分片TwemproxyTwemproxy是一种代理分片机制，由Twitter开源。Twemproxy作为代理，可接受来自多个程序的访问，按照路由规则，转发给后台的各个Redis服务器，再原路返回。 这个方案顺理成章地解决了单个Redis实例承载能力的问题。当然，Twemproxy本身也是单点，需要用Keepalived做高可用方案。 我想很多人都应该感谢Twemproxy，这么些年来，应用范围最广、稳定性最高、最久经考验的分布式中间件，应该就是它了。只是，他还有诸多不方便之处。 Twemproxy最大的痛点在于，无法平滑地扩容/缩容。 这样导致运维同学非常痛苦：业务量突增，需增加Redis服务器；业务量萎缩，需要减少Redis服务器。但对Twemproxy而言，基本上都很难操作（那是一种锥心的、纠结的痛……）。 或者说，Twemproxy更加像服务器端静态sharding。有时为了规避业务量突增导致的扩容需求，甚至被迫新开一个基于Twemproxy的Redis集群。 Twemproxy另一个痛点是，运维不友好，甚至没有控制面板。 Redis ClusterCodis Cluster参考资料： 高效运维最佳实践（03）：Redis集群技术及Codis实践]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[HBase主键设计]]></title>
      <url>%2F2016%2F12%2F13%2Fhbase-rowkey-design%2F</url>
      <content type="text"><![CDATA[HBase是一个分布式的、面向列的数据库，它和一般关系型数据库的最大区别是：HBase很适合于存储非结构化的数据，还有就是它基于列的而不是基于行的模式。 既然HBase是采用KeyValue的列存储，那Rowkey就是KeyValue的Key了，表示唯一一行。Rowkey也是一段二进制码流，最大长度为64KB，内容可以由使用的用户自定义。数据加载时，一般也是根据Rowkey的二进制序由小到大进行的。 HBase是根据Rowkey来进行检索的，系统通过找到某个Rowkey (或者某个 Rowkey 范围)所在的Region，然后将查询数据的请求路由到该Region获取数据。HBase的检索支持3种方式： （1） 通过单个Rowkey访问，即按照某个Rowkey键值进行get操作，这样获取唯一一条记录； （2） 通过Rowkey的range进行scan，即通过设置startRowKey和endRowKey，在这个范围内进行扫描。这样可以按指定的条件获取一批记录； （3） 全表扫描，即直接扫描整张表中所有行记录。 HBASE按单个Rowkey检索的效率是很高的，耗时在1毫秒以下，每秒钟可获取1000~2000条记录，不过非key列的查询很慢。 Rowkey长度原则Rowkey是一个二进制码流，Rowkey的长度被很多开发者建议说设计在10~100个字节，不过建议是越短越好，不要超过16个字节。 原因如下： （1）数据的持久化文件HFile中是按照KeyValue存储的，如果Rowkey过长比如100个字节，1000万列数据光Rowkey就要占用100*1000万=10亿个字节，将近1G数据，这会极大影响HFile的存储效率； （2）MemStore将缓存部分数据到内存，如果Rowkey字段过长内存的有效利用率会降低，系统将无法缓存更多的数据，这会降低检索效率。因此Rowkey的字节长度越短越好。 （3）目前操作系统是都是64位系统，内存8字节对齐。控制在16个字节，8字节的整数倍利用操作系统的最佳特性。 Rowkey散列原则如果Rowkey是按时间戳的方式递增，不要将时间放在二进制码的前面，建议将Rowkey的高位作为散列字段，由程序循环生成，低位放时间字段，这样将提高数据均衡分布在每个Regionserver实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息将产生所有新数据都在一个RegionServer上堆积的热点现象，这样在做数据检索的时候负载将会集中在个别RegionServer，降低查询效率。 Rowkey唯一原则必须在设计上保证其唯一性。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据库索引原理]]></title>
      <url>%2F2016%2F12%2F13%2Fdatabase-index-theory%2F</url>
      <content type="text"><![CDATA[BTree索引B-Tree有许多变种，其中最常见的是B+Tree，例如MySQL就普遍使用B+Tree实现其索引结构。 与B-Tree相比，B+Tree有以下不同点： 每个节点的指针上限为2d而不是2d+1。 内节点不存储data，只存储key；叶子节点不存储指针。 哈希索引全文索引参考资料： MySQL索引背后的数据结构及算法原理 MySQL索引原理及慢查询优化 你知道数据库索引的工作原理吗？]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[消息中间件(Message-Oriented Middleware)]]></title>
      <url>%2F2016%2F12%2F13%2Fmessage-oriented-middleware%2F</url>
      <content type="text"><![CDATA[分布式系统中,广泛运用消息中间件（Message-Oriented Middleware）进行系统间的数据交换,便于异步解耦。 Message Queue 的通讯模式点对点通讯：点对点方式是最为传统和常见的通讯方式，它支持一对一、一对多、多对多、多对一等多种配置方式，支持树状、网状等多种拓扑结构。多点广播：MQ 适用于不同类型的应用。其中重要的，也是正在发展中的是”多点广播”应用，即能够将消息发送到多个目标站点 (Destination List)。可以使用一条 MQ 指令将单一消息发送到多个目标站点，并确保为每一站点可靠地提供信息。MQ 不仅提供了多点广播的功能，而且还拥有智能消息分发功能，在将一条消息发送到同一系统上的多个用户时，MQ 将消息的一个复制版本和该系统上接收者的名单发送到目标 MQ 系统。目标 MQ 系统在本地复制这些消息，并将它们发送到名单上的队列，从而尽可能减少网络的传输量。发布/订阅 (Publish/Subscribe) 模式：发布/订阅功能使消息的分发可以突破目的队列地理指向的限制，使消息按照特定的主题甚至内容进行分发，用户或应用程序可以根据主题或内容接收到所需要的消息。发布/订阅功能使得发送者和接收者之间的耦合关系变得更为松散，发送者不必关心接收者的目的地址，而接收者也不必关心消息的发送地址，而只是根据消息的主题进行消息的收发。群集 (Cluster)：为了简化点对点通讯模式中的系统配置，MQ 提供 Cluster(群集) 的解决方案。群集类似于一个域 (Domain)，群集内部的队列管理器之间通讯时，不需要两两之间建立消息通道，而是采用群集 (Cluster) 通道与其它成员通讯，从而大大简化了系统配置。此外，群集中的队列管理器之间能够自动进行负载均衡，当某一队列管理器出现故障时，其它队列管理器可以接管它的工作，从而大大提高系统的高可靠性。 Apache KafkaKafka是LinkedIn开源的分布式发布-订阅消息系统，目前归属于Apache定级项目。Kafka主要特点是基于Pull的模式来处理消息消费，追求高吞吐量，一开始的目的就是用于日志收集和传输。0.8版本开始支持复制，不支持事务，对消息的重复、丢失、错误没有严格要求，适合产生大量数据的互联网服务的数据收集业务。Kafka 设计中将每一个主题分区当作一个具有顺序排列的日志。同处于一个分区中的消息都被设置了一个唯一的偏移量。Kafka 只会保持跟踪未读消息，一旦消息被置为已读状态，Kafka 就不会再去管理它了。Kafka 的生产者负责在消息队列中对生产出来的消息保证一定时间的占有，消费者负责追踪每一个主题 (可以理解为一个日志通道) 的消息并及时获取它们。基于这样的设计，Kafka 可以在消息队列中保存大量的开销很小的数据，并且支持大量的消费者订阅。 RocketMQRabbitMQ是使用Erlang语言开发的开源消息队列系统，基于AMQP(Advanced Message Queuing Protocol)协议来实现。AMQP的主要特征是面向消息、队列、路由（包括点对点和发布/订阅）、可靠性、安全。AMQP协议更多用在企业系统内，对数据一致性、稳定性和可靠性要求很高的场景，对性能和吞吐量的要求还在其次。 RabbitMQRocketMQ是阿里开源的消息中间件，它是纯Java开发，具有高吞吐量、高可用性、适合大规模分布式系统应用的特点。RocketMQ思路起源于Kafka，但并不是Kafka的一个Copy，它对消息的可靠传输及事务性做了优化，目前在阿里集团被广泛应用于交易、充值、流计算、消息推送、日志流式处理、binglog分发等场景。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring动态代理(Dynamic Proxy)]]></title>
      <url>%2F2016%2F12%2F13%2Fspring-dynamic-proxy%2F</url>
      <content type="text"><![CDATA[cglib（Code Generlize Library）动态代理CGLIB（Code Generlize Library）代理是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的所有方法，所以该类或方法不能声明称final的。首先使用Maven引入CGLIB的依赖包。 12345&lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt;&lt;/dependency&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package dolphin.test;/** * @Author: jiangxiaoqiang * Created by 12/13/16 on 5:23 PM. */public class BookCglibImpl &#123; public void addBook() &#123; System.out.println("增加图书Cglib..."); &#125;&#125;package dolphin.test;import net.sf.cglib.proxy.Enhancer;import net.sf.cglib.proxy.MethodInterceptor;import net.sf.cglib.proxy.MethodProxy;import java.lang.reflect.Method;/** * @Author: jiangxiaoqiang * Created by 12/13/16 on 5:24 PM. */public class BookCglibPoxy implements MethodInterceptor &#123; private Object target; /** * 创建代理对象 * * @param target * @return */ public Object getInstance(Object target) &#123; this.target = target; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(this.target.getClass()); // 回调方法 enhancer.setCallback(this); // 创建代理对象 return enhancer.create(); &#125; // 回调方法 public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; System.out.println("事物开始"); proxy.invokeSuper(obj, args); System.out.println("事物结束"); return null; &#125;&#125;package dolphin.test;/** * @Author: jiangxiaoqiang * Created by 12/13/16 on 5:43 PM. */public class TestCglib &#123; public static void main(String[] args) &#123; BookCglibPoxy cglib=new BookCglibPoxy(); BookCglibImpl bookCglib=(BookCglibImpl)cglib.getInstance(new BookCglibImpl()); bookCglib.addBook(); &#125;&#125; JDK的动态代理JDK动态代理的两个核心分别是InvocationHandler和Proxy，此时代理对象和目标对象实现了相同的接口，目标对象作为代理对象的一个属性，具体接口实现中，可以在调用目标对象相应方法前后加上其他业务处理逻辑。代理模式在实际使用时需要指定具体的目标对象，如果为每个类都添加一个代理类的话，会导致类很多，同时如果不知道具体类的话，怎样实现代理模式呢？这就引出动态代理。JDK动态代理只能针对实现了接口的类生成代理。而不能实现接口的类就不能实现JDK的动态代理，cglib是针对类来实现代理的，他的原理是对指定的目标类生成一个子类，并覆盖其中方法实现增强，但因为采用的是继承，所以不能对final修饰的类进行代理。JDK动态代理中包含一个类和一个接口： InvocationHandler接口123public interface InvocationHandler &#123; public Object invoke(Object proxy,Method method,Object[] args) throws Throwable;&#125; 参数说明： Object proxy：指被代理的对象。 Method method：要调用的方法 Object[] args：方法调用时所需要的参数 可以将InvocationHandler接口的子类想象成一个代理的最终操作类，替换掉ProxySubject。 Proxy类Proxy类是专门完成代理的操作类，可以通过此类为一个或多个接口动态地生成实现类，此类提供了如下的操作方法： 1public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces,InvocationHandler h) throws IllegalArgumentException 参数说明： ClassLoader loader：类加载器 Class&lt;?&gt;[] interfaces：得到全部的接口 InvocationHandler h：得到InvocationHandler接口的子类实例 Ps:类加载器在Proxy类中的newProxyInstance（）方法中需要一个ClassLoader类的实例，ClassLoader实际上对应的是类加载器，在Java中主要有一下三种类加载器; Booststrap ClassLoader：此加载器采用C++编写，一般开发中是看不到的； Extendsion ClassLoader：用来进行扩展类的加载，一般对应的是jre\lib\ext目录中的类; AppClassLoader：(默认)加载classpath指定的类，是最常使用的是一种加载器。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package dolphin.test;/** * @Author: jiangxiaoqiang * Created by 12/13/16 on 5:08 PM. */public interface Book &#123; public void addBook();&#125;package dolphin.test;/** * @Author: jiangxiaoqiang * Created by 12/13/16 on 5:09 PM. */public class BookImpl implements Book&#123; public void addBook() &#123; System.out.println("增加书"); &#125;&#125;package dolphin.test;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;/** * @Author: jiangxiaoqiang * Created by 12/13/16 on 5:10 PM. */public class BookProxy implements InvocationHandler&#123; private Object target; /** * 绑定委托对象并返回一个代理类 * @param target * @return */ public Object bind(Object target) &#123; this.target = target; //取得代理对象 return Proxy.newProxyInstance(target.getClass().getClassLoader(), target.getClass().getInterfaces(), this); //要绑定接口(这是一个缺陷，cglib弥补了这一缺陷) &#125; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object result=null; System.out.println("事物开始"); //执行方法 result=method.invoke(target, args); System.out.println("事物结束"); return result; &#125;&#125;package dolphin.test;/** * @Author: jiangxiaoqiang * Created by 12/13/16 on 5:12 PM. */public class TestProxy &#123; public static void main(String[] args) &#123; BookProxy bookProxy = new BookProxy(); Book book = (Book) bookProxy.bind(new BookImpl()); book.addBook(); &#125;&#125; 区别 JDK动态代理只能对实现了接口的类生成代理，而不能针对类 。 CGLIB是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法 。 因为是继承，所以该类或方法最好不要声明成final ，final可以阻止继承和多态。 final 所修饰的数据具有“终态”的特征，表示“最终的”意思： final 修饰的类不能被继承。 final 修饰的方法不能被子类重写。 final 修饰的变量（成员变量或局部变量）即成为常量，只能赋值一次。 final 修饰的成员变量必须在声明的同时赋值，如果在声明的时候没有赋值，那么只有 一次赋值的机会，而且只能在构造方法中显式赋值，然后才能使用。 final 修饰的局部变量可以只声明不赋值，然后再进行一次性的赋值。 动态代理的应用AOP（Aspect-Oriented Programming，面向切面编程），AOP包括切面（aspect）、通知（advice）、连接点（joinpoint），实现方式就是通过对目标对象的代理在连接点前后加入通知，完成统一的切面操作。在Spring中，有如下规则： 如果目标对象实现了接口，在默认情况下会采用JDK的动态代理实现AOP 如果目标对象实现了接口，也可以强制使用CGLIB生成代理实现AOP 如果目标对象没有实现接口，必须引入CGLIB，Spring会在JDK的动态代理和CGLIB代理之间进行切换。 实现AOP的技术，主要分为两大类： 一是采用动态代理技术(Dynamic Proxy)，利用截取消息的方式，对该消息进行装饰，以取代原有对象行为的执行； 二是采用静态织入(Static Weaving)的方式，引入特定的语法创建“方面”，从而使得编译器可以在编译期间织入有关“方面”的代码。 Spring提供了两种方式来生成代理对象: JDKProxy和Cglib，具体使用哪种方式生成由AopProxyFactory根据AdvisedSupport对象的配置来决定。 默认的策略是如果目标类是接口，则使用JDK动态代理技术，如果目标对象没有实现接口，则默认会采用CGLIB代理。如果目标对象实现了接口，可以强制使用CGLIB实现代理: 12&lt;!-- 添加CGLIB库，并在spring配置中加入如下配置 --&gt;&lt;aop:aspectj-autoproxy proxy-target-class="true"/&gt;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java容器类总结]]></title>
      <url>%2F2016%2F12%2F12%2Fjava-container%2F</url>
      <content type="text"><![CDATA[Java容器类是java提供的工具包，包含了常用的数据结构：集合、链表、队列、栈、数组、映射等。Java容器类库定义了两个不同概念的容器，Collection和Map。 Collection接口Collection的定义如下： 1public interface Collection&lt;E&gt; extends Iterable&lt;E&gt; &#123;&#125; 它是一个接口，是高度抽象出来的集合，它包含了集合的基本操作：添加、删除、清空、遍历(读取)、是否为空、获取大小、是否保护某元素等等。在Java中所有实现了Collection接口的类都必须提供两套标准的构造函数，一个是无参，用于创建一个空的Collection，一个是带有Collection参数的有参构造函数，用于创建一个新的Collection，这个新的Collection与传入进来的Collection具备相同的元素。 Graphiz绘制图形源码： 12345678910111213141516171819202122digraph&#123;size=&quot;8,8&quot;; edge[fontname=&quot;FangSong&quot;]; node[shape=&quot;Mrecord&quot;,fontname=&quot;FangSong&quot;,size=&quot;20,20&quot;,fontsize=12,color=&quot;skyblue&quot;,style=&quot;filled&quot;] Iterator -&gt; &quot;Collection&lt;&lt;Interface&gt;&gt;&quot;; Iterator -&gt; &quot;Map&lt;&lt;Interface&gt;&gt;&quot;; &quot;Collection&lt;&lt;Interface&gt;&gt;&quot; -&gt; &quot;Set&lt;&lt;Interface&gt;&gt;&quot;; &quot;Set&lt;&lt;Interface&gt;&gt;&quot; -&gt; HashSet; &quot;Set&lt;&lt;Interface&gt;&gt;&quot; -&gt; LinkedHashSet; &quot;Set&lt;&lt;Interface&gt;&gt;&quot; -&gt; TreeSet; &quot;Collection&lt;&lt;Interface&gt;&gt;&quot; -&gt; &quot;List&lt;&lt;Interface&gt;&gt;&quot;; &quot;List&lt;&lt;Interface&gt;&gt;&quot; -&gt; ArrayList; &quot;List&lt;&lt;Interface&gt;&gt;&quot; -&gt; Vector; &quot;List&lt;&lt;Interface&gt;&gt;&quot; -&gt; LinkedList; &quot;Collection&lt;&lt;Interface&gt;&gt;&quot; -&gt; &quot;Queue&lt;&lt;Interface&gt;&gt;&quot;; &quot;Queue&lt;&lt;Interface&gt;&gt;&quot; -&gt; LinkedList; &quot;Queue&lt;&lt;Interface&gt;&gt;&quot; -&gt; PriorityQueue; &quot;Map&lt;&lt;Interface&gt;&gt;&quot; -&gt; HashTable; &quot;Map&lt;&lt;Interface&gt;&gt;&quot; -&gt; LinkedHashMap; &quot;Map&lt;&lt;Interface&gt;&gt;&quot; -&gt; HashMap; &quot;Map&lt;&lt;Interface&gt;&gt;&quot; -&gt; TreeMap;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java线程]]></title>
      <url>%2F2016%2F12%2F12%2Fjava-thread-join%2F</url>
      <content type="text"><![CDATA[线程合并(Join)线程的合并的含义就是将几个并行线程的线程合并为一个单线程执行，应用场景是当一个线程必须等待另一个线程执行完毕才能执行时可以使用join方法。 123456789101112131415161718192021222324public static void main(String[] args) &#123; Thread t1 = new MyThread1(); t1.start(); for (int i = 0; i &lt; 10; i++) &#123; System.out.println("主线程第" + i + "次执行！"); if (i &gt; 2) &#123; try &#123; //t1线程合并到主线程中，主线程停止执行过程，转而执行t1线程，直到t1执行完毕后继续。 t1.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;static class MyThread1 extends Thread &#123; public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; System.out.println("线程1第" + i + "次执行！"); &#125; &#125;&#125; 运行结果如下： 123456789101112131415161718192021222324Connected to the target VM, address: &apos;127.0.0.1:44229&apos;, transport: &apos;socket&apos;主线程第0次执行！主线程第1次执行！线程1第0次执行！主线程第2次执行！线程1第1次执行！主线程第3次执行！线程1第2次执行！线程1第3次执行！线程1第4次执行！线程1第5次执行！线程1第6次执行！线程1第7次执行！线程1第8次执行！线程1第9次执行！主线程第4次执行！主线程第5次执行！主线程第6次执行！主线程第7次执行！主线程第8次执行！主线程第9次执行！Disconnected from the target VM, address: &apos;127.0.0.1:44229&apos;, transport: &apos;socket&apos;Process finished with exit code 0]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java的Object类]]></title>
      <url>%2F2016%2F12%2F12%2Fjava-object%2F</url>
      <content type="text"><![CDATA[今天面试的时候，面试官问起了Object有哪些方法，回答出来大半，不过还是不完全，理解不够深入，平时还没有细细研究这些内容，回家时仔细总结，并记录如下。Java中的Object类是所有类的父类，它提供了以下11个方法： 1234567891011121314151617181920212223242526272829303132public final native Class&lt;?&gt; getClass();public native int hashCode();public boolean equals(Object obj) &#123; return (this == obj);&#125;protected native Object clone() throws CloneNotSupportedException;public String toString() &#123; return getClass().getName() + "@" + Integer.toHexString(hashCode());&#125;public final native void notify();public final native void notifyAll();public final native void wait(long timeout) throws InterruptedException;public final void wait(long timeout, int nanos) throws InterruptedException &#123; if (timeout &lt; 0) &#123; throw new IllegalArgumentException("timeout value is negative"); &#125; if (nanos &lt; 0 || nanos &gt; 999999) &#123; throw new IllegalArgumentException( "nanosecond timeout value out of range"); &#125; if (nanos &gt; 0) &#123; timeout++; &#125; wait(timeout);&#125;public final void wait() throws InterruptedException &#123; wait(0);&#125;protected void finalize() throws Throwable &#123; &#125; getClass方法getClass方法是一个final方法，不允许子类重写，并且也是一个native方法。 返回当前运行时对象的Class对象，注意这里是运行时，比如以下代码中n是一个Number类型的实例，但是java中数值默认是Integer类型，所以getClass方法返回的是java.lang.Integer： 1234"str".getClass() // class java.lang.String"str".getClass == String.class // trueNumber n = 0;Class&lt;? extends Number&gt; c = n.getClass(); // class java.lang.Integer hashCode方法hashCode方法也是一个native方法。 该方法返回对象的哈希码，主要使用在哈希表中，比如JDK中的HashMap。 哈希码的通用约定如下： 在java程序执行过程中，在一个对象没有被改变的前提下，无论这个对象被调用多少次，hashCode方法都会返回相同的整数值。对象的哈希码没有必要在不同的程序中保持相同的值。如果2个对象使用equals方法进行比较并且相同的话，那么这2个对象的hashCode方法的值也必须相等。如果根据equals方法，得到两个对象不相等，那么这2个对象的hashCode值不需要必须不相同。但是，不相等的对象的hashCode值不同的话可以提高哈希表的性能。通常情况下，不同的对象产生的哈希码是不同的。默认情况下，对象的哈希码是通过将该对象的内部地址转换成一个整数来实现的。 String的hashCode方法实现如下， 计算方法是 s[0]31^(n-1) + s[1]31^(n-2) + … + s[n-1]，其中s[0]表示字符串的第一个字符，n表示字符串长度： public int hashCode() { int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) { char val[] = value; for (int i = 0; i &lt; value.length; i++) { h = 31 * h + val[i]; } hash = h; } return h; }比如”fo”的hashCode = 102 31^1 + 111 = 3273， “foo”的hashCode = 102 31^2 + 111 * 31^1 + 111 = 101574 (‘f’的ascii码为102, ‘o’的ascii码为111) hashCode在哈希表HashMap中的应用： // Student类，只重写了hashCode方法public static class Student { private String name; private int age; public Student(String name, int age) { this.name = name; this.age = age; } @Override public int hashCode() { return name.hashCode(); } } Map map = new HashMap();Student stu1 = new Student(“fo”, 11);Student stu2 = new Student(“fo”, 22);map.put(stu1, “fo”);map.put(stu2, “fo”);上面这段代码中，map中有2个元素stu1和stu2。但是这2个元素是在哈希表中的同一个数组项中的位置，也就是在同一串链表中。 但是为什么stu1和stu2的hashCode相同，但是两条元素都插到map里了，这是因为map判断重复数据的条件是 两个对象的哈希码相同并且(两个对象是同一个对象或者两个对象相等[equals为true])。 所以再给Student重写equals方法，并且只比较name的话，这样map就只有1个元素了。 @Overridepublic boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Student student = (Student) o; return this.name.equals(student.name);}这个例子直接说明了hashCode中通用约定的第三点： 第三点：如果根据equals方法，得到两个对象不相等，那么这2个对象的hashCode值不需要必须不相同。但是，不相等的对象的hashCode值不同的话可以提高哈希表的性能。 –&gt; 上面例子一开始没有重写equals方法，导致两个对象不相等，但是这两个对象的hashCode值一样，所以导致这两个对象在同一串链表中，影响性能。 当然，还有第三种情况，那就是equals方法相等，但是hashCode的值不相等。 这种情况也就是违反了通用约定的第二点： 第二点：如果2个对象使用equals方法进行比较并且相同的话，那么这2个对象的hashCode方法的值也必须相等。 违反这一点产生的后果就是如果一个stu1实例是Student(“fo”, 11)，stu2实例是Student(“fo”, 11)，那么这2个实例是相等的，但是他们的hashCode不一样，这样是导致哈希表中都会存入stu1实例和stu2实例，但是实际情况下，stu1和stu2是重复数据，只允许存在一条数据在哈希表中。所以这一点是非常重点的，再强调一下：如果2个对象的equals方法相等，那么他们的hashCode值也必须相等，反之，如果2个对象hashCode值相等，但是equals不相等，这样会影响性能，所以还是建议2个方法都一起重写。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Fedora设置VPN连接]]></title>
      <url>%2F2016%2F12%2F10%2Ffedora-vpn-connect%2F</url>
      <content type="text"><![CDATA[在家里需要连接学校的VPN，家里的电脑安装的时Fedora 24。原以为会非常复杂，其实设置非常简单。 家里的电脑使用的时Wi-Fi,打开Wi-Fi设置（Wi-Fi Settings）,如下图所示。 选择VPN之后，选择与微软兼容的点对点隧道协议(Point-to-Point Tunneling Protocol-PPTP),Compatible with Microsoft and other PPTP VPN servers。在进入的新界面中，General下的Gatway填写域名。Optional中填写好用户名即可，密码会在打开VPN时提示填入。下图是设置成功后的界面：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[jvm工具]]></title>
      <url>%2F2016%2F12%2F08%2Fjvm-tools%2F</url>
      <content type="text"><![CDATA[jps(JVM Process Status Tools)jps是参照Unix系统的取名规则命名的，而他的功能和ps的功能类似，可以列举正在运行的饿虚拟机进程并显示虚拟机执行的主类以及这些进程的唯一ID（LVMID，对应本机来说和PID相同）。VMID与LVMID需要特别说明下：如果是本地虚拟机进程，VMID(Virtual Machine IDentifier,虚机标识符)和LVMID(Local Virtual Machine IDentifier,虚机标识符)是一致的，如果是远程虚拟机进程，那VMID的格式应当是：[protocol:][//] lvmid [@hostname[:port]/servername] 1jps -lv 输出为： 122609 sun.tools.jps.Jps -Dapplication.home=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-3.b16.fc24.x86_64 -Xms8m2479 nutstore.client.gui.NutstoreGUI -ea -Dfile.encoding=UTF-8 -Xmx320M -XX:MinHeapFreeRatio=20 -XX:MaxHeapFreeRatio=40 -Dlog4j.defaultInitOverride=true -Djava.util.logging.config.file=/home/dolphin/.nutstore/dist/conf/java.logging.properties -Dnutstore.config.dir=/home/dolphin/.nutstore/dist/conf -Dnutstore.x64=True -Djava.library.path=/home/dolphin/.nutstore/dist/lib/native jstat(JVM Statistics Monitoring Tools)jstat是HotSpot Java虚拟机的性能统计工具。jstat主要用于监控虚拟机的各种运行状态信息，如类的装载、内存、垃圾回收、JIT编译器等，在没有GUI的服务器上，这款工具是首选的一款监控工具。每20秒查询一次进程2479的垃圾回收情况，监控5次，命令如下所示： 1jstat -gc 2479 20000 5 参数gc表示监视Java堆，包含eden、2个survivor区、old区和永久带区域的容量、已用空间、GC时间合计等信息。 上图中 S0 表示Survivor 0区的空间使用比例,S0C（Survivor 0 Count）， E, O, P 分别代表Eden, Old和Perm空间使用率，YGC 表示young gc的次数，YGCT(Young Generation Consume Time) 表示young gc消耗的时间。GCT(GC Time) 则用来统计执行gc的总时间。 Column Description S0C Current survivor space 0 capacity (KB) S1C Current survivor space 1 capacity (KB) S0U Survivor space 0 utilization (KB) S1U Survivor space 1 utilization (KB) EC Current eden space capacity (KB) EU Eden space utilization (KB) OC Current old space capacity (KB) OU Old space utilization (KB) PC Current permanent space capacity (KB) PU Permanent space utilization (KB) MC Metaspace capacity (kB) MU Metacspace utilization (kB) CCSC Compressed class space capacity (kB) CCSU Compressed class space used (kB) YGC Number of young generation GC Events YGCT Young generation garbage collection time FGC Number of full GC events FGCT Full garbage collection time GCT Total garbage collection time jinfo（JVM configuration Info for Java）Jinfo的作用是实时查看虚拟机的各项参数信息jps –v可以查看虚拟机在启动时被显式指定的参数信息，但是如果你想知道默认的一些参数信息呢？除了去查询对应的资料以外，jinfo就显得很重要了。 jmap（JVM Memory Map for Java）jhat（JVM Heap Analysis Tool）jstack（JVM Stack Trace for java）jstack用于JVM当前时刻的线程快照，又称threaddump文件，它是JVM当前每一条线程正在执行的堆栈信息的集合。生成线程快照的主要目的是为了定位线程出现长时间停顿的原因，如线程死锁、死循环、请求外部时长过长导致线程停顿的原因。通过jstack我们就可以知道哪些进程在后台做些什么？在等待什么资源等！ 参考资料： jstat - Java Virtual Machine Statistics Monitoring Tool]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring事务管理]]></title>
      <url>%2F2016%2F12%2F03%2Fspring-transaction%2F</url>
      <content type="text"><![CDATA[事务最重要的两个特性，是事务的传播级别（Propagation Level）和数据隔离级别（Isolation Level）。传播级别定义的是事务的控制范围，事务隔离级别定义的是事务在数据库读写方面的控制范围。 配置12345678&lt;!-- 配置事务管理器 --&gt;&lt;bean id="txManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager" p:dataSource-ref="dataSource"&gt;&lt;/bean&gt;&lt;!-- enables scanning for @Transactional annotations，t代表事务（transaction）,x（xml?）代表什么未知--&gt;&lt;tx:annotation-driven transaction-manager="txManager" /&gt; Spring事务实现Spring的事务管理的内部实现是利用了代理技术。 传播级别(Propagation Level)1） PROPAGATION_REQUIRED ，默认的spring事务传播级别，使用该级别的特点是，如果上下文中已经存在事务，那么就加入到事务中执行，如果当前上下文中不存在事务，则新建事务执行。所以这个级别通常能满足处理大多数的业务场景。 2）PROPAGATION_SUPPORTS ，从字面意思就知道，supports，支持，该传播级别的特点是，如果上下文存在事务，则支持事务加入事务，如果没有事务，则使用非事务的方式执行。所以说，并非所有的包在transactionTemplate.execute中的代码都会有事务支持。这个通常是用来处理那些并非原子性的非核心业务逻辑操作。应用场景较少。 3）PROPAGATION_MANDATORY ， 该级别的事务要求上下文中必须要存在事务，否则就会抛出异常！配置该方式的传播级别是有效的控制上下文调用代码遗漏添加事务控制的保证手段。比如一段代码不能单独被调用执行，但是一旦被调用，就必须有事务包含的情况，就可以使用这个传播级别。 4）PROPAGATION_REQUIRES_NEW ，从字面即可知道，new，每次都要一个新事务，该传播级别的特点是，每次都会新建一个事务，并且同时将上下文中的事务挂起，执行当前新建事务完成以后，上下文事务恢复再执行。 这是一个很有用的传播级别，举一个应用场景：现在有一个发送100个红包的操作，在发送之前，要做一些系统的初始化、验证、数据记录操作，然后发送100封红包，然后再记录发送日志，发送日志要求100%的准确，如果日志不准确，那么整个父事务逻辑需要回滚。怎么处理整个业务需求呢？就是通过这个PROPAGATION_REQUIRES_NEW 级别的事务传播控制就可以完成。发送红包的子事务不会直接影响到父事务的提交和回滚。 5）PROPAGATION_NOT_SUPPORTED ，这个也可以从字面得知，not supported ，不支持，当前级别的特点就是上下文中存在事务，则挂起事务，执行当前逻辑，结束后恢复上下文的事务。 这个级别有什么好处？可以帮助你将事务极可能的缩小。我们知道一个事务越大，它存在的风险也就越多。所以在处理事务的过程中，要保证尽可能的缩小范围。比如一段代码，是每次逻辑操作都必须调用的，比如循环1000次的某个非核心业务逻辑操作。这样的代码如果包在事务中，势必造成事务太大，导致出现一些难以考虑周全的异常情况。所以这个事务这个级别的传播级别就派上用场了。用当前级别的事务模板抱起来就可以了。 6）PROPAGATION_NEVER ，该事务更严格，上面一个事务传播级别只是不支持而已，有事务就挂起，而PROPAGATION_NEVER传播级别要求上下文中不能存在事务，一旦有事务，就抛出runtime异常，强制停止执行！这个级别上辈子跟事务有仇。 7）PROPAGATION_NESTED ，字面也可知道，nested，嵌套级别事务。该传播级别特征是，如果上下文中存在事务，则嵌套事务执行，如果不存在事务，则新建事务。 隔离级别1、Serializable ：最严格的级别，事务串行执行，资源消耗最大； 2、REPEATABLE READ ：保证了一个事务不会修改已经由另一个事务读取但未提交（回滚）的数据。避免了“脏读取”和“不可重复读取”的情况，但是带来了更多的性能损失。 3、READ COMMITTED :大多数主流数据库的默认事务等级，保证了一个事务不会读到另一个并行事务已修改但未提交的数据，避免了“脏读取”。该级别适用于大多数系统。 4、Read Uncommitted ：保证了读取过程中不会读取到非法数据。 上面的解释其实每个定义都有一些拗口，其中涉及到几个术语：脏读、不可重复读、幻读。这里解释一下： 脏读 :所谓的脏读，其实就是读到了别的事务回滚前的脏数据。比如事务B执行过程中修改了数据X，在未提交前，事务A读取了X，而事务B却回滚了，这样事务A就形成了脏读。 不可重复读 ：不可重复读字面含义已经很明了了，比如事务A首先读取了一条数据，然后执行逻辑的时候，事务B将这条数据改变了，然后事务A再次读取的时候，发现数据不匹配了，就是所谓的不可重复读了。 幻读 ：小的时候数手指，第一次数十10个，第二次数是11个，怎么回事？产生幻觉了？幻读也是这样子，事务A首先根据条件索引得到10条数据，然后事务B改变了数据库一条数据，导致也符合事务A当时的搜索条件，这样事务A再次搜索发现有11条数据了，就产生了幻读。 一个对照关系表： Dirty reads non-repeatable reads phantom reads Serializable 不会 不会 不会 REPEATABLE READ 不会 不会 会 READ COMMITTED 不会 会 会 Read Uncommitted 会 会 会 所以最安全的，是Serializable，但是伴随而来也是高昂的性能开销。另外，事务常用的两个属性：readonly和timeout一个是设置事务为只读以提升性能。另一个是设置事务的超时时间，一般用于防止大事务的发生。还是那句话，事务要尽可能的小！ 注意事项 在需要事务管理的地方加@Transactional 注解。@Transactional 注解可以被应用于接口定义和接口方法、类定义和类的 public 方法上。 @Transactional 注解只能应用到 public 可见度的方法上。 如果你在 protected、private 或者 package-visible 的方法上使用 @Transactional 注解，它也不会报错， 但是这个被注解的方法将不会展示已配置的事务设置。 注意仅仅 @Transactional 注解的出现不足于开启事务行为，它仅仅 是一种元数据。必须在配置文件中使用配置元素，才真正开启了事务行为。 通过元素的 “proxy-target-class” 属性值来控制是基于接口的还是基于类的代理被创建。如果 “proxy-target-class” 属值被设置为 “true”，那么基于类的代理将起作用（这时需要CGLIB库cglib.jar在CLASSPATH中）。如果 “proxy-target-class” 属值被设置为 “false” 或者这个属性被省略，那么标准的JDK基于接口的代理将起作用。 Spring团队建议在具体的类（或类的方法）上使用 @Transactional 注解，而不要使用在类所要实现的任何接口上。在接口上使用 @Transactional 注解，只能当你设置了基于接口的代理时它才生效。因为注解是 不能继承 的，这就意味着如果正在使用基于类的代理时，那么事务的设置将不能被基于类的代理所识别，而且对象也将不会被事务代理所包装。 @Transactional 的事务开启 ，或者是基于接口的 或者是基于类的代理被创建。所以在同一个类中一个方法调用另一个方法有事务的方法，事务是不会起作用的。 Spring @Transactional工作原理]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MyBatis使用]]></title>
      <url>%2F2016%2F12%2F02%2Fmybatis-cache%2F</url>
      <content type="text"><![CDATA[MyBatis缓存正如大多数持久层框架一样，MyBatis 同样提供了一级缓存和二级缓存的支持； 一级缓存基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为 Session，当 Session flush 或 close 之后，该Session中的所有 Cache 就将清空。二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap存储，不同在于其存储作用域为 Mapper(Namespace)，并且可自定义存储源，如 Ehcache、Hazelcast等。对于缓存数据更新机制，当某一个作用域(一级缓存Session/二级缓存Namespaces)的进行了 C/U/D 操作后，默认该作用域下所有 select 中的缓存将被clear。MyBatis 的缓存采用了delegate机制 及 装饰器模式设计，当put、get、remove时，其中会经过多层 delegate cache 处理，其Cache类别有：BaseCache(基础缓存)、EvictionCache(排除算法缓存) 、DecoratorCache(装饰器缓存)： BaseCache:为缓存数据最终存储的处理类，默认为 PerpetualCache，基于Map存储；可自定义存储处理，如基于EhCache、Memcached等； EvictionCache:当缓存数量达到一定大小后，将通过算法对缓存数据进行清除。默认采用 Lru 算法(LruCache)，提供有 fifo 算法(FifoCache)等； DecoratorCache：缓存put/get处理前后的装饰器，如使用 LoggingCache 输出缓存命中日志信息、使用 SerializedCache 对 Cache的数据 put或get 进行序列化及反序列化处理、当设置flushInterval(默认1/h)后，则使用 ScheduledCache 对缓存数据进行定时刷新等。 &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD一般缓存框架的数据结构基本上都是 Key-Value 方式存储，MyBatis 对于其 Key 的生成采取规则为：[hashcode : checksum : mappedStementId : offset : limit : executeSql : queryParams]。对于并发 Read/Write 时缓存数据的同步问题，MyBatis 默认基于 JDK/concurrent中的ReadWriteLock，使用 ReentrantReadWriteLock 的实现，从而通过 Lock 机制防止在并发 Write Cache 过程中线程安全问题。 MyBatis延迟加载MyBatis打印出SQL 在方法上添加MethodLog注解即可，如下图所示。 properties文件配置 将ibatis log4j运行级别调到DEBUG可以在控制台打印出ibatis运行的sql语句,方便调试: 12345678910111213141516171819202122232425### 设置Logger输出级别和输出目的地 ###log4j.rootLogger=debug,stdout,logfile### 把日志信息输出到控制台 ###log4j.appender.stdout=org.apache.log4j.ConsoleAppender#log4j.appender.stdout.Target=System.errlog4j.appender.stdout.layout=org.apache.log4j.SimpleLayout### 把日志信息输出到文件：jbit.log ###log4j.appender.logfile=org.apache.log4j.FileAppenderlog4j.appender.logfile.File=jbit.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss&#125; %F %p %m%n###显示SQL语句部分log4j.logger.com.ibatis=DEBUG log4j.logger.com.ibatis.common.jdbc.SimpleDataSource=DEBUG log4j.logger.com.ibatis.common.jdbc.ScriptRunner=DEBUG log4j.logger.com.ibatis.sqlmap.engine.impl.SqlMapClientDelegate=DEBUG log4j.logger.java.sql.Connection=DEBUG log4j.logger.java.sql.Statement=DEBUG log4j.logger.java.sql.PreparedStatement=DEBUG xml文件配置 1234567891011121314151617181920212223&lt;!-- 调试sql日志 --&gt;&lt;Logger name="com.ibatis" level="info"&gt; &lt;AppenderRef ref="Console" /&gt;&lt;/Logger&gt;&lt;Logger name="com.ibatis.common.jdbc.SimpleDataSource" level="info"&gt; &lt;AppenderRef ref="Console" /&gt;&lt;/Logger&gt;&lt;Logger name="org.apache.ibatis.jdbc.ScriptRunner" level="info"&gt; &lt;AppenderRef ref="Console" /&gt;&lt;/Logger&gt;&lt;Logger name="com.ibatis.sqlmap.engine.impl.SqlMapClientDelegate" level="info"&gt; &lt;AppenderRef ref="Console" /&gt;&lt;/Logger&gt;&lt;Logger name="java.sql.Connection" level="DEBUG"&gt; &lt;AppenderRef ref="Console" /&gt;&lt;/Logger&gt;&lt;Logger name="java.sql.Statement" level="DEBUG"&gt; &lt;AppenderRef ref="Console" /&gt;&lt;/Logger&gt;&lt;Logger name="java.sql.PreparedStatement" level="DEBUG"&gt; &lt;AppenderRef ref="Console" /&gt;&lt;/Logger&gt; =======一般缓存框架的数据结构基本上都是 Key-Value 方式存储，MyBatis 对于其 Key 的生成采取规则为：[hashcode : checksum : mappedStementId : offset : limit : executeSql : queryParams]。对于并发 Read/Write 时缓存数据的同步问题，MyBatis 默认基于 JDK/concurrent中的ReadWriteLock，使用 ReentrantReadWriteLock 的实现，从而通过 Lock 机制防止在并发 Write Cache 过程中线程安全问题。 bdc53dc27ab592072cdf63f4d5aa23d2d8717049]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring视图解析器]]></title>
      <url>%2F2016%2F12%2F02%2Fspring-viewresolver%2F</url>
      <content type="text"><![CDATA[InternalResourceViewResolver在Sprinng MVC中，Handler执行完成后，向DispatcherServlet返回一个ModelAndView对象。此时就需要调用视图解析器(View Resolver)来渲染视图了。 1234&lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver"&gt; &lt;property name="prefix" value="/WEB-INF/views/"/&gt; &lt;property name="suffix" value=".jsp"/&gt;&lt;/bean&gt; 它是URLBasedViewResolver的子类，所以URLBasedViewResolver支持的特性它都支持。单从字面意思来看，可以把InternalResourceViewResolver解释为内部资源视图解析器，这就是InternalResourceViewResolver的一个特性。InternalResourceViewResolver会把返回的视图名称都解析为InternalResourceView对象，InternalResourceView会把Controller处理器方法返回的模型属性都存放到对应的request属性中，然后通过RequestDispatcher在服务器端把请求forword重定向到目标URL。比如在InternalResourceViewResolver中定义了prefix=/WEB-INF/，suffix=.jsp，然后请求的Controller处理器方法返回的视图名称为test，那么这个时候InternalResourceViewResolver就会把test解析为一个InternalResourceView对象，先把返回的模型属性都存放到对应的HttpServletRequest属性中，然后利用RequestDispatcher在服务器端把请求forword到/WEB-INF/test.jsp。注意InternalResourceViewResolver只能渲染jsp页面，如果需要渲染html看这里。 ThymeleafViewResolver在Java世界的MVC框架里，使用的视图技术不少，最基本的是JSP，还有知名的FreeMarker和Velocity等模板引擎。Thymeleaf也是一款优秀的模板引擎，它在HTML5/XHTML的视图层表现的很好，也能在离线情况下处理任何XML文件。它是完全可以替代JSP+JSTL(JSP Standard Tag Library，JSP标准标签库)的。 Thymeleaf官方的Q&amp;A： Q: 和FreeMarker,Velocity相比，Thymeleaf表现得怎样呢？ A：FreeMarker和Velocity都是软件领域杰出的作品，但它们在解决模板问题上的处理哲学和Thymeleaf不一样。Thymeleaf强调的是自然模板，也就是允许模板作为产品原型使用(笔者注:因为其后缀名就是.html，可以直接在浏览器打开)，而FreeMarker和Velocity不行。并且Thymeleaf的语法更简洁、更和目前Web开发的趋势相一致。其次，从架构的角度看，FreeMarker和Velocity更像个文本处理器，所以它们能够处理很多类型的内容，而Thymeleaf是基于XML的，只能处理XML格式的数据。因此这样看，FreeMarker和Velocity更通用些，但恰恰如此，Thymeleaf更能利用XML的特性，尤其是在Web应用中。 12345&lt;!-- thymeleaf thymeleafViewResolver--&gt;&lt;bean class="org.thymeleaf.spring4.view.ThymeleafViewResolver"&gt; &lt;property name="templateEngine" ref="templateEngine" /&gt; &lt;property name="characterEncoding" value="UTF-8" /&gt;&lt;/bean&gt;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Fedora上MySQL安装]]></title>
      <url>%2F2016%2F12%2F02%2Ffedora-mysql-install%2F</url>
      <content type="text"><![CDATA[12## Fedora 24 ##dnf install https://dev.mysql.com/get/mysql57-community-release-fc24-8.noarch.rpm 安装MySQL。 1dnf install -y mysql-community-server 启动MySQL。 123systemctl start mysqld.service ## use restart after updatesystemctl enable mysqld.service 登陆时会提示如下错误：ERROR 1045 (28000): Access denied for user ‘root’@’localhost’ (using password: YES)获取随机密码。 1grep 'A temporary password is generated for root@localhost' /var/log/mysqld.log |tail -1 重新设置MySQL密码,密码要求包含大小写和数字以及特殊字符，不符合安全策略的(Your password does not satisfy the current policy requirements)密码无法设置成功。 1SET PASSWORD FOR 'root'@'localhost' = PASSWORD('$jxqMySQL123456'); 數據庫操作創建數據庫。 1234#列出數據庫show databases;#創建數據庫create database dolphin;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[面试总结]]></title>
      <url>%2F2016%2F12%2F02%2Finterview-summerize%2F</url>
      <content type="text"><![CDATA[面试的过程也是气质是否和公司搭调的过程，每个公司都有自己的管理特色。找到价值观相对契合的企业非常重要。这里是面试时面试官问过的问题，或许平时开发中根本没有想过这些问题，但注意更多细节总是有益处的，总结如下。 Java中线程安全的类 ArrayList线程不安全，Vector线程安全； HashMap线程不安全，HashTable线程安全,ConcurrentHashMap线程安全； StringBuilder线程不安全，StringBuffer线程安全。 String、StringBuilder、StringBuffer区别简要的说， String 类型和 StringBuffer类型的主要性能区别其实在于 String 是不可变的对象, 因此在每次对 String 类型进行改变的时候其实都等同于生成了一个新的 String 对象，然后将指针指向新的 String 对象，所以经常改变内容的字符串最好不要用 String ，因为每次生成对象都会对系统性能产生影响，特别当内存中无引用对象多了以后， JVM 的 GC 就会进行垃圾回收，执行垃圾回收时是会block住程序的，影响性能。而如果是使用 StringBuffer 类则结果就不一样了，每次结果都会对 StringBuffer 对象本身进行操作，而不是生成新的对象，再改变对象引用。所以在一般情况下我们推荐使用 StringBuffer ，特别是字符串对象经常改变的情况下。而在某些特别情况下， String 对象的字符串拼接其实是被 JVM 解释成了 StringBuffer 对象的拼接，所以这些时候 String 对象的速度并不会比 StringBuffer 对象慢，而特别是以下的字符串对象生成中， String 效率是远要比 StringBuffer 快的： 12String S1 = “This is only a” + “ simple” + “ test”;StringBuffer Sb = new StringBuilder(“This is only a”).append(“ simple”).append(“ test”); 你会很惊讶的发现，生成 String S1 对象的速度简直太快了，而这个时候 StringBuffer 居然速度上根本一点都不占优势。其实这是 JVM 的一个把戏，在 JVM 里: 1String S1 = “This is only a” + “ simple” + “test”; 其实就是： 1String S1 = “This is only a simple test”; 所以当然不需要太多的时间了。但大家这里要注意的是，如果你的字串是来自另外的 String 对象的话，速度就没那么快了，譬如： 1234String S2 = “This is only a”;String S3 = “ simple”;String S4 = “ test”;String S1 = S2 +S3 + S4; 这时候 JVM 会规规矩矩的按照原来的方式去做 在大部分情况下 StringBuffer 优于 StringStringBufferJava.lang.StringBuffer线程安全的可变字符序列。一个类似于 String 的字符串缓冲区，但不能修改。虽然在任意时间点上它都包含某种特定的字符序列，但通过某些方法调用可以改变该序列的长度和内容。可将字符串缓冲区安全地用于多个线程。可以在必要时对这些方法进行同步，因此任意特定实例上的所有操作就好像是以串行顺序发生的，该顺序与所涉及的每个线程进行的方法调用顺序一致。StringBuffer 上的主要操作是 append 和 insert 方法，可重载这些方法，以接受任意类型的数据。每个方法都能有效地将给定的数据转换成字符串，然后将该字符串的字符追加或插入到字符串缓冲区中。append 方法始终将这些字符添加到缓冲区的末端；而 insert 方法则在指定的点添加字符。例如，如果 z 引用一个当前内容是“start”的字符串缓冲区对象，则此方法调用 z.append(“le”) 会使字符串缓冲区包含“startle”，而 z.insert(4, “le”) 将更改字符串缓冲区，使之包含“starlet”。在大部分情况下 StringBuilder 优于 StringBufferjava.lang.StringBuildejava.lang.StringBuilder一个可变的字符序列是5.0新增的。此类提供一个与 StringBuffer 兼容的 API，但不保证同步。该类被设计用作 StringBuffer 的一个简易替换，用在字符串缓冲区被单个线程使用的时候（这种情况很普遍）。如果可能，建议优先采用该类，因为在大多数实现中，它比 StringBuffer 要快。两者的方法基本相同。 使用Spring框架的原因 非侵入式：支持基于POJO的编程模式，不强制性的要求实现Spring框架中的接口或继承Spring框架中的类。 IoC容器：IoC容器帮助应用程序管理对象以及对象之间的依赖关系，对象之间的依赖关系如果发生了改变只需要修改配置文件而不是修改代码，因为代码的修改可能意味着项目的重新构建和完整的回归测试。有了IoC容器，程序员再也不需要自己编写工厂、单例，这一点特别符合Spring的精神”不要重复的发明轮子”。采用依赖注入技术之后，A的代码只需要定义一个私有的B对象，不需要直接new来获得这个对象，而是通过相关的容器控制程序来将B对象在外部new出来并注入到A类里的引用中。而具体获取的方法、对象被获取时的状态由配置文件（如XML）来指定。 AOP（面向切面编程）：日志代码往往水平地散布在所有对象层次中，而与它所散布到的对象的核心功能毫无关系。对于其他类型的代码，如安全性、异常处理和透明的持续性也是如此。这种散布在各处的无关的代码被称为横切（cross-cutting）代码，在OOP设计中，它导致了大量代码的重复，而不利于各个模块的重用。将所有的横切关注功能封装到切面（aspect）中，通过配置的方式将横切关注功能动态添加到目标代码上，进一步实现了业务逻辑和系统服务之间的分离。另一方面，有了AOP程序员可以省去很多自己写代理类的工作。AOP把软件系统分为两个部分：核心关注点和横切关注点。业务处理的主要流程是核心关注点，与之关系不大的部分是横切关注点。横切关注点的一个特点是，他们经常发生在核心关注点的多处，而各处都基本相似。比如权限认证、日志、事务处理。Aop 的作用在于分离系统中的各种关注点，将核心关注点和横切关注点分离开来。正如Avanade公司的高级方案构架师Adam Magee所说，AOP的核心思想就是“将应用程序中的商业逻辑同对其提供支持的通用服务进行分离。” MVC：Spring的MVC框架是非常优秀的，Spring MVC的配置相对于Struts 2来说较少，性能方面，Spring比Struts较快，开发效率Spring MVC确实比struts2高，Spring3 MVC更容易实现Restful URL。Struts更加很多新的技术点，比如拦截器、值栈及OGNL（Object-Graph Navigation Language）表达式，学习成本较高，springmvc 比较简单，很较少的时间都能上手。 事务管理：Spring以宽广的胸怀接纳多种持久层技术，并且为其提供了声明式的事务管理，在不需要任何一行代码的情况下就能够完成事务管理。 其他：选择Spring框架的原因还远不止于此，Spring为Java企业级开发提供了一站式选择，你可以在需要的时候使用它的部分和全部，更重要的是，你甚至可以在感觉不到Spring存在的情况下，在你的项目中使用Spring提供的各种优秀的功能。 大型网站在架构上应当考虑哪些问题 分层(Layer)：分层是处理任何复杂系统最常见的手段之一，将系统横向切分成若干个层面，每个层面只承担单一的职责，然后通过下层为上层提供的基础设施和服务以及上层对下层的调用来形成一个完整的复杂的系统。计算机网络的开放系统互联参考模型（OSI[Open Systems Interconnection]/RM）和Internet的TCP/IP模型都是分层结构，大型网站的软件系统也可以使用分层的理念将其分为持久层（提供数据存储和访问服务）、业务层（处理业务逻辑，系统中最核心的部分）和表示层（系统交互、视图展示）。需要指出的是：（1）分层是逻辑上的划分，在物理上可以位于同一设备上也可以在不同的设备上部署不同的功能模块，这样可以使用更多的计算资源来应对用户的并发访问；（2）层与层之间应当有清晰的边界，这样分层才有意义，才更利于软件的开发和维护。 分割(Split)：分割是对软件的纵向切分。我们可以将大型网站的不同功能和服务分割开，形成高内聚低耦合的功能模块（单元）。在设计初期可以做一个粗粒度的分割，将网站分割为若干个功能模块，后期还可以进一步对每个模块进行细粒度的分割，这样一方面有助于软件的开发和维护，另一方面有助于分布式的部署，提供网站的并发处理能力和功能的扩展。 分布式(Distribution)：除了上面提到的内容，网站的静态资源（JavaScript、CSS、图片等）也可以采用独立分布式部署并采用独立的域名，这样可以减轻应用服务器的负载压力，也使得浏览器对资源的加载更快。数据的存取也应该是分布式的，传统的商业级关系型数据库产品基本上都支持分布式部署，而新生的NoSQL产品几乎都是分布式的。当然，网站后台的业务处理也要使用分布式技术，例如查询索引的构建、数据分析等，这些业务计算规模庞大，可以使用Hadoop以及MapReduce分布式计算框架来处理。 集群(Cluster)：集群使得有更多的服务器提供相同的服务，可以更好的提供对并发的支持。 缓存(Cache)：所谓缓存就是用空间换取时间的技术，将数据尽可能放在距离计算最近的位置。使用缓存是网站优化的第一定律。我们通常说的CDN、反向代理、热点数据都是对缓存技术的使用。 异步(Async)：异步是实现软件实体之间解耦合的又一重要手段。异步架构是典型的生产者消费者模式，二者之间没有直接的调用关系，只要保持数据结构不变，彼此功能实现可以随意变化而不互相影响，这对网站的扩展非常有利。使用异步处理还可以提高系统可用性，加快网站的响应速度（用Ajax加载数据就是一种异步技术），同时还可以起到削峰作用（应对瞬时高并发）。能推迟处理的都要推迟处理”是网站优化的第二定律，而异步是践行网站优化第二定律的重要手段。 冗余(Redundancy)：各种服务器都要提供相应的冗余服务器以便在某台或某些服务器宕机时还能保证网站可以正常工作，同时也提供了灾难恢复的可能性。冗余是网站高可用性的重要保证。 你使用过的应用服务器优化技术有哪些 分布式缓存：缓存的本质就是内存中的哈希表，如果设计一个优质的哈希函数，那么理论上哈希表读写的渐近时间复杂度为O(1)。缓存主要用来存放那些读写比很高、变化很少的数据，这样应用程序读取数据时先到缓存中读取，如果没有或者数据已经失效再去访问数据库或文件系统，并根据拟定的规则将数据写入缓存。对网站数据的访问也符合二八定律（Pareto分布，幂律分布），即80%的访问都集中在20%的数据上，如果能够将这20%的数据缓存起来，那么系统的性能将得到显著的改善。当然，使用缓存需要解决以下几个问题： 1.频繁修改的数据；2.数据不一致与脏读；3.缓存雪崩（可以采用分布式缓存服务器集群加以解决，memcached是广泛采用的解决方案）；4.缓存预热；5.缓存穿透（恶意持续请求不存在的数据）。 异步操作：可以使用消息队列将调用异步化，通过异步处理将短时间高并发产生的事件消息存储在消息队列中，从而起到削峰作用。电商网站在进行促销活动时，可以将用户的订单请求存入消息队列，这样可以抵御大量的并发订单请求对系统和数据库的冲击。目前，绝大多数的电商网站即便不进行促销活动，订单系统都采用了消息队列来处理。 使用集群。 代码优化： 1.多线程：基于Java的Web开发基本上都通过多线程的方式响应用户的并发请求，使用多线程技术在编程上要解决线程安全问题，主要可以考虑以下几个方面：A. 将对象设计为无状态对象（这和面向对象的编程观点是矛盾的，在面向对象的世界中被视为不良设计），这样就不会存在并发访问时对象状态不一致的问题。B. 在方法内部创建对象，这样对象由进入方法的线程创建，不会出现多个线程访问同一对象的问题。使用ThreadLocal将对象与线程绑定也是很好的做法，这一点在前面已经探讨过了。C. 对资源进行并发访问时应当使用合理的锁机制。2.非阻塞I/O： 使用单线程和非阻塞I/O是目前公认的比多线程的方式更能充分发挥服务器性能的应用模式，基于Node.js构建的服务器就采用了这样的方式。Java在JDK 1.4中就引入了NIO（Non-blocking I/O）,在Servlet 3规范中又引入了异步Servlet的概念，这些都为在服务器端采用非阻塞I/O提供了必要的基础。3.资源复用：资源复用主要有两种方式，一是单例，二是对象池，我们使用的数据库连接池、线程池都是对象池化技术，这是典型的用空间换取时间的策略，另一方面也实现对资源的复用，从而避免了不必要的创建和释放资源所带来的开销。 你用过的网站前端优化的技术有哪些 浏览器访问优化： 减少HTTP请求数量：合并CSS、合并JavaScript、合并图片（CSS Sprite） 使用浏览器缓存：通过设置HTTP响应头中的Cache-Control和Expires属性，将CSS、JavaScript、图片等在浏览器中缓存，当这些静态资源需要更新时，可以更新HTML文件中的引用来让浏览器重新请求新的资源 启用压缩 CSS前置，JavaScript后置 减少Cookie传输 CDN加速：CDN（Content Distribute Network）的本质仍然是缓存，将数据缓存在离用户最近的地方，CDN通常部署在网络运营商的机房，不仅可以提升响应速度，还可以减少应用服务器的压力。当然，CDN缓存的通常都是静态资源。 反向代理：反向代理相当于应用服务器的一个门面，可以保护网站的安全性，也可以实现负载均衡的功能，当然最重要的是它缓存了用户访问的热点资源，可以直接从反向代理将某些内容返回给用户浏览器。 什么是XSS攻击？什么是SQL注入攻击？什么是CSRF攻击？ XSS（Cross Site Script，跨站脚本攻击）是向网页中注入恶意脚本在用户浏览网页时在用户浏览器中执行恶意脚本的攻击方式。跨站脚本攻击分有两种形式：反射型攻击（诱使用户点击一个嵌入恶意脚本的链接以达到攻击的目标，目前有很多攻击者利用论坛、微博发布含有恶意脚本的URL就属于这种方式）和持久型攻击（将恶意脚本提交到被攻击网站的数据库中，用户浏览网页时，恶意脚本从数据库中被加载到页面执行，QQ邮箱的早期版本就曾经被利用作为持久型跨站脚本攻击的平台）。XSS虽然不是什么新鲜玩意，但是攻击的手法却不断翻新，防范XSS主要有两方面：消毒（对危险字符进行转义）和HttpOnly（防范XSS攻击者窃取Cookie数据）。 SQL注入攻击是注入攻击最常见的形式（此外还有OS注入攻击（Struts 2的高危漏洞就是通过OGNL实施OS注入攻击导致的）），当服务器使用请求参数构造SQL语句时，恶意的SQL被嵌入到SQL中交给数据库执行。SQL注入攻击需要攻击者对数据库结构有所了解才能进行，攻击者想要获得表结构有多种方式：（1）如果使用开源系统搭建网站，数据库结构也是公开的（目前有很多现成的系统可以直接搭建论坛，电商网站，虽然方便快捷但是风险是必须要认真评估的）；（2）错误回显（如果将服务器的错误信息直接显示在页面上，攻击者可以通过非法参数引发页面错误从而通过错误信息了解数据库结构，Web应用应当设置友好的错误页，一方面符合最小惊讶原则，一方面屏蔽掉可能给系统带来危险的错误回显信息）；（3）盲注。防范SQL注入攻击也可以采用消毒的方式，通过正则表达式对请求参数进行验证，此外，参数绑定也是很好的手段，这样恶意的SQL会被当做SQL的参数而不是命令被执行，JDBC中的PreparedStatement就是支持参数绑定的语句对象，从性能和安全性上都明显优于Statement。 CSRF攻击（Cross Site Request Forgery，跨站请求伪造）是攻击者通过跨站请求，以合法的用户身份进行非法操作（如转账或发帖等）。CSRF的原理是利用浏览器的Cookie或服务器的Session，盗取用户身份，其原理如下图所示。防范CSRF的主要手段是识别请求者的身份，主要有以下几种方式：（1）在表单中添加令牌（token）；（2）验证码；（3）检查请求头中的Referer（前面提到防图片盗链接也是用的这种方式）。令牌和验证都具有一次消费性的特征，因此在原理上一致的，但是验证码是一种糟糕的用户体验，不是必要的情况下不要轻易使用验证码，目前很多网站的做法是如果在短时间内多次提交一个表单未获得成功后才要求提供验证码，这样会获得较好的用户体验。 什么是领域模型(domain model)？贫血模型(anaemic domain model)和充血模型(rich domain model)有什么区别？领域模型是领域内的概念类或现实世界中对象的可视化表示，又称为概念模型或分析对象模型，它专注于分析问题领域本身，发掘重要的业务领域概念，并建立业务领域概念之间的关系。贫血模型是指使用的领域对象中只有setter和getter方法（POJO），所有的业务逻辑都不包含在领域对象中而是放在业务逻辑层。有人将我们这里说的贫血模型进一步划分成失血模型（领域对象完全没有业务逻辑）和贫血模型（领域对象有少量的业务逻辑），我们这里就不对此加以区分了。充血模型将大多数业务逻辑和持久化放在领域对象中，业务逻辑（业务门面）只是完成对业务逻辑的封装、事务和权限等的处理。下面两张图分别展示了贫血模型和充血模型的分层架构。更加细粒度的有失血模型，贫血模型，充血模型，胀血模型。贫血模型就是domain ojbect包含了不依赖于持久化的领域逻辑，而那些依赖持久化的领域逻辑被分离到Service层。失血模型简单来说，就是domain object只有属性的getter/setter方法的纯数据类，所有的业务逻辑完全由business object来完成(又称Transaction Script)，这种模型下的domain object被Martin Fowler称之为“贫血的domain object”。充血模型和第二种模型差不多，所不同的就是如何划分业务逻辑，即认为，绝大多业务逻辑都应该被放在domain object里面(包括持久化逻辑)，而Service层应该是很薄的一层，仅仅封装事务和少量逻辑，不和DAO层打交道。 描述一下JVM加载class文件的原理机制类加载的过程包括了加载、验证、准备、解析、初始化五个阶段。JVM中类的装载是由类加载器（ClassLoader）和它的子类来实现的，Java中的类加载器是一个重要的Java运行时系统组件，它负责在运行时查找和装入类文件中的类。 由于Java的跨平台性，经过编译的Java源程序并不是一个可执行程序，而是一个或多个类文件。当Java程序需要使用某个类时，JVM会确保这个类已经被加载、连接（验证、准备和解析）和初始化。类的加载是指把类的.class文件中的数据读入到内存中，通常是创建一个字节数组读入.class文件，然后产生与所加载类对应的Class对象。加载完成后，Class对象还不完整，所以此时的类还不可用。当类被加载后就进入连接阶段，这一阶段包括验证、准备（为静态变量分配内存并设置默认的初始值）和解析（将符号引用替换为直接引用）三个步骤。最后JVM对类进行初始化，包括：A. 如果类存在直接的父类并且这个类还没有被初始化，那么就先初始化父类；B. 如果类中存在初始化语句，就依次执行这些初始化语句。 1编译 -&gt; 加载 -&gt; 链接（验证+准备+解析）-&gt;初始化（使用前的准备）-&gt;使用-&gt; 卸载 类的加载是由类加载器完成的，类加载器包括：根加载器（BootStrap）、扩展加载器（Extension）、系统加载器（System）和用户自定义类加载器（java.lang.ClassLoader的子类）。从Java 2（JDK 1.2）开始，类加载过程采取了父亲委托机制(PDM[Parerent Delegate Mechanism])。PDM更好的保证了Java平台的安全性，在该机制中，JVM自带的Bootstrap是根加载器，其他的加载器都有且仅有一个父类加载器。类的加载首先请求父类加载器加载，父类加载器无能为力时才由其子类加载器自行加载。JVM不会向Java程序提供对Bootstrap的引用。下面是关于几个类加载器的说明： Bootstrap：一般用本地代码实现，负责加载JVM基础核心类库（rt.jar）； Extension：从java.ext.dirs系统属性所指定的目录中加载类库，它的父加载器是Bootstrap； System：又叫应用类加载器，其父类是Extension。它是应用最广泛的类加载器。它从环境变量classpath或者系统属性java.class.path所指定的目录中记载类，是用户自定义加载器的默认父加载器。 什么条件下会触发垃圾回收触发Full GC除直接调用System.gc外，触发Full GC执行的情况有如下四种。 旧生代空间不足 旧生代空间只有在新生代对象转入及创建为大对象、大数组时才会出现不足的现象，当执行Full GC后空间仍然不足，则抛出如下错误：java.lang.OutOfMemoryError: Java heap space，为避免以上两种状况引起的Full GC，调优时应尽量做到让对象在Minor GC阶段被回收、让对象在新生代多存活一段时间及不要创建过大的对象及数组。 Permanet Generation空间满 Permanet Generation中存放的为一些class的信息等，当系统中要加载的类、反射的类和调用的方法较多时，Permanet Generation可能会被占满，在未配置为采用CMS GC的情况下会执行Full GC。如果经过Full GC仍然回收不了，那么JVM会抛出如下错误信息：java.lang.OutOfMemoryError: PermGen space。为避免Perm Gen占满造成Full GC现象，可采用的方法为增大Perm Gen空间或转为使用CMS GC。 CMS GC时出现promotion failed和concurrent mode failure 对于采用CMS进行旧生代GC的程序而言，尤其要注意GC日志中是否有promotion failed和concurrent mode failure两种状况，当这两种状况出现时可能会触发Full GC。 promotion failed是在进行Minor GC时，survivor space放不下、对象只能放入旧生代，而此时旧生代也放不下造成的；concurrent mode failure是在执行CMS GC的过程中同时有对象要放入旧生代，而此时旧生代空间不足造成的。CMS(Concurrent Mark-Sweep)是以牺牲吞吐量为代价来获得最短回收停顿时间的垃圾回收器。对于要求服务器响应速度的应用上，这种垃圾回收器非常适合。 应对措施为：增大survivor space、旧生代空间或调低触发并发GC的比率，但在JDK 5.0+、6.0+的版本中有可能会由于JDK的bug29导致CMS在remark完毕后很久才触发sweeping动作。对于这种状况，可通过设置-XX: CMSMaxAbortablePrecleanTime=5（单位为ms）来避免。 统计得到的Minor GC晋升到旧生代的平均大小大于旧生代的剩余空间 这是一个较为复杂的触发情况，Hotspot为了避免由于新生代对象晋升到旧生代导致旧生代空间不足的现象，在进行Minor GC时，做了一个判断，如果之前统计所得到的Minor GC晋升到旧生代的平均大小大于旧生代的剩余空间，那么就直接触发Full GC。 例如程序第一次触发Minor GC后，有6MB的对象晋升到旧生代，那么当下一次Minor GC发生时，首先检查旧生代的剩余空间是否大于6MB，如果小于6MB，则执行Full GC。 当新生代采用PS GC时，方式稍有不同，PS GC是在Minor GC后也会检查，例如上面的例子中第一次Minor GC后，PS GC会检查此时旧生代的剩余空间是否大于6MB，如小于，则触发对旧生代的回收。 除了以上4种状况外，对于使用RMI来进行RPC或管理的Sun JDK应用而言，默认情况下会一小时执行一次Full GC。可通过在启动时通过- java -Dsun.rmi.dgc.client.gcInterval=3600000来设置Full GC执行的间隔时间或通过-XX:+ DisableExplicitGC来禁止RMI调用System.gc。 对象分配规则 1.对象优先分配在Eden区，如果Eden区没有足够的空间时，虚拟机执行一次Minor GC。 2.大对象直接进入老年代（大对象是指需要大量连续内存空间的对象）。这样做的目的是避免在Eden区和两个Survivor区之间发生大量的内存拷贝（新生代采用复制算法收集内存）。 3.长期存活的对象进入老年代。虚拟机为每个对象定义了一个年龄计数器，如果对象经过了1次Minor GC那么对象会进入Survivor区，之后每经过一次Minor GC那么对象的年龄加1，知道达到阀值对象进入老年区。 4.动态判断对象的年龄。如果Survivor区中相同年龄的所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象可以直接进入老年代。 5.空间分配担保。每次进行Minor GC时，JVM会计算Survivor区移至老年区的对象的平均大小，如果这个值大于老年区的剩余值大小则进行一次Full GC，如果小于检查HandlePromotionFailure设置，如果true则只进行Monitor GC,如果false则进行Full GC。 CAS和AQSCAS（Compare and Swap，比较并交换）：乐观锁的核心算法是CAS（Compareand Swap，比较并交换），它涉及到三个操作数：内存值、预期值、新值。当且仅当预期值和内存值相等时才将内存值修改为新值。这样处理的逻辑是，首先检查某块内存的值是否跟之前我读取时的一样，如不一样则表示期间此内存值已经被别的线程更改过，舍弃本次操作，否则说明期间没有其他线程对此内存值操作，可以把新值设置给此块内存。如图2-5-4-1，有两个线程可能会差不多同时对某内存操作，线程二先读取某内存值作为预期值，执行到某处时线程二决定将新值设置到内存块中，如果线程一在此期间修改了内存块，则通过CAS即可以检测出来，假如检测没问题则线程二将新值赋予内存块。 它也有缺点： ① 观锁只能保证一个共享变量的原子操作。如上例子，自旋过程中只能保证value变量的原子性，这时如果多一个或几个变量，乐观锁将变得力不从心，但互斥锁能轻易解决，不管对象数量多少及对象颗粒度大小。 ② 长时间自旋可能导致开销大。假如CAS长时间不成功而一直自旋，会给CPU带来很大的开销。 ③ ABA问题。CAS的核心思想是通过比对内存值与预期值是否一样而判断内存值是否被改过，但这个判断逻辑不严谨，假如内存值原来是A，后来被一条线程改为B，最后又被改成了A，则CAS认为此内存值并没有发生改变，但实际上是有被其他线程改过的，这种情况对依赖过程值的情景的运算结果影响很大。解决的思路是引入版本号，每次变量更新都把版本号加一。 乐观锁是对悲观锁的改进，虽然它也有缺点，但它确实已经成为提高并发性能的主要手段，而且jdk中的并发包也大量使用基于CAS的乐观锁。 AQS（Abstract Queued Synchronizer）：juc(Java.util.concurrent)里所有的这些锁机制都是基于AQS（Abstract Queued Synchronizer）框架上构建的。 MyBatis与Hibernate区别MyBatis与Hibernate一样是个ORM(Object-relational Mapping)数据库框架。它与Hibernate区别，总结出以下几点： hibernate是全自动，而mybatis是半自动。hibernate完全可以通过对象关系模型实现对数据库的操作，拥有完整的JavaBean对象与数据库的映射结构来自动生成sql。而mybatis仅有基本的字段映射，对象数据以及对象实际关系仍然需要通过手写sql来实现和管理。 hibernate数据库移植性远大于mybatis。hibernate通过它强大的映射结构和hql语言，大大降低了对象与数据库（Oracle、MySQL等）的耦合性，而mybatis由于需要手写sql，因此与数据库的耦合性直接取决于程序员写sql的方法，如果sql不具通用性而用了很多某数据库特性的sql语句的话，移植性也会随之降低很多，成本很高。 Hibernate拥有完整的日志系统，MyBatis则欠缺一些。hibernate日志系统非常健全，涉及广泛，包括：sql记录、关系异常、优化警告、缓存提示、脏数据警告等；而mybatis则除了基本记录功能外，功能薄弱很多。 mybatis相比hibernate需要关心很多细节hibernate配置要比mybatis复杂的多，学习成本也比mybatis高。但也正因为mybatis使用简单，才导致它要比hibernate关心很多技术细节。mybatis由于不用考虑很多细节，开发模式上与传统jdbc区别很小，因此很容易上手并开发项目，但忽略细节会导致项目前期bug较多，因而开发出相对稳定的软件很慢，而开发出软件却很快。hibernate则正好与之相反。但是如果使用hibernate很熟练的话，实际上开发效率丝毫不差于甚至超越mybatis。 sql直接优化上，mybatis要比hibernate方便很多由于mybatis的sql都是写在xml里，因此优化sql比hibernate方便很多。而hibernate的sql很多都是自动生成的，无法直接维护sql；虽有hql，但功能还是不及sql强大，见到报表等变态需求时，hql也歇菜，也就是说hql是有局限的；Hibernate虽然也支持原生sql，但开发模式上却与orm不同，需要转换思维，因此使用上不是非常方便。总之写sql的灵活度上Hibernate不及MyBatis。 总结：MyBatis：小巧、方便、高效、简单、直接、半自动Hibernate：强大、方便、高效、复杂、绕弯子、全自动 mybatis： 入门简单，即学即用，提供了数据库查询的自动对象绑定功能，而且延续了很好的SQL使用经验，对于没有那么高的对象模型要求的项目来说，相当完美。 可以进行更为细致的SQL优化，可以减少查询字段。 缺点就是框架还是比较简陋，功能尚有缺失，虽然简化了数据绑定代码，但是整个底层数据库查询实际还是要自己写的，工作量也比较大，而且不太容易适应快速数据库修改。 二级缓存机制不佳。hibernate： 功能强大，数据库无关性好，O/R映射能力强，如果你对Hibernate相当精通，而且对Hibernate进行了适当的封装，那么你的项目整个持久层代码会相当简单，需要写的代码很少，开发速度很快，非常爽。 有更好的二级缓存机制，可以使用第三方缓存。 缺点就是学习门槛不低，要精通门槛更高，而且怎么设计O/R映射，在性能和对象模型之间如何权衡取得平衡，以及怎样用好Hibernate方面需要你的经验和能力都很强才行。举个形象的比喻：MyBatis：机械工具，使用方便，拿来就用，但工作还是要自己来作，不过工具是活的，怎么使由我决定。 Hibernate：智能机器人，但研发它（学习、熟练度）的成本很高，工作都可以摆脱他了，但仅限于它能做的事。 进程与线程的区别进程和线程的主要差别在于它们是不同的操作系统资源管理方式。进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。 一个程序至少有一个进程,一个进程至少有一个线程. 线程的划分尺度小于进程，使得多线程程序的并发性高。 另外，进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。 线程在执行过程中与进程还是有区别的。每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。 从逻辑角度来看，多线程的意义在于一个应用程序中，有多个执行部分可以同时执行。但操作系统并没有将多个线程看做多个独立的应用，来实现进程的调度和管理以及资源分配。这就是进程和线程的重要区别。 HTTP、TCP/IP、Socket的区别平时还真没有想过这样的问题，第一次回答这个问题时，有那么一丝丝无奈。总结如下： TPC/IP协议是传输层协议，主要解决数据如何在网络中传输，而HTTP是应用层协议，主要解决如何包装数据。关于TCP/IP和HTTP协议的关系，网络有一段比较容易理解的介绍：“我们在传输数据时，可以只使用（传输层）TCP/IP协议，但是那样的话，如果没有应用层，便无法识别数据内容，如果想要使传输的数据有意义，则必须使用到应用层协议，应用层协议有很多，比如HTTP、FTP、TELNET等，也可以自己定义应用层协议。WEB使用HTTP协议作应用层协议，以封装HTTP 文本信息，然后使用TCP/IP做传输层协议将它发到网络上。” 术语TCP/IP代表传输控制协议/网际协议，指的是一系列协议。“IP”代表网际协议，TCP和UDP使用该协议从一个网络传送数据包到另一个网络。把IP想像成一种高速公路，它允许其它协议在上面行驶并找到到其它电脑的出口。TCP和UDP是高速公路上的“卡车”，它们携带的货物就是像HTTP，文件传输协议FTP这样的协议等。 你应该能理解，TCP和UDP是FTP，HTTP和SMTP之类使用的传输层协议。虽然TCP和UDP都是用来传输其他协议的，它们却有一个显著的不同：TCP提供有保证的数据传输，而UDP不提供。这意味着TCP有一个特殊的机制来确保数据安全的不出错的从一个端点传到另一个端点，而UDP不提供任何这样的保证。 HTTP(超文本传输协议)是利用TCP在两台电脑(通常是Web服务器和客户端)之间传输信息的协议。客户端使用Web浏览器发起HTTP请求给Web服务器，Web服务器发送被请求的信息给客户端 static关键字的作用修饰变量一种是被static修饰的变量，叫静态变量或类变量；另一种是没有被static修饰的变量，叫实例变量。两者的区别是：对于静态变量在内存中只有一个拷贝（节省内存），JVM只为静态分配一次内存，在加载类的过程中完成静态变量的内存分配，可用类名直接访问（方便），当然也可以通过对象来访问（但是这是不推荐的）。对于实例变量，没创建一个实例，就会为实例变量分配一次内存，实例变量可以在内存中有多个拷贝，互不影响（灵活）。 修饰方法静态方法可以直接通过类名调用，任何的实例也都可以调用，因此静态方法中不能用this和super关键字，不能直接访问所属类的实例变量和实例方法(就是不带static的成员变量和成员成员方法)，只能访问所属类的静态成员变量和成员方法。 静态代码块static代码块也叫静态代码块，是在类中独立于类成员的static语句块，可以有多个，位置可以随便放，它不在任何的方法体内，JVM加载类时会执行这些静态的代码块，如果static代码块有多个，JVM将按照它们在类中出现的先后顺序依次执行它们，每个代码块只会被执行一次。 静态内部类Java中的嵌套类（Nested Class）分为两种：静态内部类（也叫静态嵌套类，Static Nested Class）和内部类（Inner Class）。内部类我们介绍过很多了，现在来看看静态内部类。什么是静态内部类呢？是内部类，并且是静态（static修饰）的即为静态内部类。只有在是静态内部类的情况下才能把static修复符放在类前，其他任何时候static都是不能修饰类的。 静态内部类的形式很好理解，但是为什么需要静态内部类呢？那是因为静态内部类有两个优点：加强了类的封装性和提高了代码的可读性 静态导包import static静态导入是JDK1.5中的新特性。一般我们导入一个类都用 import com…..ClassName;而静态导入是这样：import static com…..ClassName.*;这里的多了个static，还有就是类名ClassName后面多了个 . ，意思是导入这个类里的静态方法。当然，也可以只导入某个静态方法，只要把 . 换成静态方法名就行了。然后在这个类中，就可以直接用方法名调用静态方法，而不必用ClassName.方法名 的方式来调用。 这种方法的好处就是可以简化一些操作，例如打印操作System.out.println(…);就可以将其写入一个静态方法print(…)，在使用时直接print(…)就可以了。 但是这种方法建议在有很多重复调用的时候使用，如果仅有一到两次调用，不如直接写来的方便 foeach循环的原理从Java 5起，在Java中有了for-each循环，可以用来循环遍历collection和array。For each循环允许你在无需保持传统for循环中的索引，或在使用iterator /ListIterator时无需调用while循环中的hasNext()方法就能遍历collection。for-each循环仅应用于实现了Iterable接口的Java array和Collection类，而且既然所有内置Collection类都实现了java.util.Collection接口，已经继承了Iterable，这一细节通常会被忽略,这点可以在Collection接口的类型声明“ public interface Collection extends Iterable”中看到。所以为了解决上述问题，你可以选择简单地让CustomCollection实现Collection接口或者继承AbstractCollection，这是默认的通用实现并展示了如何同时使用抽象类和接口以获取更好的灵活性。在从任何Collection（例如Map、Set或List）中删除对象时总要使用Iterator的remove方法，也请谨记for-each循环只是标准Iterator代码标准用法之上的一种语法糖（syntactic sugar）而已。 valatile实现原理在多线程并发编程中synchronized和Volatile都扮演着重要的角色，Volatile是轻量级的synchronized，它在多处理器开发中保证了共享变量的“可见性”。可见性的意思是当一个线程修改一个共享变量时，另外一个线程能读到这个修改的值。java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致的更新，线程应该确保通过排他锁单独获得这个变量。Java语言提供了volatile，在某些情况下比锁更加方便。如果一个字段被声明成volatile，java线程内存模型确保所有线程看到这个变量的值是一致的。Volatile变量修饰符如果使用恰当的话，它比synchronized的使用和执行成本会更低，因为它不会引起线程上下文的切换和调度。Volatile变量修饰符如果使用恰当的话，它比synchronized的使用和执行成本会更低，因为它不会引起线程上下文的切换和调度。 有volatile变量修饰的共享变量进行写操作的时候会多第二行汇编代码，通过查IA-32架构软件开发者手册可知，lock前缀的指令在多核处理器下会引发了两件事情。 将当前处理器缓存行的数据会写回到系统内存。这个写回内存的操作会引起在其他CPU里缓存了该内存地址的数据无效。处理器为了提高处理速度，不直接和内存进行通讯，而是先将系统内存的数据读到内部缓存（L1,L2或其他）后再进行操作，但操作完之后不知道何时会写到内存，如果对声明了Volatile变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。但是就算写回到内存，如果其他处理器缓存的值还是旧的，再执行计算操作就会有问题，所以在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器要对这个数据进行修改操作的时候，会强制重新从系统内存里把数据读到处理器缓存里。 这两件事情在IA-32软件开发者架构手册的第三册的多处理器管理章节（第八章）中有详细阐述。 Lock前缀指令会引起处理器缓存回写到内存。Lock前缀指令导致在执行指令期间，声言处理器的 LOCK# 信号。在多处理器环境中，LOCK# 信号确保在声言该信号期间，处理器可以独占使用任何共享内存。（因为它会锁住总线，导致其他CPU不能访问总线，不能访问总线就意味着不能访问系统内存），但是在最近的处理器里，LOCK＃信号一般不锁总线，而是锁缓存，毕竟锁总线开销比较大。在8.1.4章节有详细说明锁定操作对处理器缓存的影响，对于Intel486和Pentium处理器，在锁操作时，总是在总线上声言LOCK#信号。但在P6和最近的处理器中，如果访问的内存区域已经缓存在处理器内部，则不会声言LOCK#信号。相反地，它会锁定这块内存区域的缓存并回写到内存，并使用缓存一致性机制来确保修改的原子性，此操作被称为“缓存锁定”，缓存一致性机制会阻止同时修改被两个以上处理器缓存的内存区域数据。 一个处理器的缓存回写到内存会导致其他处理器的缓存无效。IA-32处理器和Intel 64处理器使用MESI（修改，独占，共享，无效）控制协议去维护内部缓存和其他处理器缓存的一致性。在多核处理器系统中进行操作的时候，IA-32 和Intel 64处理器能嗅探其他处理器访问系统内存和它们的内部缓存。它们使用嗅探技术保证它的内部缓存，系统内存和其他处理器的缓存的数据在总线上保持一致。例如在Pentium和P6 family处理器中，如果通过嗅探一个处理器来检测其他处理器打算写内存地址，而这个地址当前处理共享状态，那么正在嗅探的处理器将无效它的缓存行，在下次访问相同内存地址时，强制执行缓存行填充。 final关键字作用final方法inal也可以声明方法。方法前面加上final关键字，代表这个方法不可以被子类的方法重写。如果你认为一个方法的功能已经足够完整了，子类中不需要改变的话，你可以声明此方法为final。final方法比非final方法要快，因为在编译的时候已经静态绑定了，不需要在运行时再动态绑定。 final类使用final来修饰的类叫作final类。final类通常功能是完整的，它们不能被继承。Java中有许多类是final的，譬如String, Interger以及其他包装类。 final好处final关键字提高了性能。JVM和Java应用都会缓存final变量。final变量可以安全的在多线程环境下进行共享，而不需要额外的同步开销。使用final关键字，JVM会对方法、变量及类进行优化。 final知识点 final关键字可以用于成员变量、本地变量、方法以及类。 final成员变量必须在声明的时候初始化或者在构造器中初始化，否则就会报编译错误。 你不能够对final变量再次赋值。 本地变量必须在声明时赋值。 在匿名类中所有变量都必须是final变量。 final方法不能被重写。 final类不能被继承。 final关键字不同于finally关键字，后者用于异常处理。 final关键字容易与finalize()方法搞混，后者是在Object类中定义的方法，是在垃圾回收之前被JVM调用的方法。 接口中声明的所有变量本身是final的。 final和abstract这两个关键字是反相关的，final类就不可能是abstract的。 final方法在编译阶段绑定，称为静态绑定(static binding)。 没有在声明时初始化final变量的称为空白final变量(blank final variable)，它们必须在构造器中初始化，或者调用this()初始化。不这么做的话，编译器会报错“final变量(变量名)需要进行初始化”。 将类、方法、变量声明为final能够提高性能，这样JVM就有机会进行估计，然后优化。按照Java代码惯例，final变量就是常量，而且通常常量名要大写： 1private final int COUNT = 10; 对于集合对象声明为final指的是引用不能被更改，但是你可以向其中增加，删除或者改变内容。譬如： 1234private final List Loans = new ArrayList();list.add(“home loan”); //validlist.add("personal loan"); //validloans = new Vector(); //not va transient关键字的作用Java 语言规范中提到,transient 关键字用来说明指定属性不进行序列化. Redis的数据结构在 redis 中一共有5种数据结构，那每种数据结构的使用场景都是什么呢？ String——字符串 Hash——字典 List——列表 Set——集合 Sorted Set——有序集合 各自的使用场景： String——字符串String 数据结构是简单的 key-value 类型，value 不仅可以是 String，也可以是数字（当数字类型用 Long 可以表示的时候encoding 就是整型，其他都存储在 sdshdr 当做字符串）。使用 Strings 类型，可以完全实现目前 Memcached 的功能，并且效率更高。还可以享受 Redis 的定时持久化（可以选择 RDB 模式或者 AOF 模式），操作日志及 Replication 等功能。除了提供与 Memcached 一样的 get、set、incr、decr 等操作外，Redis 还提供了下面一些操作： 12345671.LEN niushuai：O(1)获取字符串长度2.APPEND niushuai redis：往字符串 append 内容，而且采用智能分配内存（每次2倍）3.设置和获取字符串的某一段内容4.设置及获取字符串的某一位（bit）5.批量设置一系列字符串的内容6.原子计数器7.GETSET 命令的妙用，请于清空旧值的同时设置一个新值，配合原子计数器使用 Hash——字典在 Memcached 中，我们经常将一些结构化的信息打包成 hashmap，在客户端序列化后存储为一个字符串的值（一般是 JSON 格式），比如用户的昵称、年龄、性别、积分等。这时候在需要修改其中某一项时，通常需要将字符串（JSON）取出来，然后进行反序列化，修改某一项的值，再序列化成字符串（JSON）存储回去。简单修改一个属性就干这么多事情，消耗必定是很大的，也不适用于一些可能并发操作的场合（比如两个并发的操作都需要修改积分）。而 Redis 的 Hash 结构可以使你像在数据库中 Update 一个属性一样只修改某一项属性值。存储、读取、修改用户属性 List——列表List 说白了就是链表（redis 使用双端链表实现的 List），相信学过数据结构知识的人都应该能理解其结构。使用 List 结构，我们可以轻松地实现最新消息排行等功能（比如新浪微博的 TimeLine ）。List 的另一个应用就是消息队列，可以利用 List 的 PUSH 操作，将任务存在 List 中，然后工作线程再用 POP 操作将任务取出进行执行。Redis 还提供了操作 List 中某一段元素的 API，你可以直接查询，删除 List 中某一段的元素 121.微博 TimeLine2.消息队列 Set——集合Set 就是一个集合，集合的概念就是一堆不重复值的组合。利用 Redis 提供的 Set 数据结构，可以存储一些集合性的数据。比如在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。因为 Redis 非常人性化的为集合提供了求交集、并集、差集等操作，那么就可以非常方便的实现如共同关注、共同喜好、二度好友等功能，对上面的所有集合操作，你还可以使用不同的命令选择将结果返回给客户端还是存集到一个新的集合中。 共同好友、二度好友 利用唯一性，可以统计访问网站的所有独立 IP 好友推荐的时候，根据 tag 求交集，大于某个 threshold 就可以推荐 Sorted Set——有序集合和Sets相比，Sorted Sets是将 Set 中的元素增加了一个权重参数 score，使得集合中的元素能够按 score 进行有序排列，比如一个存储全班同学成绩的 Sorted Sets，其集合 value 可以是同学的学号，而 score 就可以是其考试得分，这样在数据插入集合的时候，就已经进行了天然的排序。另外还可以用 Sorted Sets 来做带权重的队列，比如普通消息的 score 为1，重要消息的 score 为2，然后工作线程可以选择按 score 的倒序来获取工作任务。让重要的任务优先执行。 1.带有权重的元素，比如一个游戏的用户得分排行榜2.比较复杂的数据结构，一般用到的场景不算太多 ArrayList、LinkedList、Hashtable、HashMap、ConcurrentHashMap、HashSet的实现原理CopyOnWrite容器和QueueConcurrentHashMap的锁分段技术ConcurrentHashMap的读是否要加锁，为什么ConcurrentHashMap的迭代器是强一致性的迭代器还是弱一致性的迭代器常用设计模式的优缺点Thread和Runnable的区别和联系多次start一个线程会怎么样常用的线程池有几种？这几种线程池之间有什么区别和联系？使用线程池的好处: 减少在创建和销毁线程上所花的时间以及系统资源的开销 如不使用线程池，有可能造成系统创建大量线程而导致消耗完系统内存 以下是Java自带的几种线程池： newFixedThreadPool创建一个指定工作线程数量的线程池。 每当提交一个任务就创建一个工作线程，如果工作线程数量达到线程池初始的最大数，则将提交的任务存入到池队列中。 newCachedThreadPool创建一个可缓存的线程池。 这种类型的线程池特点是： 1).工作线程的创建数量几乎没有限制(其实也有限制的,数目为Interger. MAX_VALUE), 这样可灵活的往线程池中添加线程。 2).如果长时间没有往线程池中提交任务，即如果工作线程空闲了指定的时间(默认为1分钟)，则该工作线程将自动终止。终止后，如果你又提交了新的任务，则线程池重新创建一个工作线程。 newSingleThreadExecutor创建一个单线程化的Executor，即只创建唯一的工作者线程来执行任务，如果这个线程异常结束，会有另一个取代它，保证顺序执行(我觉得这点是它的特色)。 单工作线程最大的特点是可保证顺序地执行各个任务，并且在任意给定的时间不会有多个线程是活动的 。 newScheduleThreadPool创建一个定长的线程池，而且支持定时的以及周期性的任务执行，类似于Timer。 总结： 一.FixedThreadPool是一个典型且优秀的线程池，它具有线程池提高程序效率和节省创建线程时所耗的开销的优点。但在线程池空闲时，即线程池中没有可运行任务时，它不会释放工作线程，还会占用一定的系统资源。 二．CachedThreadPool的特点就是在线程池空闲时，即线程池中没有可运行任务时，它会释放工作线程，从而释放工作线程所占用的资源。但是，但当出现新任务时，又要创建一新的工作线程，又要一定的系统开销。并且，在使用CachedThreadPool时，一定要注意控制任务的数量，否则，由于大量线程同时运行，很有会造成系统瘫痪。 线程池的实现原理是怎么样的？假如有Thread1、Thread2、Thread3、Thread4四条线程分别统计C、D、E、F四个盘的大小，所有线程都统计完毕交给Thread5线程去做汇总，应当如何实现？synchronized和ReentrantLock的区别Java在编写多线程程序时，为了保证线程安全，需要对数据同步，经常用到两种同步方式就是Synchronized和重入锁ReentrantLock。相似点： 这两种同步方式有很多相似之处，它们都是加锁方式同步，而且都是阻塞式的同步，也就是说当如果一个线程获得了对象锁，进入了同步块，其他访问该同步块的线程都必须阻塞在同步块外面等待，而进行线程阻塞和唤醒的代价是比较高的（操作系统需要在用户态与内核态之间来回切换，代价很高，不过可以通过对锁优化进行改善）。区别： 这两种方式最大区别就是对于Synchronized来说，它是java语言的关键字，是原生语法层面的互斥，需要jvm实现。而ReentrantLock它是JDK 1.5之后提供的API层面的互斥锁，需要lock()和unlock()方法配合try/finally语句块来完成。 SynchronizedSynchronized进过编译，会在同步块的前后分别形成monitorenter和monitorexit这个两个字节码指令。在执行monitorenter指令时，首先要尝试获取对象锁。如果这个对象没被锁定，或者当前线程已经拥有了那个对象锁，把锁的计算器加1，相应的，在执行monitorexit指令时会将锁计算器就减1，当计算器为0时，锁就被释放了。如果获取对象锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。 ReentrantLock由于ReentrantLock是java.util.concurrent包下提供的一套互斥锁，相比Synchronized，ReentrantLock类提供了一些高级功能，主要有以下3项： 1.等待可中断，持有锁的线程长期不释放的时候，正在等待的线程可以选择放弃等待，这相当于Synchronized来说可以避免出现死锁的情况。 2.公平锁，多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，Synchronized锁非公平锁，ReentrantLock默认的构造函数是创建的非公平锁，可以通过参数true设为公平锁，但公平锁表现的性能不是很好。 3.锁绑定多个条件，一个ReentrantLock对象可以同时绑定对个对象。 synchronized锁普通方法和锁静态方法、死锁的原理及排查方法String的hashCode()方法是怎么实现的List、Map、Set实现类的源代码ReentrantLock、AQS的源代码AtomicInteger的实现原理，主要能说清楚CAS机制并且AtomicInteger是如何利用CAS机制实现的线程池的实现原理Object类中的方法以及每个方法的作用想要在Spring初始化bean的时候做一些事情该怎么做想要在bean销毁的时候做一些事情该怎么做MyBatis中$和#的区别AVL树、红黑树，可以不了解它们的具体实现，但是要知道什么是二叉查找树、什么是平衡树，AVL树和红黑树的区别。索引使用的是哪种数据结构实现索引为什么要使用树来实现呢Collections.sort方法使用的是哪种排序方法（1）Java虚拟机的内存布局（2）GC算法及几种垃圾收集器（3）类加载机制，也就是双亲委派模型（4）Java内存模型（5）happens-before规则（6）volatile关键字使用规则 谈谈分布式Session的几种实现方式讲一下Session和Cookie的区别和联系以及Session的实现原理XML文档定义有几种形式？它们之间有何本质区别？解析XML文档有哪几种方式？两种定义形式 dtd（文档类型定义） schema（XML模式）； 它们之间有何本质区别？XML Schema和DTD（Document Type Define）都用于文档验证，但二者还有一定区别，本质区别:schema本身是xml的，可以被XML解析器解析(这也是从DTD上发展schema的根本目的)。另外： XML Schema是内容开放模型，可扩展，功能性强；而DTD可扩展性差； XML Schema支持丰富的数据类型，而DTD不支持元素的数据类型，对属性的类型定义也很有限； XML Schema支持命名空间机制，而DTD不支持； XML Schema可针对不同情况对整个XML文档或文档局部进行验证；而DTD缺乏这种灵活性； XML Schema完全遵循XML规范，符合XML语法，可以和DOM结合使用，功能强大；而DTD语法本身有自身的语法和要求，难以学习； 解析XML文档有哪几种方式？有DOM（文档对象模型）,SAX（Simple API for XML）,STAX等 DOM:文档驱动，处理大型文件时其性能下降的非常厉害。这个问题是由DOM的树结构所造成的，这种结构占用的内存较多，而且DOM必须在解析文件之前把整个文档装入内存,适合对XML的随机访问 SAX:不同于DOM,SAX是事件驱动型的XML解析方式。它顺序读取XML文件，不需要一次全部装载整个文件。当遇到像文件开头，文档结束，或者标签开头与标签结束时，它会触发一个事件，用户通过在其回调事件中写入处理代码来处理XML文件，适合对XML的顺序访问，且是只读的。当前浏览器不支持SAX SAXParserFactory factory= SAXParserFactory.newInstance(); SAXParser saxparser= factory.newSAXParser();//创建SAX解析器 MyHandler handler=new MyHandler();//创建事件处理器 saxParser.parse(new File(“Sax_1.xml”),handler);//绑定文件和事件处理者 STAX:Streaming API for XML (StAX) Streaming API for XML (StAX) 是用 Java™ 语言处理 XML 的最新标准。StAX 与其他方法的区别就在于应用程序能够把 XML 作为一个事件流来处理。StAX 允许应用程序代码把这些事件逐个拉出来，而不用提供在解析器方便时从解析器中接收事件的处理程序。 jsp和servlet的区别和联系 jsp经编译后就变成了Servlet.(JSP的本质就是Servlet，JVM只能识别java的类，不能识别JSP的代码,Web容器将JSP的代码编译成JVM能够识别的java类) jsp更擅长表现于页面显示,servlet更擅长于逻辑控制. Servlet中没有内置对象，Jsp中的内置对象都是必须通过HttpServletRequest对象，HttpServletResponse对象以及HttpServlet对象得到.Jsp是Servlet的一种简化，使用Jsp只需要完成程序员需要输出到客户端的内容，Jsp中的Java脚本如何镶嵌到一个类中，由Jsp容器完成。而Servlet则是个完整的Java类，这个类的Service方法用于生成对客户端的响应。联系： JSP是Servlet技术的扩展，本质上就是Servlet的简易方式。JSP编译后是“类servlet”。Servlet和JSP最主要的不同点在于，Servlet的应用逻辑是在Java文件中，并且完全从表示层中的HTML里分离开来。而JSP的情况是Java和HTML可以组合成一个扩展名为.jsp的文件。JSP侧重于视图，Servlet主要用于控制逻辑。 一个“.java”源文件中是否可以包括多个类（不包括内部类）？有什么限制？可以的，一个“.java”源文件里面可以包含多个类，但是只允许有一个public类，并且类名必须和文件名一致。每个编译单元只能有一个public 类。这么做的意思是，每个编译单元只能有一个公开的接口，而这个接口就由其public 类来表示。你可以根据需要，往这个文件里面添加任意多个提供辅助功能的package 权限的类。但是如果这个编译单元里面有两个或两个以上的public 类的话，程序就不知道从哪里导入了，编译器就会报错。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[2016年终总结与规划]]></title>
      <url>%2F2016%2F12%2F01%2F2016-summerize%2F</url>
      <content type="text"><![CDATA[转眼之间一年又过去了，不得不感叹时间过的飞起。时光如梭一点不假，年底了做一个自我总结，也算是对自己的一个交代。这一年改变了什么，什么没有变，想想又忍不住狠狠的装了一把。 技术层面Java 熟悉MySQL、Redis、HBase数据库(存储) 熟悉Spring下Web Socket\Web Service的开发，熟悉了Restful接口的开发 可以应付日常的Java开发任务，与同事合作可以应付较为复杂的开发任务（业务） 了解Apache Kafka，可以使用Ambari独立部署集群环境 可以搭建SSM(Spring\Spring MVC\Mybatis)框架 熟悉Java常见的开发工具(Eclipse\Maven\Intellij Idea\Gradle)的使用，并应用于日常开发中 进一步熟悉了GitHub、StackoverFlow网站和Git工具，开发必不可少的朋友 熟悉了Jira、Redmine等项目管理工具，并部署并应用于日常开发中 进一步熟悉持续集成思想，熟悉了Jenkins等持续集成工具，并应用于日常开发中 使用Hexo博客模板引擎搭建了自己的博客，编写开发与生活总结文章 使用LaTex编写了开发总结 熟悉了Wireshark和Tcpdump等网络包分析工具的使用 重新使用起了Linux操作系统 进一步熟悉TCP、HTTP协议、交通部T808协议 阅读了《代码大全》等编程相关书籍 学习了网页开发常见工具(Fiddler、FireBug)的使用，会根据原理进行分析 熟悉了前端的JavaScript、HTML、CSS知识 注重效率的培养，包括Intellij Idea的快捷键，JRebel等等工具的使用 Python 了解了Python的基本语法 尝试写了简单的Python代码 未来规划目前还是比较浮躁的，希望对Java底层有更加深入的了解(JVM)，不止于停留在表面，Java性能调优方面的经验、数据库性能调优经验，只有更深入的了解原理，看到错误时才可以一步一步分析原因，而不是简单的停留在猜测上。从现在开始静下心来，需要选择一个方向努力了。以热爱开源，热爱分享为指导思想，不断学习。乐于助人与寻求别人的帮助，别人没有义务帮助谁，俺有义务助人啊，软件构建里没有谁是全能型选手，取长补短即可。快乐工作，该装的A-C还是得装，一个也不能少，虽然换工作比较频繁，但是收获最大的还是认识一帮开心的朋友，工作也不是那么无趣，工作经验可以没有，换工作的经验必须得有啊。将来会多阅读人文类书籍，好多年一直抱着技术书籍在看，可见功利心还是蛮强的，好久没有阅读人文类书籍了，值得引起重视。不要原地踏步，不要重复，不要无聊。需要进步，需要创造，需要有趣。不断提高效率，想当初修改代码后不断重启项目，每次重启大概耗时1分钟，到后面完全在1秒内搞定，想想也有点小激动。希望以后在开发中由于不熟悉花1天去解决的事情，往后只需要1秒即可。来年的小计划如下： 阅读不少于5本人文类书籍 阅读《编译原理》、《算法导论》、《代码大全》 结识朋友 学习Java底层知识(原谅不能量化)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring MVC拦截规则]]></title>
      <url>%2F2016%2F12%2F01%2Fspringmvc-filter%2F</url>
      <content type="text"><![CDATA[在Spring MVC中，需要配置Mapping规则。 拦截.do、.htm这是最传统的方式，最简单也最实用。不会导致静态文件（jpg,js,css）被拦截。 1234&lt;servlet-mapping&gt; &lt;servlet-name&gt;dolphin&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 拦截/1234&lt;servlet-mapping&gt; &lt;servlet-name&gt;dolphin&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 可以实现现在很流行的REST风格。很多互联网类型的应用很喜欢这种风格的URL。 弊端：会导致静态文件（jpg,js,css）被拦截后不能正常显示。想实现REST风格，事情就是麻烦一些。后面有解决办法还算简单。 拦截/*1234&lt;servlet-mapping&gt; &lt;servlet-name&gt;dolphin&lt;/servlet-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 这是一个错误的方式，请求可以通过DispatcherServlet走到Action中，但是返回的内容，如返回的jsp还会再次被拦截，这样导致404错误，即访问不到jsp，也无法访问到html静态页面。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[spring-step-by-step（三）-权限管理]]></title>
      <url>%2F2016%2F11%2F29%2Fspring-step-by-step-privillege%2F</url>
      <content type="text"><![CDATA[配置过滤器为了在项目中使用Spring Security控制权限，在项目的web.xml文件中，添加过滤器，就可以控制对这个项目的每个请求了。 12345678&lt;filter&gt; &lt;filter-name&gt;springSecurityFilterChain&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;springSecurityFilterChain&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; Spring Security数据库表Spring Security默认情况下需要两张表，用户表和权限表。以下是mysql中的建表语句： 一个简单的HelloWorld]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java单元测试]]></title>
      <url>%2F2016%2F11%2F29%2Fjava-unit-test%2F</url>
      <content type="text"><![CDATA[在Intellij Idea中按下快捷键:Ctrl + Alt + T(Test)，弹出如下界面，选择需要创建测试的方法。 选择完毕后会自动生成一个测试类，编写相应的测试代码即可。 12345678910111213import org.junit.Test;/** * Created by jiangxiaoqiang on 2016/11/29. */public class VehicleMessageHandlerTest &#123; @Test public void parseAlarm() throws Exception &#123; VehicleMessageHandler vehicleMessageHandler = new VehicleMessageHandler(); String result = vehicleMessageHandler.parseAlarm(1232131); System.out.print(result); &#125;&#125; 在代码编辑器中右键，选择Debug ‘parseAlarm’菜单即可进行单元测试。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Sonarcube代码优化]]></title>
      <url>%2F2016%2F11%2F28%2Fsonarcube%2F</url>
      <content type="text"><![CDATA[Generic exceptions should never be thrownUsing such generic exceptions as Error, RuntimeException, Throwable, and Exception prevents calling methods from handling true, system-generated exceptions differently than application-generated errors. 不应该写成如下方式： 123public void foo(String bar) throws Throwable &#123; // Noncompliant throw new RuntimeException("My Message"); // Noncompliant&#125; 应该写成如下方式： 123public void foo(String bar) &#123; throw new MyOwnRuntimeException("My Message");&#125; Instance methods should not write to “static” fields实例化方法不写静态字段。 优化前： 1234@Overridepublic void setApplicationContext(ApplicationContext context) throws BeansException &#123; SpringApplicationContextHolder.context = context;&#125; 优化后： 12345678public static void setApplicationContextImpl(ApplicationContext context) &#123; SpringApplicationContextHolder.context = context;&#125;@Overridepublic void setApplicationContext(ApplicationContext context) throws BeansException &#123; setApplicationContextImpl(context);&#125; Neither “Math.abs” nor negation should be used on numbers that could be “MIN_VALUE”Math.abs(Integer.MIN_VALUE)的值还是其本身。通过查阅Java的API文档，我们看到对abs(int a)运算，“如果参数等于 Integer.MIN_VALUE 的值（即能够表示的最小负 int 值），那么结果与该值相同且为负。 优化前： 1int newlineId = Math.abs(randomPointId.replaceAll("-", "").hashCode()); 优化后： 1234567String hashedSourceId = sourceId.replaceAll("-", "");if (hashedSourceId.hashCode() != Integer.MIN_VALUE) &#123; Integer hashedId = Math.abs(hashedSourceId.hashCode()); hashedIds.add(hashedId.toString());&#125; else &#123; hashedIds.add(String.valueOf(Integer.MAX_VALUE));&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[推荐书《代码大全》(Code Complete)]]></title>
      <url>%2F2016%2F11%2F27%2Fcode-complete%2F</url>
      <content type="text"><![CDATA[今天晚上下班时，朋友跟我讲：应该尽量使用局部变量，避免使用全局变量。 我说：这是编程常识啊，怎么还不知道呢。在心里暗暗佩服自己，这个B装的真的是完美，完全找不到任何破绽。 闲言少叙，如果想到知道更多关于编程的经验分享，那么我推荐你阅读《代码大全》(Code Complete)。 一开始是由于好奇，什么书吹的这么神奇？等到阅读后，发现再多的溢美之词也不足以形容。 是自己真的阅读过才在这里鼓吹，开始还以为是实现各种功能的各种代码片段组成的一本书籍。心里暗自庆幸，这下拷贝更便捷了。但是阅读之后才知道，它是另一种境界了，代码片段只是表面招式而已，而这本书里所总结的经验和概括的方法论，是帮助你修炼内力。不论你是做Android开发，前端开发、后端开发，Java、C、C++，都可以从中获得灵感。 阅读此书后，你会知道什么代码是优秀的代码，什么样的写法是不太合适的。他是编程通用的方法论，不会跟你争论哪门语言是最好的语言，括号是不是应该换行。即使工作了十几年的老程序员，当你阅读他的代码时，也不会有膜拜的感觉。因为你可以看出来哪些地方写的好，哪些地方有待改进的空间。并不代表他的水平有问题。 里面还有一段关于注释的讨论，对于平时的代码是否要需要注释，会有更加深入的理解，非常经典。 由于此书实在是爱不释手，中文英文版本都买了。推荐你也一定要阅读。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[面试经历-重庆]]></title>
      <url>%2F2016%2F11%2F26%2Finterview-experience%2F</url>
      <content type="text"><![CDATA[最近有换工作的打算，把面试经历分享出来，希望可以给求职的朋友一个参考。工作就像婚姻，是一个双向选择的过程，不找最好的，要找最适合自己的。 重庆市宽景网络技术有限公司公司地址在渝北区龙溪街道金山路18号中渝都会首站4幢12-5，嘉州路站下车3B出口步行500米即到。交通还是蛮方便的。这幢大厦是居民楼，但是大堂看起来又有点写字楼的味道(商住两用)，富丽堂皇，有点高端。 进去后，由于早先已到了另一位面试者，等待了十几分钟左右。面试时，给看了一个后台相关的网页，大概问了一下做这样一个网页需要多长时间。是否熟悉Spring Boot，是否有自己的脚手架(开发框架，具备一些基础功能，如权限等)。面试官也反复强调需要招聘的一个技术经理级别的人。同时也强调，技术不是特别复杂，就是一些简单的增删改(CRUD)即可。公司需要找一个手头能有一个带有基础功能系统，或者能够在极短时间里搭建一个简单系统的人。由于没有脚手架，做到快速开发，开箱即用。最后的面试结果是被面试官婉拒了。 大致了解了宽景网络公司做的业务和技术要求，如果对技术追求不是特别苛刻，能够乐于且擅长做CRUD的同学，是一个不错的选择。 官方网站：这里 面试评价：6 重庆同方融达信息科技有限公司重庆同方融达信息科技有限公司系同方股份有限公司全资子公司，成立于2011年9月。面试过程大概持续了将近半个小时。两位面试官，其中一位年级稍长，年纪稍长的那位估计是领导罗，不停的发问，而且还打断说话。了解了以前做的项目的情况，问自己的优点，和缺点。优点就讲了：善于总结。缺点没有回答上来，有点套路的感觉。记得以前说的回答此类问题有固定套路的，一时想不起来了。比如什么工作必须追求完美什么的，总体思路就是把优点说成缺点。这个套路还没有玩转。本来想了解公司的业务情况的，面试官不愿意透露，也就作罢了。估计是政府项目比较多，项目和现在做的这一块还是比较匹配的。整体印象还是不错的。 官方网站：这里 面试评价：6 重庆致树网络科技有限公司和公司的合伙人之一聊了将近2个小时，不得不说创业公司还是蛮有意思的,也非常诚恳。天，说起产品和前景来真的是停不下来阿。虽然在心里咆哮，兄弟，淡定淡定，公司还没有赚钱呢！但是表面还是表现的像：兄弟，好样的，咱这就橹起袖子干一票吧。 公司主要做文化类产品的直播，直播一些手工艺品的制作过程。可能会衍生到更多品类的文化产品上，想法还是蛮新奇的。 官方网站：无 面试评价：7]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis主键失效原理]]></title>
      <url>%2F2016%2F11%2F24%2Fredis-expire-strategy%2F</url>
      <content type="text"><![CDATA[Redis 中的主键失效是如何实现的，即失效的主键是如何删除的？实际上，Redis 删除失效主键的方法主要有两种： 消极方法（passive way），在主键被访问时如果发现它已经失效，那么就删除它 积极方法（active way），周期性地从设置了失效时间的主键中选择一部分失效的主键删除 消极方法(Passive Way)消极方法也叫惰性删除，在大致了解了 Redis 是如何维护设置了失效时间的主键之后，我们就先来看一看 Redis 是如何实现消极地删除失效主键的。如下代码给出了一个名为 expireIfNeeded 的函数，这个函数在任何访问数据的函数中都会被调用，也就是说 Redis 在实现 GET、MGET、HGET、LRANGE 等所有涉及到读取数据的命令时都会调用它，它存在的意义就是在读取数据之前先检查一下它有没有失效，如果失效了就删除它。惰性删除也即是消极删除(Passive Way)的源代码如下所示： 123456789101112131415161718192021222324252627282930313233343536int expireIfNeeded(redisDb *db, robj *key) &#123; mstime_t when = getExpire(db,key); mstime_t now; if (when &lt; 0) return 0; /* No expire for this key */ /* Don't expire anything while loading. It will be done later. */ if (server.loading) return 0; /* If we are in the context of a Lua script, we claim that time is * blocked to when the Lua script started. This way a key can expire * only the first time it is accessed and not in the middle of the * script execution, making propagation to slaves / AOF consistent. * See issue #1525 on Github for more information. */ now = server.lua_caller ? server.lua_time_start : mstime(); /* If we are running in the context of a slave, return ASAP: * the slave key expiration is controlled by the master that will * send us synthesized DEL operations for expired keys. * * Still we try to return the right information to the caller, * that is, 0 if we think the key should be still valid, 1 if * we think the key is expired at this time. */ if (server.masterhost != NULL) return now &gt; when; /* Return when this key has not expired */ if (now &lt;= when) return 0; /* Delete the key */ server.stat_expiredkeys++; propagateExpire(db,key,server.lazyfree_lazy_expire); notifyKeyspaceEvent(NOTIFY_EXPIRED, "expired",key,db-&gt;id); return server.lazyfree_lazy_expire ? dbAsyncDelete(db,key) : dbSyncDelete(db,key);&#125; 积极方法(Active Way)以上我们通过对expireIfNeeded函数的介绍了解了Redis是如何以一种消极的方式删除失效主键的，但是仅仅通过这种方式显然是不够的，因为如果某些失效的主键迟迟等不到再次访问的话，Redis就永远不会知道这些主键已经失效，也就永远也不会删除它们了，这无疑会导致内存空间的浪费。因此，Redis还准备了一招积极的删除方法，该方法利用Redis的时间事件来实现，即每隔一段时间就中断一下完成一些指定操作，其中就包括检查并删除失效主键。实现的代码在expire.c文件中，如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121void activeExpireCycle(int type) &#123; /* This function has some global state in order to continue the work * incrementally across calls. */ static unsigned int current_db = 0; /* Last DB tested. */ static int timelimit_exit = 0; /* Time limit hit in previous call? */ static long long last_fast_cycle = 0; /* When last fast cycle ran. */ int j, iteration = 0; int dbs_per_call = CRON_DBS_PER_CALL; long long start = ustime(), timelimit; if (type == ACTIVE_EXPIRE_CYCLE_FAST) &#123; /* Don't start a fast cycle if the previous cycle did not exited * for time limt. Also don't repeat a fast cycle for the same period * as the fast cycle total duration itself. */ if (!timelimit_exit) return; if (start &lt; last_fast_cycle + ACTIVE_EXPIRE_CYCLE_FAST_DURATION*2) return; last_fast_cycle = start; &#125; /* We usually should test CRON_DBS_PER_CALL per iteration, with * two exceptions: * * 1) Don't test more DBs than we have. * 2) If last time we hit the time limit, we want to scan all DBs * in this iteration, as there is work to do in some DB and we don't want * expired keys to use memory for too much time. */ if (dbs_per_call &gt; server.dbnum || timelimit_exit) dbs_per_call = server.dbnum; /* We can use at max ACTIVE_EXPIRE_CYCLE_SLOW_TIME_PERC percentage of CPU time * per iteration. Since this function gets called with a frequency of * server.hz times per second, the following is the max amount of * microseconds we can spend in this function. */ timelimit = 1000000*ACTIVE_EXPIRE_CYCLE_SLOW_TIME_PERC/server.hz/100; timelimit_exit = 0; if (timelimit &lt;= 0) timelimit = 1; if (type == ACTIVE_EXPIRE_CYCLE_FAST) timelimit = ACTIVE_EXPIRE_CYCLE_FAST_DURATION; /* in microseconds. */ for (j = 0; j &lt; dbs_per_call; j++) &#123; int expired; redisDb *db = server.db+(current_db % server.dbnum); /* Increment the DB now so we are sure if we run out of time * in the current DB we'll restart from the next. This allows to * distribute the time evenly across DBs. */ current_db++; /* Continue to expire if at the end of the cycle more than 25% * of the keys were expired. */ do &#123; unsigned long num, slots; long long now, ttl_sum; int ttl_samples; /* If there is nothing to expire try next DB ASAP. */ if ((num = dictSize(db-&gt;expires)) == 0) &#123; db-&gt;avg_ttl = 0; break; &#125; slots = dictSlots(db-&gt;expires); now = mstime(); /* When there are less than 1% filled slots getting random * keys is expensive, so stop here waiting for better times... * The dictionary will be resized asap. */ if (num &amp;&amp; slots &gt; DICT_HT_INITIAL_SIZE &amp;&amp; (num*100/slots &lt; 1)) break; /* The main collection cycle. Sample random keys among keys * with an expire set, checking for expired ones. */ expired = 0; ttl_sum = 0; ttl_samples = 0; if (num &gt; ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP) num = ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP; while (num--) &#123; dictEntry *de; long long ttl; if ((de = dictGetRandomKey(db-&gt;expires)) == NULL) break; ttl = dictGetSignedIntegerVal(de)-now; if (activeExpireCycleTryExpire(db,de,now)) expired++; if (ttl &gt; 0) &#123; /* We want the average TTL of keys yet not expired. */ ttl_sum += ttl; ttl_samples++; &#125; &#125; /* Update the average TTL stats for this database. */ if (ttl_samples) &#123; long long avg_ttl = ttl_sum/ttl_samples; /* Do a simple running average with a few samples. * We just use the current estimate with a weight of 2% * and the previous estimate with a weight of 98%. */ if (db-&gt;avg_ttl == 0) db-&gt;avg_ttl = avg_ttl; db-&gt;avg_ttl = (db-&gt;avg_ttl/50)*49 + (avg_ttl/50); &#125; /* We can't block forever here even if there are many keys to * expire. So after a given amount of milliseconds return to the * caller waiting for the other active expire cycle. */ iteration++; if ((iteration &amp; 0xf) == 0) &#123; /* check once every 16 iterations. */ long long elapsed = ustime()-start; latencyAddSampleIfNeeded("expire-cycle",elapsed/1000); if (elapsed &gt; timelimit) timelimit_exit = 1; &#125; if (timelimit_exit) return; /* We don't repeat the cycle if there are less than 25% of keys * found expired in the current DB. */ &#125; while (expired &gt; ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP/4); &#125;&#125; EXPIRE key seconds redis学习笔记——Redis过期键的删除策略 redis github]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring中单用户登录]]></title>
      <url>%2F2016%2F11%2F23%2Fsingle-user-login%2F</url>
      <content type="text"><![CDATA[在web.xml文件的配置如下： 12345&lt;listener&gt; &lt;listener-class&gt; org.springframework.security.web.session.HttpSessionEventPublisher &lt;/listener-class&gt;&lt;/listener&gt; 单用户登录，当其他用户登录时，自动退出，在Spring-Security.xml中作如下配置： 123456789&lt;!-- 配置SpringSecurity的http安全服务 --&gt;&lt;sec:session-management invalid-session-url="/login?type=expired"&gt; &lt;!-- 单用户登陆 --&gt; &lt;!-- 仅配置max-sessions="1",则第二次登录会让第一次登录失效。 --&gt; &lt;!-- 同时配置max-sessions="1" error-if-maximum-exceeded="true"，则可以防止第二次登录--&gt; &lt;!-- max-sessions="1" 其中1表示一个帐号可同时登录的次数 --&gt; &lt;sec:concurrency-control max-sessions="1" error-if-maximum-exceeded="false" expired-url="/login?type=expired" /&gt;&lt;/sec:session-management&gt; 在标签中加入concurrency-control配置，设置max-sessions=1。当另一个用户登录之后，当前用户操作时会自动跳转到登录页面。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Google搜索技巧]]></title>
      <url>%2F2016%2F11%2F20%2Fpaper-searching%2F</url>
      <content type="text"><![CDATA[如今几乎每天都会使用搜索引擎，虽然没有必要了解搜索引擎的工作原理，但是了解一些常见的搜索技巧可以快速的帮助自己找到内容，节省不少时间。有一种技术叫Google Hacking，就是直接使用Google搜索出有价值的信息，不信可以直接在搜索引擎中输入自己的名字，相信出来的结果会让你相当震撼的。曾经使用Google直接把喜欢的一个女孩子的身份证号码都搜索出来的，其实获取这样的信息还是比较容易的。这里以Google为例，其他搜索引擎原理差不多。今天需要了解关于静脉可视化技术的发展状况。首先明确中文关键字是：静脉可视化，英文关键字是：Intravenous visualization。 根据文件类型搜索搜索所有关于静脉可视化的doc文档，在Google中输入如下查询关键字。 1静脉 可视化 filetype:pdf 总共有3600个符合条件的结果。 搜索所有关于静脉可视化的pdf文档。 1静脉 可视化 filetype:doc 仿佛没有什么值得挖掘的内容，有一篇《静脉可视化装置减少静脉被重复扎针的痛苦 - 后花园网文》勉强符合要求。此时可以精确搜索，全字匹配，缩小搜索范围。 1&quot;静脉可视化&quot; filetype:pdf 其中双引号表示精确匹配，共出来有3条结果，都不是想要的。看来不应该那么学术化，学术资料都被某一帮人锁起来了，必须要给钱才能看，个人认为是非常不合理的。此时直接放弃学术资料。直接在搜索框里输入如下关键字： 1静脉可视化 共有58500个结果，如下图所示。这些才是想要的结果。 排除关键字假设要寻找“红外热成像技术”的相关论文，在Google学术(Google Scholar)里输入关键字“红外热成像技术”，出来65400条结果。如果想要排除红外热成像技术在电力方面的应用的论文，键入如下关键字： 12红外热成像技术 -电力红外热成像技术 -步态 -夜视 -反潜 -制冷 -问题 -看法 -测温 -隐身 -机载 -煤 -保温 -车 -飞机 -水泥 -森林 -几何 -电力 -金属 -视频 -建筑 -军事 -无损 -火焰 -农业 -物理 -电气 -电路 参考： Google 搜索技巧]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[HBase重复数据问题解决思路]]></title>
      <url>%2F2016%2F11%2F19%2Fhbase-dulplicate-data%2F</url>
      <content type="text"><![CDATA[解决思路1可以设计消息主键，每次插入数据时，查询判断此消息是否已经写入，可以排除重复写入数据问题。但是此种手段需要查询HBase，由于写入并发量相对较高&gt;1000条/s。会影响写入速度。由于此时有单点的Redis,由于数据到达的时间相隔不会太长，可以考虑将数据持久化到Redis中，插入时，到Redis中查询，Redis中查询复杂度为常数，基本可以忽略查询对性能的影响。判断是否已经写入。而写入到Redis中的数据可以设置过期时间，可以自动清除。 采用此种方式，需要考虑，新的消费者不能消费数据from beginning。如果新消费者从头开始消费数据，此时Redis缓存已经清除，也会重复写入数据。需要在写入时作相应判断，超过Redis缓存到达的数据不用写入。 12345678910111213141516171819202122232425262728public void positionInfoHandler(KafkaRecievedLocationMessage&lt;KafkaRecievedLocationMessageBody&gt; kafkaRecievedLocationMessage, String groupId) &#123; if (groupId != null &amp;&amp; groupId.equals(PublicVariable.HBASE_GROUPID)) &#123; /** * 将最近的位置数据缓存到Redis中 * 判断重复数据查询Redis数据库 * Redis的Key设计采用topic+消息类型+时间来标志消息的唯一性 * 如果消息在Redis存在，代表已经写入，将忽略 * 后期可以考虑添加事务 * */ String gpsTime = String.valueOf(kafkaRecievedLocationMessage.getData().getMsgBody().getGps_time()); String positionKey = kafkaRecievedLocationMessage.getDesc().getTopic() + "-" + kafkaRecievedLocationMessage.getDesc().getMsgID() + "-" + gpsTime; String persistStatus = RedisHelper.get(positionKey, PublicVariable.REDIS_DEFAULT_DATABASE); if (StringUtils.isBlank(persistStatus)) &#123; /** * 缓存的位置数据将在12小时后过期 * 只有第一次设置会成功 * Redis本身是原子操作 */ String result = RedisHelper.setValueNx(positionKey, "1", 2592000, PublicVariable.REDIS_DEFAULT_DATABASE); if (result != null &amp;&amp; result.equals("OK")) &#123; //保存数据 persistOilImpl(kafkaRecievedLocationMessage, groupId); &#125; &#125; &#125;&#125; 设置Redis的Key，方法setValueNx，如果Key设置成功会返回OK，如果设置失败则返回null： 1234EX seconds -- Set the specified expire time, in seconds.PX milliseconds -- Set the specified expire time, in milliseconds.NX -- Only set the key if it does not already exist.XX -- Only set the key if it already exist. 1234567891011121314151617181920212223242526272829/** * 设置数据 * 如果已经存在Key * 键已经存在，则设置失败 * * @param key //key * @param value //值 * @param timeOut //过期时间 * @param db //数据库序号 */public static String setValueNx(String key, String value, Integer timeOut, Integer db) &#123; Jedis jredis = null; String result = null; try &#123; JedisPool poolItem = pools.get(db); jredis = poolItem.getResource(); if (timeOut &gt; 0) &#123; jredis.expire(key, timeOut); &#125; result = jredis.set(key, value, "nx"); &#125; catch (Exception e) &#123; log.error("set value encount an error", e); &#125; finally &#123; if (jredis != null) &#123; jredis.close(); &#125; &#125; return result;&#125; SET key value [EX seconds] [PX milliseconds] [NX|XX]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[curl使用]]></title>
      <url>%2F2016%2F11%2F19%2Fcurl-using%2F</url>
      <content type="text"><![CDATA[简介curl is a tool to transfer data from or to a server, using one of the supported protocols (DICT, FILE, FTP, FTPS, GOPHER, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMB, SMBS, SMTP, SMTPS, TELNET and TFTP). The command is designed to work without user interaction. curl offers a busload of useful tricks like proxy support, user authentication, FTP upload, HTTP post, SSL connections, cookies, file transfer resume, Metalink, and more. As you will see below, the number of features will make your head spin! 查看网页源码直接在curl命令后加上网址，就可以看到网页源码。我们以网址www.jiangxiaoqiang.com为例，输入如下命令行： 1curl www.jiangxiaoqiang.com 返回的结果为： 1234567&lt;html&gt;&lt;head&gt;&lt;title&gt;301 Moved Permanently&lt;/title&gt;&lt;/head&gt;&lt;body bgcolor="white"&gt;&lt;center&gt;&lt;h1&gt;301 Moved Permanently&lt;/h1&gt;&lt;/center&gt;&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;&lt;/body&gt;&lt;/html&gt; Ｃookie使用--cookie参数，可以让curl发送cookie，如下命令行所示。 1curl --cookie "name=xxx" www.example.com -c cookie-file可以保存服务器返回的cookie到文件，-b cookie-file可以使用这个文件作为cookie信息，进行后续的请求。 12curl -c cookies http://example.comcurl -b cookies http://example.com 如果服务端采用Cookie验证登录请求。那么可以首先用浏览器登录网页，找到相应的Cookie，如下图所示： 当Curl请求时，附加上Cookie就可以模拟浏览器的登录请求动作了，如下命令所示： 1curl --cookie "cc-o-t=Q2ZWTGp3bFQrTmtOcG9pcTJ4SFV2VnpmNzNhaEhMbktoQWlXeWVyaGhVS3JWeVJEcEprTXBsbldDS3VCN01CeVRFNGxYVTZmQ2hIbHhTSy8zKzF3cDQvYjEwRncxUXgrQ3pMWC9XeTYzcXFrQ1NqQWkrdXRhMUJEc3RZV3ExK0k" http://localhost:28080/api/xzss/detail/1 请求API数据比如需要在请求头中添加Key、Value键值对，可以添加-H(Header)参数，如下命令行所示： 1curl -H "APPID:123" http://localhost:28080/api/xysj?xdrShxym=addd 如果是需要发送多个请求头，那么多次指定-H参数即可,如下命令所示。 1curl -H "APPID:123" -H "TIMESTAMP:2016-12-19 16:58:02" -H "ECHOSTR:sdsaasf" -H "TOKEN:sdsss" http://localhost:28080/api/xysj?xdrShxym=addd 参数在Restful多参数请求时，需要使用转义符号。url为http://mywebsite.com/index.PHP?a=1&amp;b=2&amp;c=3时，web形式下访问url地址，使用$_GET是可以获取到所有的参数。然而在Linux下 1curl http://mywebsite.com/index.php?a=1&amp;b=2&amp;c=3 $_GET只能获取到参数a,由于url中有&amp;，其他参数获取不到，在linux系统中&amp;会使进程系统后台运行,必须对&amp;进行下转义才能$_GET获取到所有参数。 1curl http://mywebsite.com/index.php?a=1\&amp;b=2\&amp;c=3 这一个细节需要注意。 下载文件如下命令是使用curl下载XX-net工具。 1curl -o xxnet.zip https://codeload.github.com/XX-net/XX-Net/zip/3.2.8 -o, –output ，文件的保存名称。 提交POST请求使用Curl提交Post请求如下： 1curl -H "APPID:hlb11529c136998cb6" -H "TIMESTAMP:2016-12-19 16:58:02" -H "ECHOSTR:sdsaasf" -H "TOKEN:14d45648c62a746ae9dd9b90c03c50893061222d" -H "Accept:application/json" -H "Accept:application/json" -H "Content-Type:application/json" -X POST -d '&#123;"id":1&#125;' http://localhost:28080/api/xzss/savejson 使用Curl请求时，默认是Get请求，如果需要显示指定请求类型，那么需要加上-X(–request)参数。 参考资料： curl网站开发指南]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[迁移解析服务]]></title>
      <url>%2F2016%2F11%2F16%2Fmigration-parse-service%2F</url>
      <content type="text"><![CDATA[迁移解析服务是将服务在不同主机上部署。 启动服务在解析服务器上，使用如下命令启动Tomcat: 12cd /opt/tomcat/bin./catalina.sh start 查看输出日志，检查服务是否正常启动： 12cd /opt/tomcat/logstail -f catalina.out 配置映射主机使用如下命令打开hosts文件： 1vim /etc/hosts 添加主机映射。 12192.168.24.195 hostname1192.168.24.226 hostname2 添加主机映射之后，从新启动解析服务即可，不必重新启动计算机。 验证迁移部署完毕后一定要验证，因为在解析服务器上日志打印OK并不代表数据成功写入Kafka集群中，所以验证最后验证迁移是否成功，在Kafka服务器中，切换到Kafka的目录： 1cd /usr/hdp/2.4.3.0-227/kafka/bin 使用如下命令查看迁移是否成功： 1./kafka-console-consumer.sh --zookeeper localhost:2181 --topic 0720688 其中，0720688是需要消费的主题。启动解析服务后，将解析服务Tomcat日志中写入的主题拷贝进命令行中启动消费者，如果能够成功消费到数据，代表迁移成功。否则，迁移失败。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redmine安装]]></title>
      <url>%2F2016%2F11%2F15%2Fredmine-install%2F</url>
      <content type="text"><![CDATA[Redmine简介Redmine 是一个网页界面的项目管理与缺陷跟踪管理系统的自由及开放源代码软件工具。它集成了项目管理所需的各项功能：日历、燃尽图和甘特图 以协助可视化表现项目与时间限制，问题跟踪和版本控制。此外，Redmine也可以同时处理多个项目。Redmine 是以 Ruby on Rails 撰写的架构，它横跨多个平台与数据库，它的设计很明显是受一些类似功能软件包的Trac所影响。此外，它也是Bitnami 应用库的一部分。 安装的环境是CentOS 7.2，查看CentOS版本可以使用命令： 1cat /etc/redhat-release 安装前查看Redmine版本的对应关系。 安装依赖包(Install pre-dependencies)使用如下命令安装依赖包。 1yum -y install libyaml-devel zlib-devel curl-devel openssl-devel httpd-devel apr-devel apr-util-devel gcc ruby-devel gcc-c++ make postgresql-devel ImageMagick-devel sqlite-devel perl-LDAP mod_perl perl-Digest-SHA 安装Ruby(Install Ruby)12cd ~/Downloads # YOUR FOLDER OF CHOICEftp ftp.ruby-lang.org 如果ftp未安装，输入如下命令安装ftp。 1yum install -y ftp 从FTP上获取Ruby安装文件： 12345ftp&gt; Anonymous # USERLOGINftp&gt; 'none', just hit Enter # NO PASSWORDftp&gt; cd /pub/rubyftp&gt; get get ruby-1.8.7-p358.tar.gz # XXX is currently 358, as of 03/2012ftp&gt; bye 也可以使用wget下载安装文件，此处采用此种方式，使用wget命令下载能够看到文件下载的进度，FTP方式等了许久没有响应，遂放弃，采用wget下载： 1wget ftp://ftp.ruby-lang.org/pub/ruby/ruby-1.8.7-p358.tar.gz 解压安装文件： 1tar zxvf ruby-1.8.7-p358.tar.gz 安装： 1234cd ruby-1.8.7.p358./configuremakemake install 编译Ruby源码需要GCC(GNU C Compiler)，如果编译时提示没有安装GCC，输入如下命令安装： 1yum install gcc gcc-c++ kernel-devel 安装Ruby(Install Ruby (Option 2))1yum install ruby 检查安装是否成功： 1ruby -v 安装RubyGems 1.4.2RubyGems是一个方便而强大的Ruby程序包管理器(RubyGems is a package management framework for Ruby. )，Ruby的第三方插件是用gem方式来管理，非常容易发布和共享，一个简单的命令就可以安装上第三方的扩展库。特点：能远程安装包，包之间依赖关系的管理，简单可靠的卸载，查询机制，能查询本地和远程服务器的包信息，能保持一个包的不同版本，基于Web的查看接口，能查看你安装的gem的信息。更换gem源为Ruby中国的源。默认的官方源因为网络问题速度慢. 12345678#删除官方源gem sources --remove https://rubygems.org/#添加Ruby中国源，添加源的时间比较久#没有进度提示，所以需要耐心等待#大概在10几分钟左右gem sources --add https://gems.ruby-china.org/#查询gem源记录gem sources –l 12345wget http://production.cf.rubygems.org/rubygems/rubygems-1.4.2.tgztar zxvf rubygems-1.4.2.tgzcd rubygems-1.4.2ruby setup.rbgem -v 安装Ruby on Rails安装Rails 4.2版本。 1gem install rails -v=4.2 提示错误： 12345ERROR: Error installing rails: ERROR: Failed to build gem native extension. /usr/bin/ruby extconf.rbmkmf.rb can&apos;t find header files for ruby at /usr/share/include/ruby.h 输入如下命令即可解决： 1yum -y install gcc mysql-devel ruby-devel rubygems 安装完毕后输入如下命令查看版本。 1rails -v 安装Redmine下载安装包： 1wget http://www.redmine.org/releases/redmine-3.3.1.tar.gz 解压缩： 1tar zxvf redmine-3.3.1.tar.gz 数据库配置(Link Redmine to the Database)12345678910#登录数据库mysql -u root -p#重置密码alter user root@localhost identified by '$zwkj123456ZWKJ';#创建数据库create database redmine character set utf8;#创建用户create user 'redmine'@'localhost' identified by '$zwkj123456ZWKJ';#添加权限grant all privileges on redmine.* to 'redmine'@'localhost'; Configure database.yml123cd /var/www/redmine/configcp database.yml.example database.ymlnano database.yml Rails配置(Rails Settings)安装bundler： 1gem install bundler --verbose 安装依赖： 12345#切换到redmine目录#redmine目录下有安装时需要的配置文件#所以需要切换到此目录下运行bundle命令cd /var/www/redminebundle install 输出如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384[root@localhost redmine]# bundle installDon't run Bundler as root. Bundler can ask for sudo if it is needed, and installing your bundle as root will break thisapplication for all non-root users on this machine./usr/share/gems/gems/psych-2.0.0/lib/psych.rb:98: warning: already initialized constant Psych::VERSION/usr/share/ruby/psych.rb:98: warning: previous definition of VERSION was here/usr/share/gems/gems/psych-2.0.0/lib/psych.rb:101: warning: already initialized constant Psych::LIBYAML_VERSION/usr/share/ruby/psych.rb:101: warning: previous definition of LIBYAML_VERSION was hereFetching gem metadata from https://rubygems.org/..........Fetching version metadata from https://rubygems.org/..Fetching dependency metadata from https://rubygems.org/.Resolving dependencies...Using rake 11.3.0Using i18n 0.7.0Installing json 1.8.3 with native extensionsUsing minitest 5.9.1Using thread_safe 0.3.5Using builder 3.2.2Using erubis 2.7.0Using mini_portile2 2.1.0Using rack 1.6.5Using mime-types-data 3.2016.0521Using arel 6.0.3Using public_suffix 2.0.4Using bundler 1.13.6Installing ffi 1.9.14 with native extensionsUsing coderay 1.1.1Using concurrent-ruby 1.0.2Using docile 1.1.5Using htmlentities 4.3.1Using thor 0.19.1Using metaclass 0.0.4Using mimemagic 0.3.2Using multi_json 1.12.1Installing mysql2 0.3.21 with native extensionsUsing net-ldap 0.12.1Using ruby-openid 2.3.0Using rbpdf-font 1.19.0Installing rdoc 5.0.0Installing redcarpet 3.3.4 with native extensionsUsing request_store 1.0.5Installing rmagick 2.16.0 with native extensionsUsing rubyzip 1.2.0Using websocket 1.2.3Using simplecov-html 0.9.0Using yard 0.9.5Using tzinfo 1.2.2Using nokogiri 1.6.8.1Using rack-test 0.6.3Using mime-types 3.1Installing addressable 2.5.0Installing childprocess 0.5.9Using sprockets 3.7.0Installing mocha 1.2.1Installing rack-openid 1.4.2Installing rbpdf 1.19.0Installing simplecov 0.9.2Installing activesupport 4.2.7.1Using loofah 2.0.3Installing xpath 2.0.0Using mail 2.6.4Installing css_parser 1.4.6Installing selenium-webdriver 3.0.1Using rails-deprecated_sanitizer 1.0.3Using globalid 0.3.7Installing activemodel 4.2.7.1Using rails-html-sanitizer 1.0.3Installing capybara 2.10.1Installing roadie 3.2.0Using rails-dom-testing 1.0.7Installing activejob 4.2.7.1Installing activerecord 4.2.7.1Installing protected_attributes 1.1.3Installing actionview 4.2.7.1Installing actionpack 4.2.7.1Installing actionmailer 4.2.7.1Installing actionpack-action_caching 1.1.1Installing actionpack-xml_parser 1.0.2Installing railties 4.2.7.1Using sprockets-rails 3.2.0Installing jquery-rails 3.1.4Installing roadie-rails 1.1.1Installing rails 4.2.7.1Bundle complete! 31 Gemfile dependencies, 71 gems now installed.Use `bundle show [gemname]` to see where a bundled gem is installed. 12#安装依赖软件gem install bundler 初始化redmine数据库： 1234rake db:migrate RAILS_ENV=productionrake redmine:plugins:migrate RAILS_ENV=productionrake tmp:cache:clearrake tmp:sessions:clear 启动Redmine启动Redmine： 12345678#查找rails安装目录whereis rails#切换到redmine目录下(此步骤不可缺)cd /var/www/redmine#启动redmineruby /usr/local/bin/rails server webrick -e production –dbundle exec rake assets:precompile RAILS_ENV=production 常见问题访问地址http://localhost:3000时提示错误： 1Missing `secret_token` and `secret_key_base` for 'production' environment, set these values in `config/secrets.yml` Rails的安全机制需要一个秘钥。在Rails 4.x版本的时候, 秘钥的设置在RAILS_ROOT/config/secrets.yml。在非生产环境下, 秘钥都是’明文’, ‘硬编码’, 写在secrets.yml里面的。这种方式会由于源代码的泄露, 造成安全问题, 所以这种方式存在安全隐患。所以Rails要求在生产环境下, 通过操作系统的环境变量来设置秘钥, 这样相对比较稳妥。 这里可以采取2种方法: 1. 自己动手, 利用linux系统的机制来设置环境变量 SECRET_KEY_BASE = XXX 2. 使用GEM dotenv-deployment帮你设置, 具体机制和方法1本质没区别。 PS: Rails产生秘钥的指令: rake secret RAILS_ENV=production, 会产生一个秘钥。这里我选择方法2, 利用/etc/profile.d/ 下面添加脚本的方式来设置秘钥: 12345#产生一个秘钥rake secret RAILS_ENV=productionexport SECRET_KEY_BASE=生成的Key#查看设置的Keyecho $SECRET_KEY_BASE 然后刷新你的shell,echo $SECRET_KEY_BASE, 输出成功，重新启动计算机。访问http://localhost:3000。如果无法访问，检查防火墙是否允许3000端口，或者关闭防火墙即可。如果需要在外网访问，需要绑定地址： 12#参数b表示绑定(bind)ruby /usr/local/bin/rails server webrick -e production –d -b 192.168.24.221 参考资料： Redmine on CentOS installation HOWTO rails production secret_key的设置 centos6.7安装redmine系统]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis使用]]></title>
      <url>%2F2016%2F11%2F15%2Fredis-using%2F</url>
      <content type="text"><![CDATA[过期时间生存时间可以通过使用 DEL 命令来删除整个 key 来移除，或者被 SET 和 GETSET 命令覆写(overwrite)。 123456789101112131415161718192021/** * 设置数据（永久有效） * * @param key //key * @param value //值 * @param db //数据库序号 */public static void set(String key, String value, Integer db) &#123; Jedis jredis = null; try &#123; JedisPool poolItem = pools.get(db); jredis = poolItem.getResource(); jredis.set(key, value); &#125; catch (Exception e) &#123; log.error("set value encount an error", e); &#125; finally &#123; if (jredis != null) &#123; jredis.close(); &#125; &#125;&#125; 取出包含指定规则的Key从Redis中取出包含指定规则的Key的集合。 123456789101112131415161718192021222324/** * 取出包含指定key的集合 */public void pushCacheStatusInfo() &#123; JedisPool jedisPool = RedisHelper.getRedisPool(0); Jedis jredis = null; try &#123; jredis = jedisPool.getResource(); //找到包含指定Key的集合 Set keys = jredis.keys("*websocketsession"); Iterator iteratorKeys = keys.iterator(); while (iteratorKeys.hasNext()) &#123; String redisKey = iteratorKeys.next().toString(); String session = jredis.get(redisKey); //do something &#125; &#125; catch (Exception ex) &#123; log.error("推送信息出错", ex); &#125; finally &#123; if (jredis != null) &#123; jredis.close(); &#125; &#125;&#125; 常用指令登录Redis： 1./redis-cli -h 192.168.24.252 -p 6379 查看客户端数量： 1192.168.24.252:6379&gt; info clients]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[电影《恋恋风尘》(Dust in the Wind)]]></title>
      <url>%2F2016%2F11%2F13%2Fdust-in-the-wind%2F</url>
      <content type="text"><![CDATA[评价一部作品，它的真实性在心里占有相当的重量。而上映于1987年《恋恋风尘》(Dust in the Wind)所讲述的故事给自己的感觉是真实的，对观众来说它是仅仅一件小事。但对当时亲身经历的主人公来说，这些都是刻骨铭心的回忆，相信许多人也会有相似的感受，那就是：初恋告吹。 第一次看《恋恋风尘》(Dust in the Wind)，还是在大二、大三的学生时期。当时只觉得非常新奇，电影原来可以拍的这样安静和特别。往后参加工作，反复观看，每一次观看感受都是如此相似。久远的记忆仿佛就在昨天，具有贫苦农村生活经验和当兵经历的人士观看效果更佳。整部电影没有夸张的剧情和场景，都是一个个简单的回忆按时间顺序依次展现。 电影里有2个场景给我的印象最深： 一次是在台北时，阿远得了气管炎，阿云专程过来看他，当阿云过来时，恒春仔已经睡下了，说明已是很晚了，看到躺在床上无力的阿远，阿云遂打了热水，用热水浸湿的帕子搭在阿远的额头上，没有对白。第二天很早时阿云去上班。注意电影并没有强调阿云整晚是如何休息的，但是从屋内摆设观众应该能够想到她整夜打盹的场景。当清晨阿远送她出门时，电影的镜头慢慢拉到阿云离去的背影。这个离去的场景，我想阿远一生都不会忘记。 一次是在部队时，士兵们聚在一起打台球，一群人突然在讨论7号(一个女孩子)，说是和男朋友闹翻了，才出来做(应该是皮肉生意)。谈起阿远时，说他都不出去玩一玩，跟大家显得格格不入，遂嘲讽他“坚心为君，独守我青春”，说着一起大笑起来。镜头切到阿远呕吐的镜头，我想，他是由于太过于思念阿云的缘故。不是自己，怎么能理解一个人在心中的位置，旁人是没法理解的。 当忙碌了一天的工作后，或者闲暇时刻，回到家打开电视看看这么一部简单轻松的电影，也不失为一种放松方式。 PS:优酷下面的群众的评论能发现很多细节。 恋恋风尘优酷播放地址 恋恋风豆瓣介绍页面 《恋恋风尘》影评]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Intellij-Idea远程调试]]></title>
      <url>%2F2016%2F11%2F12%2Fintellij-idea-remote-debbuging%2F</url>
      <content type="text"><![CDATA[因为我们用的是Tomcat，所以在IDEA中点击右上角那个“Edit Configurations”按钮，然后在弹出的界面中点击左上角的加号，选择tomcat server-&gt;remote。 服务器配置要让远程服务器运行的代码支持远程调试，则启动的时候必须加上特定的JVM参数，这些参数是： 1-Xdebug -Xrunjdwp:transport=dt_socket,suspend=n,server=y,address=$&#123;debug_port&#125; 其中的${debug_port}是用户自定义的，为debug端口，本例以53996端口为例。在Windows下到tomcat目录下的catalina.bat文件中，添加如下内容，设置catalina环境变量： 1set CATALINA_OPTS="-agentlib:jdwp=transport=dt_socket,address=53996,suspend=n,server=y" 如果是Linux，在catalina.sh文件中中，设置catalina环境变量： 1export CATALINA_OPTS="-agentlib:jdwp=transport=dt_socket,address=53996,suspend=n,server=y" Sun虚拟机实现需要指定命令行选项，以加载JDWP（Java Debug Wire Protocol Transport）代理来debug。JDK 5.0以前需要指定-Xdebug和-Xrunjdwp这两个参数，以后则可以使用参数-agentlib:jdwp替代之，它们指定了JVM使用的连接器。从上面的例子代码可以看到几个jdwp支持的参数选项，包括transport、server、suspend、address等等，这些都很常见，还包括timeout、launch（中断并开始调试的时候，执行什么程序）、onuncaught（如果出现无法捕获的异常是否需要中断并调试）等等。运行如下命令启动Tomcat： 12345#Windows下启动Tomcatcatalina.bat start#Linux下启动Tomcatcatalina.sh start 使用如下命令查看是否配置成功： 12345#Windows下用此命令查看netstat -ano#Linux下用此命令查看lsof -i:53996 查看是否有53996的端口处于监听状态。如下图所示： Intellij Idea配置Intellij Idea远程调试配置如下： 参考资料： JVM问题定位工具]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Websocket数据查看]]></title>
      <url>%2F2016%2F11%2F12%2Fwebsocket-check%2F</url>
      <content type="text"><![CDATA[FireFox中查看WebSocket使用FireFox查看WebSocket内容需要安装一个WebSocket-Monitor插件。安装完毕后在FireFox的Web控制台(Ctrl + Shift + K)中。 Google Chrome中查看WebSocketGoogle Chrome自带查看模块，如下图所示。F12进入开发者页面，选择NetWork选项卡，选择WS(WebSocket)选项卡。 Fiddler中查看WebSocket在请求时选择任意WebSocket Session，即可出现WebSocket流量数据画面，不过是乱码，未找到合适的解决乱码的方案。 Fiddler中的WebSocket显示乱码是因为数据已经被压缩(Compressed),要查看乱码的内容，在Response中添加头信息。按下F2按钮后，即可在Response中手动添加头信息(Add Headers)。 1Content-Encoding: deflate 添加头信息之后，点击TextView会出现黄色的Decode提示框。 点击之后即可看到原始的Json数据。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[grep使用]]></title>
      <url>%2F2016%2F11%2F12%2Fgrep%2F</url>
      <content type="text"><![CDATA[grep (global search regular expression(RE) and print out the line,全面搜索正则表达式并把行打印出来)是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 Unix的grep家族包括grep、egrep和fgrep。egrep和fgrep的命令只跟grep有很小不同。egrep是grep的扩展，支持更多的re元字符， fgrep就是fixed grep或fast grep，它们把所有的字母都看作单词，也就是说，正则表达式中的元字符表示回其自身的字面意义，不再特殊。linux使用GNU版本的grep。它功能更强，可以通过-G、-E、-F命令行选项来使用egrep和fgrep的功能。 多个匹配模式1tail -f catalina.out | grep -e "苏E22222" -e "服务器" 使用此命令可以过滤catalina.out文件中包含苏E22222且包含服务器的内容。使用grep匹配“与”或者“或”模式grep命令加-e参数，这一扩展允许使用扩展模式匹配。例如，要抽取城市代码为2 1 9或2 1 6，方法如下： 1grep –E '219|216' 还可以为匹配的内容增加高亮的颜色。 1tail -f catalina.out |grep --color=auto -E 'topic|0194592|0720724|512|0146636|S000099' 全局配置自动显示颜色： 123vim ~/.bashrcalias grep='grep --color'source ~/.bashrc 选出不包含512且不包含topic的行。 1tail -f catalina.out |grep -v -E '512|topic' Grep正则表达式^ 锚定行的开始 如：’^grep’匹配所有以grep开头的行。如下语句匹配Tomcat日志输出所有开始为2016的行。 1tail -f catalina.out |grep -E "^2016" 过滤所有结尾为”}}”的行。 1tail -f catalina.out |grep -E "&#125;&#125;$" . 匹配一个非换行符的字符 如：’gr.p’匹配gr后接一个任意字符，然后是p。 * 匹配零个或多个先前字符 如：’ grep’匹配所有一个或多个空格后紧跟grep的行。 .一起用代表任意字符。 [] 匹配一个指定范围内的字符，如’[Gg]rep’匹配Grep和grep。 [^] 匹配一个不在指定范围内的字符，如：’[^A-FH-Z]rep’匹配不包含A-F和H-Z的一个字母开头，紧跟rep的行。 (..) 标记匹配字符，如：’(love)’，love被标记为1。 > 锚定单词的结束，如’grep&gt;’匹配包含以grep结尾的单词的行。 x{m} 连续重复字符x，m次，如：’o{5}’匹配包含连续5个o的行。 x{m,} 连续重复字符x,至少m次，如：’o{5,}’匹配至少连续有5个o的行。 x{m,n} 连续重复字符x，至少m次，不多于n次，如：’o{5,10}’匹配连续5–10个o的行。 w 匹配一个文字和数字字符，也就是[A-Za-z0-9]，如：’Gw*p’匹配以G后跟零个或多个文字或数字字符，然后是p。 W w的反置形式，匹配一个非单词字符，如点号句号等。W*则可匹配多个。 b 单词锁定符，如: ‘bgrepb’只匹配grep，即只能是grep这个单词，两边均为空格。 匹配条件附近相关内容有时日志较大时，需要查看日志中央的内容，那么可以使用-C参数。参数后附加一个数，代表日志前后的行数。 1cat catalina.out | grep -C 500 '16 11 28 11 00 00' | grep '0720738' 以上命令查看满足日志中包含16 11 28 11 00 00且包含0720738的，前后500行的内容。如果是两个条件满足其中任意一个的话，使用如下命令： 1cat catalina.out | grep -C 500 -E '16 11 28 11 00 00|0720738']]></content>
    </entry>

    
    <entry>
      <title><![CDATA[地理位置纠偏]]></title>
      <url>%2F2016%2F11%2F08%2Flocation-rectifying%2F</url>
      <content type="text"><![CDATA[偏移的起因：天朝测绘局以国家安全为理由，用法律的形式对所有在天朝发行的地图类产品加了强制性规范，要求所有地图类产品都必须使用国家测绘局的一种加偏移的算法，对地图的真实坐标进行加偏移处理，之后才可能通过审批准许上市。因此，天朝的所有官方及商用地图的坐标都是偏移的，这种偏移属于非线性的，偏移量在300至500米不等，偏移方向也不定。这种加过偏移的地图坐标就是所谓“火星坐标”。GPS接收机本身接收卫星的信号，计算出本机所在位置的经纬度，在没有做特别处理的时候，这个经纬度是正确的。但是如果GPS支持加载地图的话，这个GPS中的地图就得受上述第一条法规的约束了，所以正式在天朝销售的行货GPS设备中的地图必须也得加偏移，处理成火星坐标。地理位置纠偏代码(处理成火星坐标)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** Created by jiangxiaoqiang on 2016/11/7.* 适用于Google,高德体系的地图*/public class GpsDataTranslate &#123; /** * 圆周率 */ private static double PI = Math.PI; /** * 地球的半径(单位:米) */ private static double EARTH_RADIUS = 6378245.0; /** * ee: 椭球的偏心率(eccentricity of ellipsoid) */ private static double ECCENTRICITY_OF_ELLIPSOID = 0.0066934216229659433; private static boolean IsOutOfChina(double latitude, double longitude) &#123; return longitude &lt; 72.004 || longitude &gt; 137.8347 || (latitude &lt; 0.8293 || latitude &gt; 55.8271); &#125; private static double TransformLat(double x, double y) &#123; double num = -100.0 + 2.0 * x + 3.0 * y + 0.2 * y * y + 0.1 * x * y + 0.2 * Math.sqrt(Math.abs(x)); num += (20.0 * Math.sin(6.0 * x * PI) + 20.0 * Math.sin(2.0 * x * PI)) * 2.0 / 3.0; num += (20.0 * Math.sin(y * PI) + 40.0 * Math.sin(y / 3.0 * PI)) * 2.0 / 3.0; return num + (160.0 * Math.sin(y / 12.0 * PI) + 320.0 * Math.sin(y * PI / 30.0)) * 2.0 / 3.0; &#125; private static double TransformLon(double x, double y) &#123; double num = 300.0 + x + 2.0 * y + 0.1 * x * x + 0.1 * x * y + 0.1 * Math.sqrt(Math.abs(x)); num += (20.0 * Math.sin(6.0 * x * PI) + 20.0 * Math.sin(2.0 * x * PI)) * 2.0 / 3.0; num += (20.0 * Math.sin(x * PI) + 40.0 * Math.sin(x / 3.0 * PI)) * 2.0 / 3.0; return num + (150.0 * Math.sin(x / 12.0 * PI) + 300.0 * Math.sin(x / 30.0 * PI)) * 2.0 / 3.0; &#125; /** * 地理位置纠偏 * * @param wgLat * @param wgLon */ public static double[] transform(double wgLat, double wgLon) &#123; double[] latlng = new double[2]; if (IsOutOfChina(wgLat, wgLon)) &#123; latlng[0] = wgLat; latlng[1] = wgLon; return latlng; &#125; double dLat = TransformLat(wgLon - 105.0, wgLat - 35.0); double dLon = TransformLon(wgLon - 105.0, wgLat - 35.0); double radLat = wgLat / 180.0 * PI; double magic = Math.sin(radLat); magic = 1 - ECCENTRICITY_OF_ELLIPSOID * magic * magic; double sqrtMagic = Math.sqrt(magic); dLat = (dLat * 180.0) / ((EARTH_RADIUS * (1 - ECCENTRICITY_OF_ELLIPSOID)) / (magic * sqrtMagic) * PI); dLon = (dLon * 180.0) / (EARTH_RADIUS / sqrtMagic * Math.cos(radLat) * PI); latlng[0] = (wgLat + dLat); latlng[1] = (wgLon + dLon); return latlng; &#125;&#125; 一对纠偏的数据组示例： 1234567原始数据：29.54163(纬度)106.508188(经度)纠偏后数据：29.538885930651567(纬度)106.5120118854562(经度)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis客户端连接]]></title>
      <url>%2F2016%2F11%2F04%2Fredis-client-connect%2F</url>
      <content type="text"><![CDATA[Redis特点 Redis 是一个 key-value 的缓存(cache)和存储(store)系统（现在我们只用它来做缓存，目前还未当作DB用，数据存放在 Cassandra 里） 支持丰富的数据结构，List 就专门用于存储列表型数据，默认按操作时间排序。Sorted Set 可以按分数排序元素，分数是一种广义概念，可以是时间或评分。其次，其丰富的数据结构为日后扩展提供了很大的方便。 提供的所有操作都是原子操作，为并发天然保驾护航。 超快的性能，见其官方性能测试《How fast is Redis?》。 拥有比较成熟的Java客户端 - Jedis，像新浪微博都是使用它作为客户端。（官方推荐的Clients） Redis运行一段时间后，出现错误，无法获得连接： 1redis.clients.jedis.exceptions.JedisException: Could not get a resource from the pool 使用命令查看客户端数量。 1D:\Program Files\Redis&gt;redis-cli.exe info clients 结果如下所示： 12345# Clientsconnected_clients:11client_longest_output_list:0client_biggest_input_buf:0blocked_clients:0 Redis客户端连接在修改了Redis的绑定IP后，用客户端登录需要显示的指定IP和端口： 1./redis-cli -h 192.168.24.252 -p 6379 h代表hostname，主机，p代表port，端口。 Redis是否正确关闭连接可以看出目前的客户端已经超出了最大的客户端数量(配置的是10个)。应该是没有释放连接导致的问题。明显一个请求一次连接是很不靠谱的。这个问题发生有两方面的原因： 未正确使用对象池的空闲队列行为（LIFO“后进先出”栈方式） “关闭集群链接时异常导致连接泄漏”问题 修改配置： 123456789101112131415161718192021private static Dictionary&lt;Integer, JedisPool&gt; pools = new Hashtable();static &#123; JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(200); config.setMaxIdle(50); /*设置最小空闲数,在并发量不高时可以降低最小空闲数*/ config.setMinIdle(8); config.setMaxWaitMillis(10000); config.setTestOnBorrow(true); config.setTestOnReturn(true); //Idle时进行连接扫描 config.setTestWhileIdle(true); //表示idle object evitor两次扫描之间要sleep的毫秒数 config.setTimeBetweenEvictionRunsMillis(30000); //表示idle object evitor每次扫描的最多的对象数 config.setNumTestsPerEvictionRun(10); //表示一个对象至少停留在idle状态的最短时间，然后才能被idle object evitor扫描并驱逐；这一项只有在timeBetweenEvictionRunsMillis大于0时才有意义 config.setMinEvictableIdleTimeMillis(60000); //循环创建16个redis数据库连接池,存放在字典里面 for (int i = 0; i &lt; 2; i++) &#123; JedisPool item = new JedisPool(config, "127.0.0.1", 6379, 0); pools.put(i, item); &#125;&#125; 使用命令client-cli.exe info clients查看客户端的连接数量时，一般为最小空闲连接数量与客户端数量之和。比如查看客户端连接数量为17，设置的最小空闲连接数量是8，有2个连接池，即为16，加一个当前客户端的连接，刚好17个连接。此处Could not get a resource from the pool错误的原因是在打开了连接之后未关闭连接，此处使用的Redis版本为3.2.100 for Windows。 1234567891011121314151617181920212223242526/*** 获取数据** @param key //key* @param db //数据库序号*/public static String get(String key, Integer db) &#123; JedisPool poolItem = pools.get(db); Jedis jredis = null; String result = null; try &#123; jredis = poolItem.getResource(); result = jredis.get(key); &#125; catch (Exception e) &#123; log.error("get value error", e); &#125; finally &#123; if (jredis != null) &#123; /* 关闭Redis连接，ReturnResource方法已经标记为Deprecated 新的关闭连接的方式为直接调用close方法 */ jredis.close(); &#125; &#125; return result;&#125; Redis配置是否正确绑定在Redis的配置文件中： 12345678910111213## ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the# internet, binding to all the interfaces is dangerous and will expose the# instance to everybody on the internet. So by default we uncomment the# following bind directive, that will force Redis to listen only into# the IPv4 lookback interface address (this means Redis will be able to# accept connections only from clients running into the same computer it# is running).## IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES# JUST COMMENT THE FOLLOWING LINE.# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bind 192.168.24.252 bind的地址如果和程序中配置的地址不一致，也会提示此错误。在Linux里，如果没有指定配置文件，则会使用默认的配置文件，所以在修改了配置文件之后，启动Redis服务的时候显示的指定使用修改后的配置文件： 1./redis-server ../redis.conf &amp; 参考资料： Jedis - When to use returnBrokenResource()]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Intellij Idea小技巧(tips)]]></title>
      <url>%2F2016%2F11%2F03%2Fintellij-idea-tips%2F</url>
      <content type="text"><![CDATA[用键盘选择选框有时我们可以使用Tab按键将焦点移动到单选框里，如何使用键盘选中和取消选中单选框呢？只需要按下Space按键即可，即是键盘的空格按键。 键盘调整Debug窗口的大小键盘调整Debug窗口的大小快捷键是Ctrl + Shift + Up/Down，注意一定要定位到Debug视图，也就是Debug视图但是当前窗口的活跃(Active)视图才会生效。 同理，在调整项目树宽度时，可以使用快捷键Ctrl + Alt + Left/Right。 Debug视图Tab页切换快捷键是Shift + Tab，如下图所示。 Debug视图日志滚动到末尾如果想在Debug视图中将日志始终定位到末尾，可以点击左侧的Scroll To End按钮，也可以使用快捷键Ctrl + End。 Debug视图清空日志清空日志没有快捷键，但是可以使用键盘上的鼠标右键来做到。 关闭Editor Tabs在Editor Tabs中切换比较耗费时间，可以尝试关闭Editor Tabs功能，关闭掉Editor Tabs功能还有一个好处是加载的时候会变快，因为不用加载Editor Tabs里打开的页面的内容了。关闭Editor Tabs功能按下Ctrl + Alt + S快捷键。搜索Editor Tabs关键字即可，在右侧Tab Appearence中选择None即可。关闭掉后直接可以通过Ctrl + E快捷键或者Ctrl + Shift + E快捷键访问文件。 跳转到特定文件夹在Intellij Idea中可以跳转到特定的文件夹，只需要按下两次Shift之后，在搜索的关键字前面加一个斜杠即可，如下图所示。 自动补全分号在语句的结尾补全分号，可以使用快捷键Ctrl + Shift + Enter，注意光标需要移动到补全分号的行。 打开无快捷键的Tab在Intellij Idea中有些Tab时没有快捷键的，比如Maven Projects。要打开可以使用快捷键Ctrl + Alt + A，输入Maven projects关键字即可。 粘贴版历史如果使用过 Mac 里面的 Alfred 的话，可能大家早就习惯了 Alfred 的粘贴板历史的能力，真是复制粘贴的神器啊。其实 Intellij IDEA 里面也提供了这样的能力，我们可以直接通过Command + Shift + V来进行访问历史粘贴板： Language Injection在 Java 的 String 中编辑有 JSON 非常麻烦，在 Intellij IDEA 中，我们可以直接使用 Intellij IDEA 的 Language Injection 的功能（Alt + Enter）将一个字符串标记为 JSON，就可以非常方便地编写 JSON 了，再也不用担心转义的问题了： Intellij Idea注释模板File – Settings – Editor – Code Style – File and Code Templates设置类生成模板。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kafka消费position]]></title>
      <url>%2F2016%2F11%2F02%2Fkafka-consume-position%2F</url>
      <content type="text"><![CDATA[kafka允许通过seek(TopicPartition，long)指定新的位置，或者seekToBeginning，seekToEnd定位到最早或最近的offset。注意seek重置offsets只对当前消费者起作用，它并不会触发consumer的rebalance，或者影响其他消费者的fetchOffsets。在大多数情况下，消费者消费记录只是简单地从一开始到结束，并且定时地提交它的位置(不管是自动的还是手动的)。不过新的API也允许消费者手动控制它的位置，消费者可以在一个partition钟随意地往前或者往后移动位置。这就意味着消费者可以重新消费旧的记录(多次读取相同的记录)，或者直接跳到最近的记录，忽略掉中间的记录。 1234567891011121314151617181920212223242526272829/* * 得到分区 * * @param topics * @return void * @throws @author Jiangxiaoqiang * @Title: initialTopicsPartitions */public void initialTopicsPartitions(String[] topics) &#123; if (topics != null &amp;&amp; topics.length &gt; 0) &#123; for (String topic : topics) &#123; if (!Converter.toBlank(topic).equals("")) &#123; topicPartitions.add(new TopicPartition(topic, PublicVariable.KAFKA_COMSUME_PARTION)); &#125;&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD &#125; //从最新的位置开始消费 consumer.seekToEnd(); consumer.assign(topicPartitions);======= &#125; consumer.assign(topicPartitions); /* 从最新的位置开始消费,Special methods for seeking to the earliest and latest offset the server maintains are also available ( seekToBeginning(TopicPartition...) and seekToEnd(TopicPartition...) respectively) */ consumer.seekToEnd();&gt;&gt;&gt;&gt;&gt;&gt;&gt; 2e811b88860dc244827b13d566fed966b8243aaa &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis在Windows下配置]]></title>
      <url>%2F2016%2F11%2F01%2Fredis%2F</url>
      <content type="text"><![CDATA[简介Redis是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API，其实当前最热门的NoSQL数据库之一，NoSQL还包括了Memcached和mongodb。 下载安装下载在这里，这里下载的版本是：Redis-x64-3.2.100.msi。下载完毕后安装即可。安装Redis的目录D:\Program Files\Redis。启动Redis Service服务: 1234#切换到Redis目录cd /d D:\Program Files\Redis#启动Redis服务redis-server.exe redis.windows-service.conf 双击打开 redis-cli.exe , 如果不报错,则连接上了本地服务器,然后测试，比如 set命令，get命令，如下图所示。 Java连接Redis引入Client jar包，在Maven中引入配置： 1234567&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.8.1&lt;/version&gt; &lt;type&gt;jar&lt;/type&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt; 编写Java测试代码： 12345678910111213141516package com;import redis.clients.jedis.Jedis;/* * Created by jiangxiaoqiang on 2016/11/1. */public class RedisServiceTest &#123; public static void main(String[] args) &#123; //连接本地的 Redis 服务 Jedis jedis = new Jedis("localhost"); System.out.println("Connection to server sucessfully"); //查看服务是否运行 System.out.println("Server is running: "+jedis.ping()); &#125;&#125; 连接123456789101112static &#123; JedisPoolConfig config = new JedisPoolConfig(); config.setMaxIdle(2); config.setMaxTotal(10); config.setTestOnBorrow(true); config.setMaxWaitMillis(2000); //循环创建16个redis数据库连接池,存放在字典里面 for (int i = 0; i &lt; 16; i++) &#123; JedisPool item = new JedisPool(config, "127.0.0.1", 6379,10*1000); pools.put(i, item); &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[she]]></title>
      <url>%2F2016%2F11%2F01%2Fshe%2F</url>
      <content type="text"><![CDATA[like: 鱼(Fish) 广味香肠 节目 《真正男子漢》(Takes a Real man) dislike: 生食(日本料理),可以少量吃北极贝和甜虾]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kafka常用操作]]></title>
      <url>%2F2016%2F10%2F29%2Fkafka-common-operation%2F</url>
      <content type="text"><![CDATA[启动kafka： 1./kafka-server-start.sh ../config/server.properties &amp; 创建主题： 1./kafka-topics.sh --zookeeper localhost:2181 --create --topic test1 --partitions 1 --replication-factor 1 --config max.message.bytes=64000 --config flush.messages=1 查看所有主题： 1./kafka-topics.sh --list --zookeeper 192.168.24.11:2181 删除主题： 1./kafka-topics.sh --zookeeper 192.168.24.244:2181 --delete --topic 0085000 消费主题： 12345# 从开始处消费主题./kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic 0085000# 从最新位置消费主题./kafka-console-consumer.sh --zookeeper localhost:2181 --topic 0085000 改变主题L000000的默认分区数： 1/kafka-topics.sh --zookeeper 192.168.24.238:2181,192.168.24.11:2181,192.168.24.71:2181 --alter --topic L000000 --partitions 2]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kafka彻底删除主题]]></title>
      <url>%2F2016%2F10%2F29%2Fkafka-delete-topic%2F</url>
      <content type="text"><![CDATA[这里的Kafka的版本是0.9.0.1，查看所有Topics和分区： 1./kafka-topics.sh --describe --zookeeper 192.168.244.11:2181 只查看所有Topic： 1./kafka-topics.sh --list --zookeeper 192.168.24.244:2181 删除主题： 1./kafka-topics.sh --zookeeper 192.168.24.244:2181 --delete --topic 0085000 删除主题并不是真正的删除，仅仅是标记为删除(marked for deletion)，如果想彻底删除主题，可以修改kafka的配置： 1delete.topic.enable=true 删除Kafka存储目录(server.properties文件log.dirs配置，默认为”/tmp/kafka-logs”)相关topic目录。配置了delete.topic.enable=true直接通过命令删除，如果命令删除不掉，直接通过zookeeper-client删除掉broker下的topic即可。登录ZooKeeper客户端： 1234567891011#切换到ZooKeeper目录cd /usr/hdp/2.4.3.0-227/zookeeper/bin#登录ZooKeeper客户端./zookeeper-client#找到topic所在的目录ls /brokers/topics#彻底删除topic(remove recursively)rmr /brokers/topics/0085000 如果不知道ZooKeeper客户端的目录，可以通过如下命令找到。 1find / -name zookeeper-client]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kafka常见错误]]></title>
      <url>%2F2016%2F10%2F28%2Fkafka-error%2F</url>
      <content type="text"><![CDATA[Invalid partition given with record在Kafka生产者里写入消息时，提示写入失败，详细的错误信息如下所示： 123456789[ERROR]-[2016年-10月-28日16:17:35.083]-[Thread-25]-[com.zw.socket.service.kafka.producer.ClientKafkaProducer]-&#123;生产者发送消息出错&#125;java.lang.IllegalArgumentException: Invalid partition given with record: 1 is not in the range [0...1]. at org.apache.kafka.clients.producer.KafkaProducer.partition(KafkaProducer.java:671) ~[kafka-clients-0.9.0.1.jar:?] at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:430) ~[kafka-clients-0.9.0.1.jar:?] at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:339) ~[kafka-clients-0.9.0.1.jar:?] at com.zw.socket.service.kafka.producer.ClientKafkaProducer.sendMessage(ClientKafkaProducer.java:102) [classes/:?] at com.zw.socket.service.handler.common.CommonCommandHandler.commonMessageWriteIntoKafka(CommonCommandHandler.java:95) [classes/:?] at com.zw.socket.service.handler.device.DeviceMessageHandler.sendRegisterResult(DeviceMessageHandler.java:175) [classes/:?] at com.zw.socket.service.handler.device.DeviceMessageHandler.isAllowRegisted(DeviceMessageHandler.java:161) [classes/:?] at com.zw.socket.service.handler.device.DeviceMessageHandler.saveDeviceInfo(DeviceMessageHandler.java:216) [classes/:?] 在Kafka的配置文件中，修改Kafka每个topic的默认分区数的配置： 12#每个topic的分区个数，更多的partition会产生更多的segment filenum.partitions=2 The group coordinator is not available1234562016-10-29 14:52:56.387 INFO [nioEventLoopGroup-3-1][org.apache.kafka.common.utils.AppInfoParser$AppInfo:82] - Kafka version : 0.9.0.12016-10-29 14:52:56.387 INFO [nioEventLoopGroup-3-1][org.apache.kafka.common.utils.AppInfoParser$AppInfo:83] - Kafka commitId : 23c69d62a0cabf062016-10-29 14:52:56.409 ERROR [nioEventLoopGroup-3-1][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$DefaultOffsetCommitCallback:489] - Offset commit failed.org.apache.kafka.common.errors.GroupCoordinatorNotAvailableException: The group coordinator is not available.2016-10-29 14:52:56.519 WARN [kafka-producer-network-thread | producer-1][org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater:582] - Error while fetching metadata with correlation id 0 : &#123;0085000=LEADER_NOT_AVAILABLE&#125;2016-10-29 14:52:56.612 WARN [pool-6-thread-1][org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater:582] - Error while fetching metadata with correlation id 1 : &#123;0085000=LEADER_NOT_AVAILABLE&#125; 产生问题具体原因不详，可能是修改了默认分区导致的，解决方法：停止Kafka Broker，登录ZooKeeper客户端，删除所有主题即可。 1234567891011#切换到ZooKeeper目录cd /usr/hdp/2.4.3.0-227/zookeeper/bin#登录ZooKeeper客户端./zookeeper-client#找到topic所在的目录ls /brokers/topics#彻底删除topicrmr /brokers/topics/0085000 无法往集群中写入数据检查部署服务器节点，也就是写入节点的/etc/hosts配置文件中是否有IP和主机名的映射。 1vim /etc/hosts 配置如下： 12345127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.24.136 节点1主机名192.168.24.137 节点2主机名192.168.24.244 localhost Consumer Marking the coordinator XXXXX deadMarking the coordinator dead happens when there is a Network communication error between the Consumer Client and the Coordinator (Also this can happen when the Coordinator dies and the group needs to rebalance). There are a variety of situations (offset commit request, fetch offset, etc) that can cause this issue. I will suggest that you research what’s causing this situations。解决此问题，重新启动消费者/生产者。 LEADER_NOT_AVAILABLE在Kafka消费数据时，提示如下错误： 114:37:19.717]-[Thread-24]-[org.apache.kafka.clients.NetworkClient]-&#123;Error while fetching metadata with correlation id 63 : &#123;0402080=LEADER_NOT_AVAILABLE, T16092920=LEADER_NOT_AVAILABLE, TH003086=LEADER_NOT_AVAILABLE, 65565665666=LEADER_NOT_AVAILABLE, 0146636=LEADER_NOT_AVAILABLE, 16687896589=LEADER_NOT_AVAILABLE, CQSZ=LEADER_NOT_AVAILABLE, 25698568=LEADER_NOT_AVAILABLE, 1037494=LEADER_NOT_AVAILABLE, 1551555=LEADER_NOT_AVAILABLE, 0085000=LEADER_NOT_AVAILABLE, L000010=LEADER_NOT_AVAILABLE, 145263078=LEADER_NOT_AVAILABLE&#125;&#125; 重新启动生产者即可。KafkaProducer/Sender都需要获取集群的配置信息Metadata。所谓Metadata，Topic/Partion与broker的映射关系：每一个Topic的每一个Partion，得知道其对应的broker列表是什么，其中leader是谁，follower是谁。Sender从集群获取信息，然后更新Metadata； KafkaProducer先读取Metadata，然后把消息放入队列。如果没有获取到相应的元素据(Metadata)，则会有如下错误：fetching topic metadata for topics from broker failed。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[HP KU 1158键盘]]></title>
      <url>%2F2016%2F10%2F27%2Fnew-keyboard%2F</url>
      <content type="text"><![CDATA[以前的旧键盘打字的声音有点大，中午打字都担心吵到旁边休息的同学，而且按键比较生硬，今天特意请飞哥换了一款键盘HP KU 1158，确实好用不少。一直在找一款适合长时间输入的键盘，而且要求声音要尽量小一些，价格平民化一些(Realforce一千多也还蛮贵的)。这款键盘符合要求。 Amaozn购买链接.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[WebSocket连接(SessionConnectEvent)、断开连接(SessionDisconnectEvent)事件]]></title>
      <url>%2F2016%2F10%2F27%2Fwebsocket-event%2F</url>
      <content type="text"><![CDATA[WebSocket事件：SessionConnectEvent(连接时), SessionConnectedEvent(连接后), SessionDisconnectEvent(断开连接)。 12345678public class WebSocketDisconnectHandler implements ApplicationListener&lt;SessionDisconnectEvent&gt; &#123; @Override public void onApplicationEvent(SessionDisconnectEvent sessionDisconnectEvent) &#123; StompHeaderAccessor stompHeaderAccessor = StompHeaderAccessor.wrap(sessionDisconnectEvent.getMessage()); //do something &#125;&#125; 添加了断开连接实现类后的逻辑后，还需要注入Bean，否则不会生效。在相关配置文件(spring-socket-servlet.xml)中增加如下配置即可。 1&lt;bean class="com.zw.socket.service.config.WebSocketDisconnectHandler"&gt;&lt;/bean&gt;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Stomp + WebSocket消息实时推送]]></title>
      <url>%2F2016%2F10%2F25%2Fstomp-spring-message-push%2F</url>
      <content type="text"><![CDATA[消息广播消息广播将消息发送给所有用户。 12345678/* * 推送给所有用户 */private void pushInfoImpl(String url, String content) &#123; if (simpMessagingTemplate != null) &#123; simpMessagingTemplate.convertAndSend(url, content); &#125;&#125; 推送给指定用户12345678/* * 推送给指定用户 */private void pushInfoImpl(String user, String url, String content) &#123; if (simpMessagingTemplate != null) &#123; simpMessagingTemplate.convertAndSendToUser(user, url, content); &#125;&#125; url直接为/location，在发给客户端的时，会自动添加user前缀和用户名，客户端订阅的url像这样：/user/admin/location，订阅的url中，不包含topic。content为需要发送的消息的内容。 客户端客户端订阅消息如下代码片段所示： 123stompClient.subscribe('/user/admin/location/', function (greeting) &#123; console.log('接收到订阅的信息：' + greeting.body);&#125;); 客户端订阅的URL中，admin是用户名。发送给指定用户时会默认添加user前缀。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[SQuirreL GUI客户端集成Phoenix、MySQL配置]]></title>
      <url>%2F2016%2F10%2F25%2Fsquirrel-phoenix-configuration%2F</url>
      <content type="text"><![CDATA[SQuirreL GUI客户端简介SQuirreL GUI(下载地址)客户端来连接Phoenix，就像MySQL使用Navicat for MySQL，Oracle使用PL/SQL Developer一样，在进行一些数据库操作的时候能够更加的直观和方便。同时它还可以连接MySQL。它可以运行在Windows、Linux、Mac OS操作系统上。 下载并安装SQuirreL GUI下载好的文件是一个压缩包(squirrel-sql-3.7.1-standard.zip)。不用解压缩，直接修改文件的后缀名为jar(squirrel-sql-3.7.1-standard.jar)，双击即可打开安装界面。如果无法双击打开，则可以通过命令行的方式打开： 1java -jar squirrel-sql-3.7-standard.jar 下载驱动到Phoenix镜像站点下载包，这里选择的是apache-phoenix-4.8.0-HBase-1.2-bin.tar.gzip，到解压的apache-phoenix-4.8.0-HBase-1.2-bin.tar.gzip包的主目录下，将如下几个jar包拷贝到SQuirreL安装目录的lib目录下，例如本机路径是D:\Program Files\squirrel-sql-3.7.1\lib。 1phoenix-4.8.0-HBase-1.2-client.jar 注意顺序，先拷贝jar包，在启动SQuirreL添加驱动，如果是拷贝jar包时已经启动了SQuirreL，那么SQuirreL需要重启一下，拷贝的jar包才生效，这个是需要注意的地方，可以参见SQuirreL Configure: could not initial class org.apache.phoenix.jdbc.PhoenixDriver。在连接不同的HBase数据库时，需要注意驱动的版本，不同版本的驱动是无法正确连接的。高版本的驱动无法连接低版本的HBase。 添加Driver添加Driver如下图所示。 Name填写用户自定义名称。Example URL填写ZooKepper地址。Class Name填写：org.apache.phoenix.jdbc.PhoenixDriver。 添加Alias连接Hbase如下图所示。 Name是Alias的名字，可以填写自定义名称。Driver选择上一步配置好的Driver名称，这里是Phoenix。url填写Phoenix连接串：jdbc:phoenix:192.168.24.226,192.168.24.195:2181:/hbase-unsecure。UserName和Password填写操作系统的登录用户和登录密码。 查询SQL查询如下图所示。 连接MySQL将MySQL的驱动mysql-connector-java-5.1.39.jar拷贝到lib目录下。 驱动链接填写：jdbc:mysql://192.168.24.234:3306/clbs?useUnicode=true&amp;characterEncoding=utf-8。用户名和密码填写登录Linux主机的用户名和密码。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Fedora使用技巧]]></title>
      <url>%2F2016%2F10%2F21%2FFedora-shortkey%2F</url>
      <content type="text"><![CDATA[如下是我在使用Fedora的過程中整理的快捷键列表，网络上有许多快捷键版本，下表是经过测试可用，并一直在使用的快捷键。希望使用Fedora的朋友可以參考，加快熟悉Fedora的速度。 快捷键 作用 Windows + A 显示软件列表 Windows + H 隐藏当前窗口 Alt + F10 最大化、还原当前窗口 Alt + F2 打开命令行界面 Windows/ Alt+ F2 在当前活动窗口和所有窗口概览视图之间切换 Ctrl + Alt + Up/Down 切换当前窗口 Alt + Tab 应用程序之间进行切换 Windows + Left/Right/Up/Down 将窗口移向左右、最大化、还原 Windows + PageUp/PageDown 切换工作区 Ctrl + L 清空终端(Terminal) Ctrl + Shift + T(Terminal) 以Tab頁的形式打開終端 Ctrl + Shift + N(New) 打開一個新的終端 Ctrl + Alt + Del 调出关机界面 打开终端在Ubuntu中打开终端可以使用快捷键Ctrl + Alt + T(Terminal)，在Fedora中打开All Settings-&gt;Keyboard-&gt;Custom Shortcuts，添加快捷键即可。在快捷键的command中输入： 1gnome-terminal 在平时使用中，总有一个目录是使用频率最高的，此时打开终端时可以默认切换到对应的目录，只需要将启动命令改为： 1gnome-terminal --working-directory=/home/dolphin/dolphin/project/blogs/xiaoqiang-blog-source 此时使用快捷键打开终端时默认为设置的目录，非常之方便。 打开文件夹在Fedora中打开All Settings-&gt;Keyboard-&gt;Custom Shortcuts，添加快捷键即可，这里设置的是Ctrl + Alt + H(Home)。在快捷键的command中输入命令： 1nautilus 即可打开文件夹。 打开常用应用快捷键在Fedora中打开All Settings-&gt;Keyboard-&gt;Custom Shortcuts，添加快捷键即可，这里设置的是。在快捷键的command中输入命令： 1234#打开微信，Ctrl + Alt + W(WeChat)/opt/electronic-wechat-linux-x64/electronic-wechat#打开Google Chrome,快捷键设置为：Ctrl + Alt + G(Google Chrome)/opt/google/chrome/chrome 即可快速打开微信,微信使用的是electronic-wechat，安装目录是/opt/electronic-wechat-linux-x64/。 Useful keyboard shortcuts]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring模块化配置]]></title>
      <url>%2F2016%2F10%2F21%2Fspring-module-config%2F</url>
      <content type="text"><![CDATA[Spring的一大缺点就是配置文件非常多，想象如果没有注解扫描Bean，所有的Bean都配置在XML文件中，将会是一个噩梦，项目中会充斥着大量的配置文件。这也是Spring-Boot项目所要避免的问题之一，在做项目开发时，为了使配置显得有条理化，易于理解，可以采用Spring Import配置文件，项目中需要一个Spring的主文件，在Web.xml中指定Spring的主文件位置，主文件再Import各类配置文件。指定主文件如下代码片段所示： 1234&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring-config/spring-main-config.xml&lt;/param-value&gt;&lt;/context-param&gt; 其中spring-main-config.xml即是项目的Spring配置主文件。classpath是指WEB-INF文件夹下的classes目录,classpath 和 classpath 区别是：classpath：只会到你的class路径中查找找文件;classpath：不仅包含class路径，还包括jar文件中(class路径)进行查找.在部署完毕的WEB项目中，一般包含WEB-INF和META-INF文件夹。META-INF相当于一个信息包，目录中的文件和目录获得Java 2平台的认可与解释，用来配置应用程序、扩展程序、类加载器和服务manifest.mf文件，在用jar打包时自动生成。其中主配置文件中使用Import Resource如下代码片段所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:context="http://www.springframework.org/schema/context" xmlns:util="http://www.springframework.org/schema/util" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd"&gt; &lt;description&gt;spring主配置文件&lt;/description&gt; &lt;!-- 属性和配置文件读入 ,多个用逗号隔开 数据库参数和系统参数 --&gt; &lt;util:properties id="applicationProperties" location="classpath:application.properties" /&gt; &lt;context:property-placeholder properties-ref="applicationProperties" ignore-resource-not-found="true" /&gt; &lt;!-- 扫描注解@Component , @Service , @Repository。--&gt; &lt;context:component-scan base-package="main.src.*"&gt; &lt;context:include-filter type="annotation" expression="org.aspectj.lang.annotation.Aspect" /&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.stereotype.Controller" /&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.web.bind.annotation.RestController" /&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.web.bind.annotation.ControllerAdvice" /&gt; &lt;/context:component-scan&gt; &lt;!--aop 注解风格支持 proxy-targer-class默认false,用jdk动态代理,true是cglib .expose-proxy当前代理是否为可暴露状态,值是"ture",则为可访问。 --&gt; &lt;aop:aspectj-autoproxy expose-proxy="true" proxy-target-class="true" /&gt; &lt;!--aop xml风格支持 --&gt; &lt;aop:config expose-proxy="true" proxy-target-class="true" /&gt; &lt;!-- 导入其它spring配置文件 --&gt; &lt;import resource="classpath:spring-config/spring-filters.xml" /&gt; &lt;import resource="classpath:spring-config/spring-datasource.xml" /&gt; &lt;import resource="classpath:spring-config/spring-mybatis.xml" /&gt; &lt;import resource="classpath:spring-config/spring-cache.xml" /&gt; &lt;import resource="classpath:spring-config/spring-i18n.xml" /&gt; &lt;import resource="classpath:spring-config/spring-json.xml" /&gt; &lt;import resource="classpath:spring-config/spring-security.xml" /&gt; &lt;import resource="classpath:spring-config/spring-exception.xml" /&gt; &lt;import resource="classpath:spring-config/spring-log.xml" /&gt; &lt;import resource="classpath:spring-config/spring-validator.xml" /&gt; &lt;import resource="classpath:spring-config/spring-quartz.xml" /&gt; &lt;import resource="classpath:spring-config/spring-socket-servlet.xml"/&gt;&lt;/beans&gt;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Stomp WebSocket路由]]></title>
      <url>%2F2016%2F10%2F20%2Fstomp-url-route%2F</url>
      <content type="text"><![CDATA[STOMP即Simple (or Streaming) Text Orientated Messaging Protocol，简单(流)文本定向消息协议，它提供了一个可互操作的连接格式，允许STOMP客户端与任意STOMP消息代理（Broker）进行交互。STOMP协议由于设计简单，易于开发客户端，因此在多种语言和多种平台上得到广泛地应用。WebSocket协议的应用层子协议STOMP（流文本定向消息协议）。在应用中直接使用WebSocket API显得有些低端，直到统一标准规范时也只有一小部分框架可以解析信息或通过注解方式使用它。这正是考虑在应用中运用子协议和产生基于WebSocket支持的Spring的STOMP的原因。当运用一个上层协议，WebSocket API的细节就显得不那么重要了，正如运用了HTTP后TCP的通信细节不再暴漏在应用中一样。STOMP是为了简单而创建的一种消息协议。它基于模仿HTTP协议的帧。帧由一个命令、可选的头和可选的体组成。 业务中需要实现不同的消息类别分发，在客户端进行不同的处理。此时想到Stomp的路由。 服务端启动代理中继1234567891011121314151617181920212223@Configuration@EnableWebSocketMessageBrokerpublic class WebSocketConfig extends AbstractWebSocketMessageBrokerConfigurer &#123; @Override public void configureMessageBroker(MessageBrokerRegistry config) &#123; /** * 启用了STOMP代理中继功能：并将其目的地前缀设置为 "/topic"； * spring就能知道 所有目的地前缀为"/topic" 的消息都会发送到STOMP代理中； */ config.enableSimpleBroker("/topic"); /** * 设置了应用的前缀为"app"：所有目的地以"/app"打头的消息（发送消息url not连接url） * 都会路由到带有@MessageMapping注解的方法中，而不会发布到代理队列或主题中； */ config.setApplicationDestinationPrefixes("/app"); &#125; @Override public void registerStompEndpoints(StompEndpointRegistry registry) &#123; registry.addEndpoint("/gs-guide-websocket").withSockJS(); &#125;&#125; 或者以XML文件的方式进行配置，两者是等价的： 1234567891011121314151617&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:websocket="http://www.springframework.org/schema/websocket" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/websocket http://www.springframework.org/schema/websocket/spring-websocket.xsd"&gt; &lt;websocket:message-broker application-destination-prefix="/app"&gt; &lt;websocket:stomp-endpoint path="/gs-guide-websocket"&gt; &lt;websocket:sockjs/&gt; &lt;/websocket:stomp-endpoint&gt; &lt;websocket:simple-broker prefix="/topic, /queue"/&gt; &lt;/websocket:message-broker&gt;&lt;/beans&gt; 服务端添加ControllerSpring官方的例子演示了Send-Response模型，如果需要请求之后，服务端多次推送消息，主动推送消息，可采用如下方式： 12345678910111213141516@Controllerpublic class GreetingController &#123; public SimpMessagingTemplate template; @Autowired public GreetingController(SimpMessagingTemplate template) &#123; this.template = template; &#125; @MessageMapping("/hello") @SendTo("/topic/greetings") public void greeting(HelloMessage message) throws Exception &#123; template.convertAndSend("/topic/greetings", "aaaaaaa"); &#125;&#125; SimpMessagingTemplate是Spring实现的一个发送模板类，直接自动注入获取相应实例即可。SimpMessagingTemplate实例可以实现服务端主动向客户端订阅的Url推送消息。第一个参数为推送地址，第二个参数为需要推送的消息内容。 浏览器端1234567891011function connect() &#123; var socket = new SockJS('/clbs/gs-guide-websocket'); stompClient = Stomp.over(socket); stompClient.connect(&#123;&#125;, function (frame) &#123; setConnected(true); console.log('Connected: ' + frame); stompClient.subscribe('/topic/greetings', function (greeting) &#123; console.log(greeting.body); &#125;); &#125;);&#125; clbs是项目名称，gs-guide-websocket是终结点名称。greeting.body是取出服务端响应的消息体(Message Body)。Stomp协议与HTTP协议有许多相似之处，命令 + 消息头 + 空行 + 消息体。 1stompClient.send("/app/gs-guide-websocket/location",&#123;&#125;,JSON.stringify(requestStr)); STOMP Over WebSocket WebSocket Support]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[expected single matching bean but found 2]]></title>
      <url>%2F2016%2F10%2F20%2Fspring-encount-error%2F</url>
      <content type="text"><![CDATA[在WebSocket往客户端推送消息的开发过程中，获取推送消息实例SimpMessagingTemplate时，错误如下： 1[ERROR]-[2016年-10月-19日17:53:25.026]-[RMI TCP Connection(2)-127.0.0.1]-[org.springframework.web.context.ContextLoader]-&#123;Context initialization failed&#125; org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name &apos;com.zw.socket.service.controller.InstanceMessageController#0&apos; defined in class path resource [spring-config/spring-socket-servlet.xml]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.NoUniqueBeanDefinitionException: No qualifying bean of type [org.springframework.messaging.simp.SimpMessagingTemplate] is defined: expected single matching bean but found 2:org.springframework.messaging.simp.SimpMessagingTemplate#0,brokerMessagingTemplate 发生此错误的原因是在配置文件spring-socket-servlet.xml里重复做了如下配置： 123456789&lt;websocket:message-broker application-destination-prefix="/app"&gt; &lt;websocket:stomp-endpoint path="/vehicle"&gt; &lt;websocket:handshake-interceptors&gt; &lt;bean class="org.springframework.web.socket.server.support.HttpSessionHandshakeInterceptor"/&gt; &lt;/websocket:handshake-interceptors&gt; &lt;websocket:sockjs session-cookie-needed="true" /&gt; &lt;/websocket:stomp-endpoint&gt; &lt;websocket:simple-broker prefix="/topic"/&gt;&lt;/websocket:message-broker&gt; 扫描配置文件时会自动生成一个brokerMessagingTemplate实例，与实例simpMessagingTemplate冲突。解决的方法就是去掉配置文件中的配置。 参考： Could not autowire. No beans of SimpMessagingTemplate type found]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring Bean是否注册判断]]></title>
      <url>%2F2016%2F10%2F18%2Fspring-bean-registered%2F</url>
      <content type="text"><![CDATA[在项目的开发过程中有许多Bean，怎么判断一个Bean是否已经纳入容器管理了呢？当然最直接的方式是直接使用，如果不能用，那么肯定就未注册成功，但是也有的情况不是那么明显的，即使没有注册成功也不会有明显的错误。如何有效的鉴别和判断是否已经注册？ 日志在项目启动时会输出日志，提示注册了哪些Bean，那么只需要使用Bean的名称，在日志里面搜索一遍，即可知晓Bean是否已经注册，如果有相关注册成功输出提示，那么代表注册OK，如果没有相关日志，此时就需要排查Bean的配置了。 方法获取Bean可以通过如下方法获取特定注解的Bean的集合： 1Map&lt;String,Object&gt; beans = applicationContext.getBeansWithAnnotation(Foo.class); 其中Foo代表@Autowired、@Controller等等注解。 Is there any way to query bean of spring container]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring线程中Bean注入问题]]></title>
      <url>%2F2016%2F10%2F18%2Fspring-get-bean%2F</url>
      <content type="text"><![CDATA[实现ApplicationContextAware在Spring中开启线程时，无法使用Bean的自动注入，此时需要手动获取Bean。方法如下： 12345678910111213141516public class SpringApplicationContextHolder implements ApplicationContextAware &#123; private static ApplicationContext context; @Override public void setApplicationContext(ApplicationContext context) throws BeansException &#123; SpringApplicationContextHolder.context = context; &#125; public static Object getSpringBean(String beanName) &#123; return context == null ? null : context.getBean(beanName); &#125; public static String[] getBeanDefinitionNames() &#123; return context.getBeanDefinitionNames(); &#125;&#125; 配置文件注册在Spring中注册工具类的bean： 1&lt;bean class="com.zw.socket.service.kafka.comsumer.SpringApplicationContextHolder"&gt;&lt;/bean&gt; 获取Bean获取Bean实例： 1ClientMessageTransfer clientMessageTransfer=(ClientMessageTransfer)SpringApplicationContextHolder.getSpringBean("clientMessageTransfer");]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[port aready in use]]></title>
      <url>%2F2016%2F10%2F17%2Fport-aready-in-use%2F</url>
      <content type="text"><![CDATA[在使用Intellij Idea调试时，以Application方式启动程序时提示： 123456789101112132016-10-17 11:28:38.537 ERROR 23156 --- [ main] o.s.b.d.LoggingFailureAnalysisReporter : ***************************APPLICATION FAILED TO START***************************Description:The Tomcat connector configured to listen on port 8080 failed to start. The port may already be in use or the connector may be misconfigured.Action:Verify the connector&apos;s configuration, identify and stop any process that&apos;s listening on port 8080, or configure this application to listen on another port. 提示8080端口已经被占用了，由于此时是以Application方式启动的，没有Tomcat的相关端口配置。Spring Boot uses embedded Tomcat by default, but it handles it differently without using tomcat-maven-plugin. To change the port use –server.port parameter for example。添加程序启动参数：–server.port=8181，更改嵌入的Tomcat的端口即可解决此问题。如图所示： 解决方案来自StackOverflow： Launching Spring application Address already in use]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Fedora 24搭建Git Server]]></title>
      <url>%2F2016%2F10%2F16%2Ffedora-git-server%2F</url>
      <content type="text"><![CDATA[想将代码拷贝到家里的电脑，在安静的时刻可以阅读消化。每天用U盘拷贝也是比较麻烦，本来Github挺好用的，但是闭源的托管需要费用。刚好办公电脑和家里的电脑安装了OpenVPN,所以就想在家里的电脑搭建一个Git Server，通过OpenVPN将家里的电脑和办公电脑相连(相当于局域网)。这样就可以随时提交代码了。同时也想以SSH连接家里的电脑，所以同时也安装了OpenSSH。也可以SSH远程Copy，但是无法增量Copy，Copy一次大概需要2-3个小时。遂放弃SSH Copy的方案。 启动SSH服务确定是否已经安装SSH服务： 1rpm -qa | grep openssh-server 如果没有安装服务，输入如下命令安装： 1dnf install openssh-server -y 修改配置文件: 1vi /etc/ssh/sshd_config 1234#Port 22 监听的端口号，默认是22，可以自定义。#Protocol 2 支持的协议，默认就好，不用修改#PermitRootLogin yes 是否允许root直接登录，最好设置为no#MMaxAuthTries 6 最大登录数，默认是6，建议设置为3，防止别人密码穷举。 修改完配置后，重启SSH服务： 1service ssh restart 查看SSH服务状态: 1service sshd status 允许此端口（22）访问： 1iptables -A INPUT -p tcp --dport 22 -j ACCEPT 初始化Git仓库先在Fedora机器上，选定一个目录作为Git仓库，这里选择的是 1/home/dolphin/dolphin/source/zwlbs/plateform3.0/zwlbs.git 在目录下输入命令： 1$ sudo git init --bare zwlbs.git 签出Git库在办公的电脑上输入如下命令签出Git仓库： 1git clone dolphin@10.0.0.6:/home/dolphin/dolphin/source/zwlbs/plateform3.0/zwlbs.git 签出时会提示确定指纹，选择yes，同时需要输入用户dolphin的密码，输入即可。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在Fedora 24中使用xx-net]]></title>
      <url>%2F2016%2F10%2F15%2FFedora-24-using-xxnet%2F</url>
      <content type="text"><![CDATA[虽然此处实际在Linux平台进行的配置，但是XX-Net是跨平台的，在Windows、Mac OS下一样可以使用，在其他的操作系统下配置类似。在Fedora 24中安装完Google Chrome之后，还需要同步在Windows平台上保存的Google Chrome的书签（里面有好多收藏的好网站）、Cookie(不用每次登陆网页输入用户名和密码，记忆用户名和密码非常具有迷惑性，目前注册的用户名密码真的太多了，根本记不住，现在都是借助KeePass来记忆)等等数据。平时的搜索还是需要用Google，Google相比于百度，搜索出的结果更加精确，内容对于用户更加有意义。当你迫切想要知道某个问题的思路时，如果搜索出来一些无关痛痒的内容，是非常浪费时间的，消磨你的意志，会让你有一种被掏空了的感觉，对就是那种感觉。极大的降低了效率，想想如此庞大的用户基数，如果搜索出的内容不精准、没有意义所造成的资源浪费（时间、精力、意志力等）是非常恐怖的。所以这也是为什么费尽心思要使用Google，当你顺利的找到自己想要的内容并快速完成任务时，会深刻的体会到前期在科学上网里花费的时间是值得的。要做到以上两点，就需要借助XX-Net。 xx-net简介XX-Net是一款让你可以让你提高工作效率的工具，它通过让你可以获取到更多的信息的方式达到。XX-Net is a free desktop application that delivers fast, reliable and secure access to the open Internet for users in censored regions. It uses google app engine (GAE:Google App Engine) as a proxy server through the firewall.截至目前，xx-net在Github上(全球最大的同性交友网站)已经有10000+的star和1000+的watch。 下载xx-net下载在Github.这里下载的是稳定版(Stable Version). 启动xx-net下载完毕后,解压文件夹，在终端中切换到解压的文件夹下，运行启动命令。如果是在Windows下，运行start.vbs。 1./start 启动之后，等待程序扫描IP，大概半个小时左右。 配置配置主要分为两步，第一步是安装代理自动切换插件，第二步是导入备份文件，第三步是导入证书。 安装代理切换插件打开Google Chrome浏览器，切换到插件管理页面,可以选择Settings-Extensions,也可以访问链接：chrome://extensions/，将文件/opt/xx-net/SwitchyOmega/SwitchyOmega.crx托放到浏览器中，即可安装代理自动切换插件SwitchyOmega。 导入备份文件到SwitchyOmega的设置页面，找到Import/Export选项卡即可。导入完成后，点击SwitchyOmega，切换成“XX-Net自动切换”，如果使用的是X-Tunnel或SS，请点击“X-Tunnel自动切换”。 导入证书在Google Chrome浏览器中，访问地址： 1chrome://settings/certificates 选择Authorities选项卡，选择导入(Import)证书。证书的存放路径为:/data/gae-proxy/下。如下图所示。 证书导入完成后在浏览器中访问地址： 1localhost:8085 出现如下图所示页面表示导入成功。 验证配置并使用访问Google搜索引擎，如果能够成功访问Google,代表配置成功。如果首次无法访问，可以待后台程序多运行一些时间，几十分钟到几小时不等。一段时间之后再次尝试。需要注意的是XX-Net代理上网不具有匿名性，或者说匿名性很弱。 参考资料： 使用Chrome浏览器]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux开机自动启动程序]]></title>
      <url>%2F2016%2F10%2F14%2Fscript-startup-with-linux%2F</url>
      <content type="text"><![CDATA[开机时自动运行一般有守护进程的服务在Fedora 24中都可以通过systemctl命令自动运行。 1systemctl enable ServiceName 对于没有服务的程序，如果想在开机时随系统启动可以通过脚本来实现。 1nohup openvpn /etc/openvpn/client.conf 添加nohup后台启动，避免父进程结束的时候一并结束子进程。在/etc/rc.d/rc.local脚本中加入如下命令： 1/etc/openvpn/startopenvpn.sh 0:Halt 1:Single-user mode 2:Multi-user mode 3:Multi-user mode with networking 4:Not used/user-definable 5:Start the system normally with appropriate display manager (with GUI) 6:Reboot 登录后自动运行程序用户登录时，bash首先自动执行系统管理员建立的全局登录script:/etc/profile。然后bash在用户起始目录下按顺序查找三个特殊文件中的一个：/.bash_profile、/.bash_login、/.profile，但只执行最先找到的 一个。因此，只需根据实际需要在上述文件中加入命令就可以实现用户登录时自动运行某些程序（类似于DOS下的 Autoexec.bat）。简单的方法,在/etc/inittab 结尾加上你要启动的程序。/etc/profile： 此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行。/etc/bashrc: 为每一个运行bash shell的用户执行此文件。~/.bashrc: 当登录时以及每次打开新的shell时,该文件被执行。设置登陆时启动OpenVPN,在/etc/profile文件中添加执行脚本： 1/etc/openvpn/startopenvpn.sh &amp; &gt;&gt; /tmp/openvpn.log 其中startopenvpn.sh脚本中： 12cd /etc/openvpnnohup openvpn /etc/openvpn/client.conf 需要注意的是，执行时需要切换到/etc/openvpn目录，默认的配置文件例如key等默认在当前目录下寻找。OpenVPN启动时需要root权限。需要成功启动OpenVPN客户端首次登陆时需要以root用户登陆。 Ubuntu开机之后会执行/etc/rc.local文件中的脚本，所以可以直接在/etc/rc.local中添加启动脚本。当然要添加到语句exit 0前面才行。 联网后自动运行程序网络连接建立后运行的脚本可以实现诸多实用功能，如动态域名绑定、连接VPN、上网认证等。实现这一目标的大体思路有两种：在基于NetworkManager的系统中，可配置其dispatcher脚本；Fedora对这一功能支持的不是很好，只能在网络连接建立后运行一个脚本，即/sbin/ifup-local。这个文件默认不存在，需要手动创建。下面的例子用vi编辑/创建这个文件，并添加执行权限。 12vi /sbin/ifup-localchmod 755 /sbin/ifup-local Runlevel]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java 8 使用Stream API]]></title>
      <url>%2F2016%2F10%2F14%2Fjava-8-using-stream-api%2F</url>
      <content type="text"><![CDATA[遍历集合中的列Stream是Java8中，操作集合的一个重要特性。这里要遍历集合中对象的某一个属性，并取出来用“，”拼接成字符串，传统的写法是写循环遍历每个对象，从中取出某一个属性，进行拼接操作。在Java 8里可以使用Stream API只需要一行代码，非常简洁。 1234List&lt;WorkDayDataInfo&gt; workDayDataInfo = workDayInfos.get(currentWorkDay).getDatas();String assignmentIdStream = assignments.stream() .map(p -&gt; p.getId()) .collect(Collectors.joining(",")); 对于基本数据类型的拼接： 12List&lt;Integer&gt; numbers = Arrays.asList( 4, 8, 15, 16, 23, 42 );return numbers.stream().map( n -&gt; n.toString() ).collect( Collectors.joining( "," ) ); 去List除重复数据去除List中对象的重复数据。 1List&lt;ClientVehicleInfo&gt; distinctVehicles = clientVehicleInfos.stream().distinct().collect(Collectors.toList()); 对于Stream中包含的元素进行去重操作（去重逻辑依赖元素的equals方法），新生成的Stream中没有重复的元素。（根据.equals行为排除所有重复的元素。） 去除String数组重复数据其中deviceNumbers为String类型的数组。 1deviceNumbers= new HashSet&lt;&gt;(Arrays.asList(deviceNumbers)).toArray(new String[0]); 过滤器(Filter)从List中过滤出指定条件的数据： 1List&lt;CreditDocument&gt; xzxkList = creditDocuments.stream().filter(a -&gt; a.getInfoType() == 1).collect(Collectors.toList());]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[linux中的chkconfig、service和systemctl]]></title>
      <url>%2F2016%2F10%2F13%2Flinux-chkconfig-service-systemctl%2F</url>
      <content type="text"><![CDATA[最近在Fedora里使用开机自动启动命令时，提示如下： 1234567Note: This output shows SysV services only and does not include native systemd services. SysV configuration data might be overridden by native systemd configuration. If you want to list systemd services use &apos;systemctl list-unit-files&apos;. To see services enabled on particular target use &apos;systemctl list-dependencies [target]&apos;. 原来是systemctl命令要逐渐取代原来的chkconfig和services命令。在目前很多linux的新发行版本里，系统对于daemon的启动管理方法不再采用SystemV形式，而是使用了sytemd的架构来管理daemon的启动。UpStart\SystemV\systemd三种形式。Linux 操作系统的启动首先从 BIOS 开始，接下来进入 boot loader，由 bootloader 载入内核，进行内核初始化。内核初始化的最后一步就是启动 pid 为 1 的 init 进程。这个进程是系统的第一个进程。它负责产生其他所有用户进程。大多数 Linux 发行版的 init 系统是和 System V 相兼容的，被称为 sysvinit。这是人们最熟悉的 init 系统。一些发行版如 Slackware 采用的是 BSD 风格 Init 系统，这种风格使用较少。其他的发行版如 Gentoo 是自己定制的。Ubuntu 和 RHEL 采用 upstart 替代了传统的 sysvinit。而 Fedora 从版本 15 开始使用了一个被称为 systemd 的新 init 系统。如果需要服务随计算机启动时启用，在Fedora 24中，以SSH服务为例： 1systemctl enable sshd.service 这样SSH守护进程就会在开机时自动启动了。输出的执行结果为： 1Created symlink from /etc/systemd/system/multi-user.target.wants/sshd.service to /usr/lib/systemd/system/sshd.service. 查看SSH守护进程当前的状态。 1systemctl start sshd.service 在 Linux 主要应用于服务器和 PC 机的时代，SysVinit 运行非常良好，概念简单清晰。它主要依赖于 Shell 脚本，这就决定了它的最大弱点：启动太慢。在很少重新启动的 Server 上，这个缺点并不重要。而当 Linux 被应用到移动终端设备的时候，启动慢就成了一个大问题。为了更快地启动，人们开始改进 sysvinit，先后出现了 upstart 和 systemd 这两个主要的新一代 init 系统。Upstart 已经开发了 8 年多，在不少系统中已经替换 sysvinit。Systemd 出现较晚，但发展更快，大有取代 upstart 的趋势。而SystemV对应的是service、UpStart对应的是chkconfig、systemd对应的是systemctl命令。 参考来源： 浅析 Linux 初始化 init 系统，第 1 部分: sysvinit]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Fedora 24添加桌面图标]]></title>
      <url>%2F2016%2F10%2F12%2Ffedora-desktop-icon%2F</url>
      <content type="text"><![CDATA[安装好了Google Chrome和Intellij Idea之后没有桌面图标，也没有快速启动图标，每次启动都要打开终端，甚是麻烦。故将图标放在桌面能够节省不少时间，同时也更加方便。首先安装gnome-tweak-tool: 1dnf install gnome-tweak-tool -y 运行gnome-tweak-tool命令，在弹出的窗口的Desktop选项卡中打开Icons on Desktop. 桌面图标切换到Desktop目录。 1cd /home/dolphin/Desktop dolphin是当前用户名。新建Google-Chrome.desktop文件。内容为： 123456789101112#!/usr/bin/env xdg-open[Desktop Entry]Encoding=UTF-8Name=Google ChromeGenericName=Web BrowserExec='/opt/google/chrome/google-chrome'Icon=/opt/google/chrome/product_logo_256.pngTerminal=falseType=ApplicationCategories=Network;Name[en_US]=Google Chrome.desktop 如下是Intellij Idea的桌面图标配置文件: 123456789101112#!/usr/bin/env xdg-open[Desktop Entry]Encoding=UTF-8Name=Intellij IdeaGenericName=IDEExec='/opt/idea/idea-IC-162.2032.8/bin/idea.sh'Icon=/opt/idea/idea-IC-162.2032.8/bin/idea.pngTerminal=falseType=ApplicationCategories=IDE;Name[en_US]=Intellij Idea 保存之后双击打开，一定要双击打开哟，否则启动图标不会出现，会出现一个确认界面。提示需要授权启动项，选择授权即可。配置好图标后的效果如图所示。 快速启动图标要让图标在快速启动栏里出现，直接将刚才新建Google-Chrome.desktop文件复制到/usr/share/application目录下即可。 1cp Google-Chrome.desktop /usr/share/application 配置好快速启动图标后的效果如下如所示。 /usr/share/application目录是Gnome中所有用户启动的快捷方式存放的目录。局部的快捷方式存放的目录在：~/.local/share/applications。desktop文件的结构如下。 12345678[Desktop Entry]Encoding=UTF-8 //字符编码Name=vim //显示的名字MimeType=text/plain; //类型Exec=vim %f //运行的程序 %f表示一个参数Type=Application //类型Terminal=true //是否使用终端NoDisplay=true //是否显示在gnome菜单里]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Intellij Idea热部署]]></title>
      <url>%2F2016%2F10%2F10%2Fintellij-hot-deply%2F</url>
      <content type="text"><![CDATA[下午遇到一个问题，在开发时，HTML修改后浏览器刷新始终不显示修改后的效果。经过朋友的指导，原来是要选择Exploded包进行部署。如图所示。 在Intellij的官方文档上如此描述：To have the application deployed as a directory, choose Web Application: Exploded.To have the application deployed in the packed form, choose Web Application: Archive.大意是如果想以目录形式部署，选择Exploded(adj. 爆炸了的；分解的；被破除的)模式,如果想以打包模式部署，则选择Archive模式。Exploded模式带来的好处是支持热部署，这样不用在开发过程中每次修改了内容后(HTML\JSP)都重新部署，重启一次大概要1-3分钟，有此可见大大提高了开发效率。缺点是多次部署后，Tomcat可能内存溢出，此时就必须重启Tomcat。 参考资料： What does Exploded Development mean? (In Java) Configuring Web Application Deployment]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Fedora 24 OpenVPN客户端配置]]></title>
      <url>%2F2016%2F10%2F09%2Ffedora-openvpn-client%2F</url>
      <content type="text"><![CDATA[安装输入如下命令安装。 1234#Fedora 24安装命令dnf install openvpn -y#CentOS 6.8安装命令(可输入lsb_release -a命令查看版本)yum intall openvpn -y 配置生成客户端文件到OpenVPN服务端easy-rsa目录下，输入如下命令生成客户端key： 1build-key client 这里介绍在Fedora中如何设置OpenVPN客户端。将生成的客户端文件拷贝到Fedora的/etc/openvpn配置目录中即可，生成的客户端文件有： ca.crt client.crt client.key client.ovpn 在Fedora中将client.ovpn改为client.conf即可。启动OpenVPN客户端(root权限): 1openvpn client.conf 开机启动输入如下命令开启开启自动启动： 123456789101112#检查OpenVPN是否在本运行级别下设置为开机启动chkconfig --list openvpn#如果没设置启动就设置下chkconfig --level 2345 openvpn onchkconfig openvpn on#重新启动service sshd restart#看是否启动了1194端口.确认下netstat -antp |grep openvpn#看看是否放行了1194口iptables -nL#setup----&gt;防火墙设置 如果没放行就设置放行. chkconfig provides a simple command-line tool for maintaining the /etc/rc[0-6].d directory hierarchy by relieving system administrators of the task of directly manipulating the numerous symbolic links in those directories. delete from positional where vtime in ( select vtime from positional group by vtime having count()&gt;1) and id not in (select top 1 id from positional group by vtime having count()&gt;1 ); 问题解决在Fedora 24中运行openvpn client.conf后提示如下错误： 123456789101112131415Sat Oct 8 23:32:19 2016 Socket Buffers: R=[87380-&gt;87380] S=[16384-&gt;16384]Sat Oct 8 23:32:19 2016 Attempting to establish TCP connection with [AF_INET]114.24.5.55:1194 [nonblock]Sat Oct 8 23:32:20 2016 TCP connection established with [AF_INET]114.24.5.55:1194Sat Oct 8 23:32:20 2016 TCPv4_CLIENT link local: [undef]Sat Oct 8 23:32:20 2016 TCPv4_CLIENT link remote: [AF_INET]114.24.5.55:1194Sat Oct 8 23:32:20 2016 TLS: Initial packet from [AF_INET]114.24.5.55:1194, sid=efc00936 581068f2Sat Oct 8 23:32:20 2016 VERIFY OK: depth=1, C=US, ST=CA, L=SanFrancisco, O=OpenVPN, OU=changeme, CN=OpenVPN_CA, name=changeme, emailAddress=mail@host.domainSat Oct 8 23:32:20 2016 VERIFY ERROR: depth=0, error=certificate signature failure: C=US, ST=CA, L=SanFrancisco, O=OpenVPN, OU=changeme, CN=server, name=changeme, emailAddress=mail@host.domainSat Oct 8 23:32:20 2016 OpenSSL: error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failedSat Oct 8 23:32:20 2016 TLS_ERROR: BIO read tls_read_plaintext errorSat Oct 8 23:32:20 2016 TLS Error: TLS object -&gt; incoming plaintext read errorSat Oct 8 23:32:20 2016 TLS Error: TLS handshake failedSat Oct 8 23:32:20 2016 Fatal TLS error (check_tls_errors_co), restartingSat Oct 8 23:32:20 2016 SIGUSR1[soft,tls-error] received, process restartingSat Oct 8 23:32:20 2016 Restart pause, 5 second(s) 发生此错误的原因是OpenSSL包中，当前使用的OpenSSL(版本：OpenSSL 1.0.0e 6 Sep 2011)默认的摘要算法为MD5。而MD5算法在目前是非常不安全的(Hash碰撞攻击等)，较大的彩虹表可以轻易的找出Hash对应值。所以在较新的操作系统(这里是Fedora 24)已经默认不使用MD5算法，所以会有此错误(Windows 7支持MD5)。解决此问题的思路不外乎2种，一种是使用sha256、sha512等摘要算法(MD)，另一种就是启用操作系统对MD5的支持,推荐前者。 修改加密方式(推荐)修改加密方式在OpenVPN目录的文件中(我的是在C:\Program Files (x86)\OpenVPN\easy-rsa\openssl-1.0.0.cnf)。将 1default_md = md5 # use public key default MD 改为 1default_md = sha256 # use public key default MD 启用操作系统MD5支持Temporally enable it. 12export NSS_HASH_ALG_SUPPORT=+MD5export OPENSSL_ENABLE_MD5_VERIFY=1 Enable MD5 support through NetworkManager 1$ sudo vim /usr/lib/systemd/system/NetworkManager.service Append this. 12[Service]Environment=&quot;OPENSSL_ENABLE_MD5_VERIFY=1 NSS_HASH_ALG_SUPPORT=+MD5&quot; And restart daemon 12$ sudo systemctl daemon-reload$ sudo systemctl restart NetworkManager.service 提示如下错误： 12345678910111213Sun Oct 09 11:33:22 2016 OpenVPN 2.2.2 Win32-MSVC++ [SSL] [LZO2] [PKCS11] built on Dec 15 2011Sun Oct 09 11:33:22 2016 NOTE: OpenVPN 2.1 requires &apos;--script-security 2&apos; or higher to call user-defined scripts or executablesSun Oct 09 11:33:22 2016 LZO compression initializedSun Oct 09 11:33:22 2016 Control Channel MTU parms [ L:1544 D:140 EF:40 EB:0 ET:0 EL:0 ]Sun Oct 09 11:33:22 2016 Socket Buffers: R=[8192-&gt;8192] S=[8192-&gt;8192]Sun Oct 09 11:33:22 2016 Data Channel MTU parms [ L:1544 D:1450 EF:44 EB:135 ET:0 EL:0 AF:3/1 ]Sun Oct 09 11:33:22 2016 Local Options hash (VER=V4): &apos;69109d17&apos;Sun Oct 09 11:33:22 2016 Expected Remote Options hash (VER=V4): &apos;c0103fa8&apos;Sun Oct 09 11:33:22 2016 Attempting to establish TCP connection with 113.204.5.58:1194Sun Oct 09 11:33:23 2016 TCP: connect to 113.204.5.58:1194 failed, will try again in 5 seconds: Connection refused (WSAECONNREFUSED)Sun Oct 09 11:33:29 2016 TCP: connect to 113.204.5.58:1194 failed, will try again in 5 seconds: Connection refused (WSAECONNREFUSED)Sun Oct 09 11:33:35 2016 TCP: connect to 113.204.5.58:1194 failed, will try again in 5 seconds: Connection refused (WSAECONNREFUSED)Sun Oct 09 11:33:41 2016 TCP: connect to 113.204.5.58:1194 failed, will try again in 5 seconds: Connection refused (WSAECONNREFUSED) 首先检查服务端OpenVPN是否已经启动；其次检查服务端的配置文件无误，这里是将位数由1024改为2048后在服务端的配置文件没有修改为2048pem。 123456# Diffie hellman parameters.# Generate your own with:# openssl dhparam -out dh1024.pem 1024# Substitute 2048 for 1024 if you are using# 2048 bit keys.dh dh2048.pem #将此处由dh1024.pem修改为dh2048.pem即可 出现如下错误： 1234567Tue Oct 11 12:44:00 2016 Socket Buffers: R=[124928-&gt;124928] S=[124928-&gt;124928]Tue Oct 11 12:44:00 2016 UDPv4 link local: [undef]Tue Oct 11 12:44:00 2016 UDPv4 link remote: [AF_INET]192.168.24.243:1194Tue Oct 11 12:45:00 2016 TLS Error: TLS key negotiation failed to occur within 60 seconds (check your network connectivity)Tue Oct 11 12:45:00 2016 TLS Error: TLS handshake failedTue Oct 11 12:45:00 2016 SIGUSR1[soft,tls-error] received, process restartingTue Oct 11 12:45:00 2016 Restart pause, 2 second(s) 检查防火墙是否过滤了1194端口的数据。 123iptables -A INPUT -p tcp --dport 1194 -j ACCEPT#保存防火墙规则/etc/init.d/iptables save 检查配置文件是否通过TCP协议而不是UDP。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Intellij IDEA推荐插件]]></title>
      <url>%2F2016%2F10%2F08%2Fintelli-idea-plugin%2F</url>
      <content type="text"><![CDATA[presentation assistant这款插件可以实时的在Intellij屏幕底部展示当前按下的快捷键，包括Windows的快捷键和Mac的快捷键，可以帮助您记忆快捷键，清楚当前的行为(Action)。效果如下图所示。 Grep Console允许你定义一系列将通过控制台输出或文件测试的正则表达式。匹配代码行的每个表达式将会影响整行的样式，或播放声音。例如，错误消息可以被设置在一个红色的背景中显示。例如错误(ERROR)输出为红色，报警(WARNING)输出为黄色，信息(INFO)输出为绿色。 JRebel for IntelliJ(Commercial)JRebel的热部署可以让你修改代码以后不用重新启动项目即可加载效果，即所谓的热部署，可以大大提高开发效率。在IntelliJ Idea-&gt;Setting-&gt;Plugin-&gt;Browse Repositories中，输入JRebel for IntelliJ关键字即可。 Lombok自动生成实体的Getter、Setter，可以大大减少实体的编码量。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[rm命令安全]]></title>
      <url>%2F2016%2F10%2F08%2Frm-command%2F</url>
      <content type="text"><![CDATA[rm命令是一个非常危险的命令，由于这个命令引发的事故不少，最近也是深深的体会到了。在此记录下如何将rm命令变为安全的命令。对于需要在Linux下开发的朋友来说，这一步(屏蔽rm危险操作)觉得是必须的，非常、非常、非常重要。怎么强调都不为过。一定要花时间做rm命令的安全工作。 建立回收站机制回收站机制-建立新命令在/usr/bin目录下建立文件erase： 12cd /usr/bintouch erase 拷贝如下Shell脚本到文件中： 1234567891011121314151617181920#! /bin/bashRecycleBin=~/.temp(($#==0)) &amp;&amp; &#123; echo "No paraments!";exit 1; &#125;if [ ! -d $RecycleBin ]; then mkdir $RecycleBinfifor i in $*do if test -e $i then cd $(dirname $i) mv -f $(basename $i) $RecycleBin/$(find $(pwd) -maxdepth 1 -name $(basename $i) | tr "/" "=") cd - else echo "$i:No such file or directory!" fidone 添加执行权限： 1chmod 777 erase 此脚本会在用户目录下新建一个隐藏的temp文件夹，将删除的文件移动到此文件夹下。命名为：=用户名=删除的文件名=。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Ambari Metrics重装]]></title>
      <url>%2F2016%2F10%2F07%2Fambari-metrics-reinstall%2F</url>
      <content type="text"><![CDATA[移除服务采用Ambari REST API移除相关服务。 123456#停止服务curl -i -H "X-Requested-By: ambari" -u admin:admin -X PUT -d '&#123;"RequestInfo":&#123;"context":"Stop Service"&#125;,"Body":&#123;"ServiceInfo":&#123;"state":"INSTALLED"&#125;&#125;&#125;' http://192.168.24.226:8080/api/v1/clusters/zwlbs/services/AMBARI_METRICS#查看服务状态curl -u admin:admin -H "X-Requested-by:ambari" -i -k -X GET http://192.168.24.226:8080/api/v1/clusters/zwlbs/services/AMBARI_METRICS/#移除服务curl -u admin:admin -H "X-Requested-By: ambari" -X DELETE http://192.168.24.226:8080/api/v1/clusters/zwlbs/services/AMBARI_METRICS cURL是一个命令行工具，通过不同的协议传输数据，1997年首次发布。cURL is a computer software project providing a library and command-line tool for transferring data using various protocols. The cURL project produces two products, libcurl and cURL. It was first released in 1997. The name originally stood for “see URL”.curl支持的协议有FTP, FTPS, HTTP, HTTPS, SCP, SFTP, TFTP, TELNET, DICT, LDAP, LDAPS, FILE, POP3, IMAP, SMTP and RTSP at the time of this writing. Wget支持HTTP, HTTPS and FTP三种协议. 移除包移除各个节点啊上安装的包。 12345678#主节点yum remove ambari-metrics-hadoop-sink-2.2.2.0-460.x86_64 -yyum remove ambari-metrics-monitor-2.2.2.0-460.x86_64 -yyum remove ambari-metrics-grafana-2.2.2.0-460.x86_64 -yyum remove ambari-metrics-collector-2.2.2.0-460.x86_64 -y#从节点yum remove ambari-metrics-monitor-2.2.2.0-460.x86_64 -yyum remove ambari-metrics-hadoop-sink-2.2.2.0-460.x86_64 -y 删除文件删除与Ambari Metrics相关的文件。 12345678910111213141516rm -rf#Ambari Metrics/usr/lib/ambari-metrics-grafana/usr/lib/ambari-metrics-hadoop-sink/usr/lib/ambari-metrics-kafka-sink/var/lib/ambari-metrics-collector/var/lib/ambari-metrics-grafana/var/run/ambari-metrics-collector/var/run/ambari-metrics-monitor/var/run/ambari-metrics-grafana/var/log/ambari-metrics-collector/var/log/ambari-metrics-monitor/var/log/ambari-metrics-grafana/etc/ambari-metrics-collector/etc/ambari-metrics-monitor/etc/ambari-metrics-grafana 可将以上代码写入Shell脚本，执行即可。 添加服务以上步骤完成后，重新到Ambari UI界面添加Ambari Metrics服务即可。如果哪个服务实在无法修复，最无奈的方法，重装此服务。 参考文章： Services and State with Ambari REST API]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[卸载HDP和Ambari]]></title>
      <url>%2F2016%2F10%2F07%2Fambari-uninstall%2F</url>
      <content type="text"><![CDATA[移除Ambari服务依次运行如下命令： 1rpm -qa | grep ambari 123456789101112ambari-server stopambari-server resetambari-agent stoprpm -qa | grep ambari#移除Ambari Server安装包yum erase ambari-server -yrm -rf /var/lib/ambari-serverrm -rf /var/run/ambari-serverrm -rf /usr/lib/amrbari-serverrm -rf /etc/ambari-serverrm -rf /var/log/ambari-serverrm -rf /usr/lib/python2.6/site-packages/ambari* Ambari Agent Cleanup Script1python /usr/lib/python2.6/site-packages/ambari_agent/HostCleanup.py -s -k users Remove Packages12yum erase -y `yum list | grep @HDP-2 | awk '&#123; print $1 &#125;'`yum erase -y `yum list | grep 2_3_ | awk '&#123; print $1&#125;'` 移除文件夹(Clean Folders)移除文件夹(Clean Folders)脚本： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889rm -rf# Log dirs/var/log/ambari-metrics-monitor/var/log/hadoop/var/log/hbase/var/log/hadoop-yarn/var/log/hadoop-mapreduce/var/log/hive/var/log/oozie/var/log/zookeeper/var/log/flume/var/log/hive-hcatalog/var/log/falcon/var/log/knox/var/lib/hive/var/lib/oozie# DataNode HDFS dirs/grid*/hadoop# Hadoop dirs/usr/hdp/usr/bin/hadoop/tmp/hadoop/var/hadoop/hadoop/*/local/opt/hadoop# Config dirs/etc/hadoop/etc/hbase/etc/oozie/etc/phoenix/etc/hive/etc/zookeeper/etc/flume/etc/hive-hcatalog/etc/tez/etc/falcon/etc/knox/etc/hive-webhcat/etc/mahout/etc/pig/etc/hadoop-httpfs# PIDs/var/run/hadoop/var/run/hbase/var/run/hadoop-yarn/var/run/hadoop-mapreduce/var/run/hive/var/run/oozie/var/run/zookeeper/var/run/flume/var/run/hive-hcatalog/var/run/falcon/var/run/webhcat/var/run/knox# ZK db files/local/home/zookeeper/*# libs/usr/lib/flume/usr/lib/storm/var/lib/hadoop-hdfs/var/lib/hadoop-yarn/var/lib/hadoop-mapreduce/var/lib/flume/var/lib/knox#Ambari Metrics/usr/lib/ambari-metrics-grafana/usr/lib/ambari-metrics-hadoop-sink/usr/lib/ambari-metrics-kafka-sink/var/lib/ambari-metrics-collector/var/lib/ambari-metrics-grafana/var/run/ambari-metrics-collector/var/run/ambari-metrics-monitor/var/run/ambari-metrics-grafana/var/log/ambari-metrics-collector/var/log/ambari-metrics-monitor/var/log/ambari-metrics-grafana/etc/ambari-metrics-collector/etc/ambari-metrics-monitor/etc/ambari-metrics-grafana# other/var/tmp/oozie Clean Repository1yum clean all 在重装之前一定要移除干净。 各个服务的目录位置如下: 1234567/etc/&lt;service_name&gt;/usr/lib/&lt;service_name&gt;/var/lib/&lt;service_name&gt;/var/log/&lt;service_name&gt;/var/run/&lt;service_name&gt;/var/tmp/&lt;service_name&gt;/tmp/&lt;service_name&gt; 参考资料来自： Uninstalling and Cleaning a HDP Node]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[dnf与yum]]></title>
      <url>%2F2016%2F10%2F05%2Fdnf-vs-yum%2F</url>
      <content type="text"><![CDATA[Fedora 24使用yum命令时标记为过期，推荐使用dnf安装。想了解一下Fedora为什么要从yum转移到dnf。大致有如下几个原因： Dependency resolution of YUM is a nightmare(包依赖解析简直是噩梦–不觉得啊) YUM don’t have a documented API(没有API文档-很稀奇吗，没文档才正常吧) No support for extensions other than Python. Lower memory reduction and less automatic synchronization of metadata – a time taking process. DNF包管理器克服了YUM包管理器的一些瓶颈，提升了包括用户体验，内存占用，依赖分析，运行速度等多方面的内容。DNF使用 RPM, libsolv 和 hawkey库进行包管理操作。DNF从Yum分支出来，使用专注于性能的C语言库hawkey进行依赖关系解析工作，大幅度提升包管理操作效率并降低内存消耗。Yum不能“Python 3 as default”，而DNF支持Python 2和Python 3。（Python 3分支自2008年发布以来积极开发了五年，已经成熟和稳定，而目前仍在维护的Python 2分支不增加新特性，只接受bug和安全修正，它最早的版本是在2000年发布的。） 参考资料来自： DNF – The Next Generation Package Management Utility for RPM Based Distributions]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Ambari与HDP离线安装]]></title>
      <url>%2F2016%2F10%2F05%2Fambari-offline-install%2F</url>
      <content type="text"><![CDATA[Ambari安装包大概有400MB，HDP所包含的所有的服务(HDFS/ZooKeeper/Kafka/Flume)大概在7GB+，采用yum安装时速度在10KB以内，简直是让人绝望的速度。所以采用离线安装，为了速度，掏出无耻的迅雷(wget下载是假的，不过可以试一试，速度不理想换迅雷)，搭建本地YUM服务。 下载包如果不知道应该下载哪个版本，可以到Repository的配置文件中查看当前版本。路径为：/etc/yum.repo.d/，查看文件ambari.repo、HDP.rep和HDP-UTILS.repo即可。 12345#下载HDP-UTILS包wget http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.20/repos/centos7/HDP-UTILS-1.1.0.20-centos7.tar.gz#下载HDP包wget http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.4.3.0/HDP-2.4.3.0-centos7-rpm.tar.gz HDP-UTILS-1.1.0.20-centos6.tar.gz包有600多MB，HDP-2.4.3.0-centos6-rpm.tar.gz包有9GB之巨。包含很多服务，瞬间理解安装的时候超时是怎么回事了。下载完毕之后将包Copy到服务器上： 12scp HDP-UTILS-1.1.0.20-centos6.tar.gzip root@192.168.24.226:/data/sourcescp ambari-2.2.2.0-centos7.tar.gz root@192.168.24.226:/data/source 解压1tar -xvf HDP-UTILS-1.1.0.20-centos6.tar.gzip -C /data/source/ 切换到目录(/data/source)下,使用Python搭建一个简单的服务器： 1python -mSimpleHTTPServer &amp; 解压之后，会有HDP，HDP-UTILS-1.1.0.20的目录生成。HDP目录：包含Hadoop的生态圈的组件，比如hdfs，hive，hbase，mahout等。HDP-UTILS-1.1.0.17目录：包含HDP平台所包含的工具组件等，比如nagios，ganglia，puppet等。 修改Repo切换到Repository配置目录下，编辑文件ambari.repo，将源地址修改为本地地址，yum将从本地地址下载安装包进行安装。 123456789#VERSION_NUMBER=2.2.2.0-460[Updates-ambari-2.2.2.0]name=ambari-2.2.2.0 - Updatesbaseurl=http://192.168.24.226:8000/AMBARI-2.2.2.0/centos7/2.2.2.0-460/gpgcheck=1gpgkey=http://192.168.24.226:8000/AMBARI-2.2.2.0/centos7/2.2.2.0-460/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1 安装Ambari Server1234#安装Ambari Serveryum install ambari-server -y#启动Ambari Serverambari-server start 下载速度40MB/s，与6.7KB/s的速度对比，幸福感油然而生。 配置Ambari Server输入如下命令配置Ambari Server： 1ambari-server setup 数据库这里使用默认的内嵌数据库PostgreSQL。 启动Ambari12#启动Ambari Serverambari-server start 注： etc目录解释：Host-specific system-wide configuration files There has been controversy over the meaning of the name itself. In early versions of the UNIX Implementation Document from Bell labs, /etc is referred to as the etcetera(n. 等等；附加物；附加的人；以及其它) directory, as this directory historically held everything that did not belong elsewhere (however, the FHS restricts /etc to static configuration files and may not contain binaries). Since the publication of early documentation, the directory name has been re-designated in various ways. Recent interpretations include backronyms such as “Editable Text Configuration” or “Extended Tool Chest”.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Fedora 20 安装NodeJs]]></title>
      <url>%2F2016%2F10%2F05%2Ffedora-20-install-nodejs%2F</url>
      <content type="text"><![CDATA[安装环境： Fedora 20 i386 nodejs 6.7.0 安装使用Hexo写博客需要安装NodeJs,更新系统 1yum update -y 安装GCC编译环境 1yum install g++ curl openssl openssl-devel make gcc-c++ glibc-devel -y 下载NodeJS 1234mkdir /root/temp ; cd /rootwget http://nodejs.org/dist/node-latest.tar.gztar -xvpzf node-latest.tar.gzcd node-v* 编译安装,编译安装的时间较长，需要耐心等待，编译大概在15分钟左右。 12./configuremake install 安装NPM 1curl http://npmjs.org/install.sh | sh 查看安装的NodeJs版本： 1node --version 常见问题/usr/bin/env: ‘python’: No such file or directory1dnf install python -y g++: Command not found1dnf install "gcc-c++.x86_64" -y]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[删除Apache Ambari服务]]></title>
      <url>%2F2016%2F10%2F04%2Fdelete-ambari-service%2F</url>
      <content type="text"><![CDATA[删除删除Apache Ambari服务可调用Apache Ambari REST接口： 12345#移除ZooKeeper服务curl -u admin:admin -H "X-Requested-By: ambari" -X DELETE http://192.168.24.226:8080/api/v1/clusters/zwlbs/services/ZooKeeper#移除MapReduce服务curl -u admin:admin -H "X-Requested-By: ambari" -X DELETE http://192.168.24.226:8080/api/v1/clusters/zwlbs/services/ZooKeeper/MAPREDUCE2 注意服务的名称要大写。有时删除服务时会提示如下： 1234&#123; &quot;status&quot; : 500, &quot;message&quot; : &quot;org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: Cannot remove ZOOKEEPER. Desired state STARTED is not removable. Service must be stopped or disabled.&quot;&#125; 此时可以先改变服务状态再执行删除命令。改变服务状态命令为： 1234curl -i -H "X-Requested-By: ambari" -u admin:admin -X PUT -d '&#123;"RequestInfo":&#123;"context":"Stop Service"&#125;,"Body":&#123;"ServiceInfo":&#123;"state":"INSTALLED"&#125;&#125;&#125;' http://192.168.24.226:8080/api/v1/clusters/CLUSTER_NAME/services/SERVICE_NAME#修改ZooKeeper的状态curl -i -H "X-Requested-By: ambari" -u admin:admin -X PUT -d '&#123;"RequestInfo":&#123;"context":"Stop Service"&#125;,"Body":&#123;"ServiceInfo":&#123;"state":"INSTALLED"&#125;&#125;&#125;' http://192.168.24.226:8080/api/v1/clusters/zwlbs/services/ZOOKEEPER]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Fedora 20安装中文输入法]]></title>
      <url>%2F2016%2F10%2F03%2Ffedora-20-install-input-method%2F</url>
      <content type="text"><![CDATA[安装12yum install scim -yyum install scim-pinyin -y SCIM(Smart Common Input Method)是基于GTK引擎，为GNOME/GTK环境下非英文/ASCII字符提供的输入。SCIM is a GTK-based input method engine for inputting non-English / non-ASCII characters in a GNOME/GTK environment. There is a KDE frontend called skim.它本身自带拼音、内码等输入法，同时提供简单的程序接口，方便程序员便捷的对其进行扩充。 配置在系统配置中添加安装的输入法， 选择输入法生效。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[install-chrome-in-fedora]]></title>
      <url>%2F2016%2F10%2F02%2Finstall-chrome-in-fedora%2F</url>
      <content type="text"><![CDATA[一种方法是通过yum安装。yum（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及SUSE中的Shell前端软件包管理器。基於RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。yum提供了查找、安装、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。 创建Repo在目录/etc/yum.repos.d下新建google-chrome.repo文件。 1vi /etc/yum.repos.d/google-chrome.repo 32位操作系统写入如下代码： 123456[google-chrome]name=google-chrome - 32-bitbaseurl=http://dl.google.com/linux/chrome/rpm/stable/i386enabled=1gpgcheck=1gpgkey=https://dl-ssl.google.com/linux/linux_signing_key.pub 64位操作系统写入如下代码： 123456[google-chrome]name=google-chrome - 64-bitbaseurl=http://dl.google.com/linux/chrome/rpm/stable/x86_64enabled=1gpgcheck=1gpgkey=https://dl-ssl.google.com/linux/linux_signing_key.pub 安装1yum install google-chrome-stable 也可以直接下载rpm包进行安装。 1rpm -ivh google-chrome-stable_current_i386.rpm 提示错误如下： 1234warning: google-chrome-stable_current_i386.rpm: Header V4 DSA/SHA1 Signature, key ID 7fac5991: NOKEYerror: Failed dependencies: lsb &gt;= 4.0 is needed by google-chrome-stable-19.0.1084.56-140965.i386 libXss.so.1 is needed by google-chrome-stable-19.0.1084.56-140965.i386 安装依赖包。 1234yum install redhat-lsb -yyum install wget -yyum install libXScrnSaver -yyum install libgcrypt.so.11 12Transaction check error: file /usr/bin from install of google-chrome-stable-19.0.1084.56-140965.i386 conflicts with file from package filesystem-3.2-19.fc20.i686]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Apache-Ambari 2.2.2.0 Agent手动安装]]></title>
      <url>%2F2016%2F10%2F02%2Fapache-ambari-using%2F</url>
      <content type="text"><![CDATA[Apache-Ambari 2.2.2.0 CentOS 7.0 Ambari是一款用于部署、管理、监控Hadoop集群的开源工具，通过Ambari用户可以更方便地管理大规模Hadoop集群。Ambari架构采用的是Server/Client的模式，主要由两部分组成：ambari-agent和ambari-server。ambari依赖其它已经成熟的工具，例如其ambari-server就依赖python，而ambari-agent还同时依赖ruby, puppet，facter等工具，还有它也依赖一些监控工具nagios和ganglia用于监控集群状况。目前能找到2种，一种是Apache Ambari，一种是Hortonworks Ambari，两者区别不大，这里安装的是Apache Ambari。 下载Ambari repository。 123cd /etc/yum.repos.d/#(Redhat/CentOS/Oracle) 6:http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.2.2.0/ambari.repowget &lt;ambari-repo-url&gt; 安装ambari-agent。 1yum install ambari-agent -y 启动Agent： 1ambari-agent start 查看Agent运行状态： 1ambari-agent status 至此，Ambari Agent安装完毕。Apache Agent的日志在目录/var/log/ambari-agent/下，配置文件是/etc/ambari-agent/conf/ambari-agent.ini。 清除停止Ambari Agent： 1ambari-agent status 运行HostCleanup.py脚本123python /usr/lib/python2.6/site-packages/ambari_agent/HostCleanup.py \--silent --skip=users \-o /tmp/cleanup.log 移除 Ambari RPM、目录和符号链接在每个 Ambari 节点上，运行以下命令： 1yum erase -y ambari-* 在Ambari服务器节点上，运行以下命令：Bash1rm -rf /usr/lib/ambari-server 在每个 Ambari 代理程序节点上，运行以下命令： 1rm -rf /usr/lib/python2.6/site-packages/ambari_agent 可使用以下代码移除已损坏链接：1rm -rvf /usr/lib/python2.6/site-packages/ambari* /usr/lib/python2.6/site-packages/resource-management]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hadoop 2.7.1集群部署(不断更新...)]]></title>
      <url>%2F2016%2F10%2F01%2Fhadoop-cluster-deploy%2F</url>
      <content type="text"><![CDATA[所用软件版本： Java 1.8.0_60 Hadoop 2.7.1.2.4.3.0-227 HBase集群建立在hadoop集群基础之上，所以在搭建HBase集群之前需要把Hadoop集群搭建起来，并且要考虑二者的兼容性。 下载JDK8u60安装包，输入如下命令进行安装： 1rpm -ivh jdk-8u60-linux-x64.rpm 添加环境变量说到可以将 Hadoop 安装目录加入 PATH 变量中，这样就可以在任意目录中直接使用 hadoo、hdfs 等命令了，如果还没有配置的，需要在 Master 节点上进行配置。首先执行 vim ~/.bashrc，加入一行： 1export PATH=$PATH:/usr/hdp/2.4.3.0-227/hadoop/bin 保存后执行source ~/.bashrc使配置生效。 配置集群/分布式环境core-site.xml12 hdfs-site.xml12 mapred-site.xml12 yarn-site.xml12 启动Hadoop关闭防火墙1234#CentOS 6.x关闭防火墙服务service iptables stop#CentOS 7，需通过如下命令关闭systemctl stop firewalld.service 启动服务(严格按照顺序)启动Zookeeper服务ZooKeeper是一个分布式开源框架，提供了协调分布式应用的基本服务，它向外部应用暴露一组通用服务——分布式同步（Distributed Synchronization）、命名服务（Naming Service）、集群维护（Group Maintenance）等，简化分布式应用协调及其管理的难度，提供高性能的分布式服务。ZooKeeper本身可以以Standalone模式安装运行，不过它的长处在于通过分布式ZooKeeper集群（一个Leader，多个Follower），基于一定的策略来保证ZooKeeper集群的稳定性和可用性，从而实现分布式应用的可靠性。如下命令启动Zookeeper服务： 1./zkServer.sh start 查看服务状态： 1./zkServer.sh status 也可以用如下命令查看： 1jps | grep Quorum The jps command lists the instrumented Java HotSpot VMs on the target system. The command is limited to reporting information on JVMs for which it has the access permissions.如果服务启动失败或者遇到问题，可到相应目录查看启动日志,日志的配置在zookeeper-env.sh文件中。 1export ZOO_LOG_DIR=/var/log/zookeeper 启动Hadoop守护进程使用如下命令启动NameNode： 1./hadoop-daemon.sh start namenode 启动Hadoop启动Hadoop集群需要启动HDFS集群和Map/Reduce集群。第一次启动先初始化namenode: 12#格式化一个新的分布式文件系统hadoop namenode -format 启动HDFS命令shell脚本在hadoop的sbin目录下。 12#启动主NameNode、DataNode./start-dfs.sh 启动YARN为从根本上解决旧MapReduce框架的性能瓶颈，促进Hadoop框架的更长远发展，从0.23.0版本开始，Hadoop的MapReduce框架完全重构，发生了根本的变化。新的Hadoop MapReduce框架命名为MapReduceV2或者叫Yarn(Yet Another Resource Negotiator，另一种资源协调者)。 1./start-yarn.sh yarn会启动ResourceManager，此处需要注意的是：Namenode和ResourceManger如果不是同一台机器，不能在NameNode上启动 yarn，应该在ResouceManager所在的机器上启动yarn。 查看集群运行状态查看集群状态验证集群是否已经成功部署。输入jps命令,输出如下所示即代表相应的服务部署OK： 1234563884 Jps1776 ResourceManager1613 SecondaryNameNode1872 NodeManager1467 DataNode1377 NameNode 参考： Hadoop集群安装配置教程 Hadoop 2.7.2安装 Hadoop新MapReduce框架Yarn详解]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hadoop集群SSH免密钥登录]]></title>
      <url>%2F2016%2F09%2F30%2Fcluster-ssh-login%2F</url>
      <content type="text"><![CDATA[免密登录原理SSH之所以能够保证安全，原因在于它采用了公钥加密。过程如下： 远程主机收到用户的登录请求，把自己的公钥发给用户； 用户使用这个公钥，将登录密码加密后，发送回来； 远程主机用自己的私钥，解密登录密码，如果密码正确，就同意用户登录。 配置SSH免密登陆首先，运行ssh localhost来产生/home/用户名/.ssh目录，然后执行下面命令。 1ssh-keygen -t rsa 生成RSA公钥和私钥。将生成的”id_rsa.pub”追加（这里切记是追加，不是覆盖）到授权的key里面去。这样的效果是实现了当前用户无密SSH登陆到自己： 12#将id_rsa.pub追加到authorized_keyscat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 如果要实现无密登陆到其它的主机，只需将生成的”id_rsa.pub”追加到其它主机的”~/.ssh/authorized_keys”中去。这里我们使用的方法是先将本机的”~/.ssh/id_rsa.pub”拷贝到你想无密登陆的主机上，再在相应的主机上使用”cat”命令将”~/.ssh/id_rsa.pub”追加到该主机的 “~/.ssh/authorized_keys”中。 1scp id_rsa.pub root@192.168.24.136:/tmp 验证当再使用如下命令： 1scp id_rsa.pub root@192.168.24.136:/tmp 从A服务器向192.168.24.136服务器拷贝时，不再提示输入密码时，则说明A服务器到192.168.24.136服务器可以免密钥登录,说明A服务器到192.168.24.136服务器的免密钥登录配置OK。 来自： SSH无密码验证配置]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hashcode和equals的理解]]></title>
      <url>%2F2016%2F09%2F28%2Fhashcode-and-equals%2F</url>
      <content type="text"><![CDATA[hashcode作用hashcode提供了一种更加高效的寻找方式，在Set集合中的元素是无序不可重复的，要保证不重复，一种方式是在添加新数据时，逐一比较集合中已经存在的所有元素，如果当前集合的元素增多，效率是非常低的。hashcode可以解决这个问题，当向一个集合中添加某个元素，集合会首先调用hashCode方法，这样就可以直接定位它所存储的位置，若该处没有其他元素，则直接保存。若该处已经有元素存在，就调用equals方法来匹配这两个元素是否相同，相同则不存，不同则进行其他处理，比如散列到其他位置或者以链表的形式存储到当前已经存放的元素的尾部。hashcode此时作用是快速寻找处当前元素在集合中的位置，hashCode可以将集合分成若干个区域，每个对象都可以计算出他们的hash码，可以将hash码分组，每个分组对应着某个存储区域，根据一个对象的hash码就可以确定该对象所存储区域，这样就大大减少查询匹配元素的数量，提高了查询效率。 hashCode与equals在Java中hashCode的实现总是伴随着equals，他们是紧密配合的，你要是自己设计了其中一个，就要设计另外一个。当然在多数情况下，这两个方法是不用我们考虑的，直接使用默认方法就可以帮助我们解决很多问题。但是在有些情况，我们必须要自己动手来实现它，才能确保程序更好的运作。 对于equals，我们必须遵循如下规则： 对称性：如果x.equals(y)返回是“true”，那么y.equals(x)也应该返回是“true”。 反射性：x.equals(x)必须返回是“true”。 类推性：如果x.equals(y)返回是“true”，而且y.equals(z)返回是“true”，那么z.equals(x)也应该返回是“true”。 一致性：如果x.equals(y)返回是“true”，只要x和y内容一直不变，不管你重复x.equals(y)多少次，返回都是“true”。 任何情况下，x.equals(null)，永远返回是“false”；x.equals(和x不同类型的对象)永远返回是“false”。 对于hashCode，我们应该遵循如下规则： 在一个应用程序执行期间，如果一个对象的equals方法做比较所用到的信息没有被修改的话，则对该对象调用hashCode方法多次，它必须始终如一地返回同一个整数。 如果两个对象根据equals(Object o)方法是相等的，则调用这两个对象中任一对象的hashCode方法必须产生相同的整数结果。 如果两个对象根据equals(Object o)方法是不相等的，则调用这两个对象中任一个对象的hashCode方法，不要求产生不同的整数结果。但如果能不同，则可能提高散列表的性能。 至于两者之间的关联关系，我们只需要记住如下即可： 如果x.equals(y)返回“true”，那么x和y的hashCode()必须相等。 如果x.equals(y)返回“false”，那么x和y的hashCode()有可能相等，也有可能不等。 判断对象相等的整个处理流程是： 1、判断两个对象的hashcode是否相等，若不等，则认为两个对象不等，完毕，若相等，则比较equals。 2、若两个对象的equals不等，则可以认为两个对象不等，否则认为他们相等。 参考： Java提高篇（二六）—–hashCode]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring step by step(二)--持久化]]></title>
      <url>%2F2016%2F09%2F27%2Fspring-mybatis-integrete%2F</url>
      <content type="text"><![CDATA[环境： Windows 7 X64 Ultimate JDK 1.8 Spring-Framework 4.2.3.RELEASE MyBatis 3.3.1 Eclipse Java EE IDE for Web Developers.Version: Neon Release (4.6.0)Build id:20160613-1800 Maven 3.3.9 MySQL 5.7 MyBatis简介MyBatis是一个可以自定义SQL、存储过程和高级映射的持久层框架。MyBatis摒除了大部分的JDBC代码、手工设置参数和结果集重获。MyBatis只使用简单的XML和注解来配置和映射基本数据类型、Map 接口和POJO(Plain Ordinary Java Object)到数据库记录。相对Hibernate和Apache OJB等“一站式”ORM(Object Relational Mapping)解决方案而言，Mybatis是一种“半自动化”的ORM实现。 引入MyBatis依赖包在Maven项目的POM.xml中定义MyBatis和Spring相关包的版本。 123456&lt;properties&gt; &lt;mybatis.version&gt;3.3.1&lt;/mybatis.version&gt; &lt;mybatis.spring.version&gt;1.2.4&lt;/mybatis.spring.version&gt; &lt;spring.version&gt;4.2.3.RELEASE&lt;/spring.version&gt; &lt;druid-version&gt;1.0.26&lt;/druid-version&gt;&lt;/properties&gt; 引入Spring依赖包在Maven项目的POM.xml中引入Spring相关的依赖包。 123456789101112131415161718192021&lt;!-- Spring --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt; 引入MySQL链接包1234567891011&lt;!-- mysql --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.30&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;$&#123;druid-version&#125;&lt;/version&gt;&lt;/dependency&gt; Java常用的数据库连接池有DBCP、C3P0、Proxool、JBoss，此处数据库连接采用Alibaba的Druid框架，Druid是Java语言中较好的数据库连接池。Druid能够提供强大的监控和扩展功能。更加详细信息科参考阿里巴巴开源项目Druid负责人温少访谈。 引入AOP包12345678910&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjrt&lt;/artifactId&gt; &lt;version&gt;1.7.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.7.2&lt;/version&gt;&lt;/dependency&gt; Spring基础配置12345678910111213141516171819202122232425262728&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:context="http://www.springframework.org/schema/context" xmlns:util="http://www.springframework.org/schema/util" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd"&gt; &lt;description&gt;spring主配置文件&lt;/description&gt; &lt;util:properties id="applicationProperties" location="classpath:application.properties" /&gt; &lt;context:property-placeholder properties-ref="applicationProperties" ignore-resource-not-found="true" /&gt; &lt;context:component-scan base-package="dolphin.service.*"&gt; &lt;/context:component-scan&gt; &lt;!--aop 注解风格支持 proxy-targer-class默认false,用jdk动态代理,true是cglib .expose-proxy当前代理是否为可暴露状态,值是"ture",则为可访问。 --&gt; &lt;aop:aspectj-autoproxy expose-proxy="true" proxy-target-class="true" /&gt; &lt;aop:config expose-proxy="true" proxy-target-class="true" /&gt; &lt;import resource="classpath:spring-config/spring-mybatis.xml" /&gt; &lt;import resource="classpath:spring-config/spring-datasource.xml" /&gt;&lt;/beans&gt; Spring 自 2.0 版本开始，陆续引入了一些注解用于简化 Spring 的开发。@Repository 注解便属于最先引入的一批，它用于将数据访问层 (DAO 层 ) 的类标识为 Spring Bean。具体只需将该注解标注在 DAO 类上即可。同时，为了让 Spring 能够扫描类路径中的类并识别出 @Repository 注解，需要在 XML 配置文件中启用 Bean 的自动扫描功能，这可以通过context:component-scan实现。context:component-scan定义了相关Bean的扫描路径。就不再需要在 XML 中显式使用 进行 Bean 的配置。Spring 在容器初始化时将自动扫描 base-package 指定的包及其子包下的所有 class 文件，所有标注了 @Repository 的类都将被注册为 Spring Bean。 新建Spring-Mybatis配置文件1234567891011121314151617181920&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:tx="http://www.springframework.org/schema/tx" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd"&gt; &lt;description&gt;Spring 整合 MyBatis&lt;/description&gt; &lt;!-- 在基本的MyBatis中，session 工厂可以使用SqlSessionFactoryBuilder.来创建。在MyBatis-Spring中，使用了SqlSessionFactoryBean来替代。 --&gt; &lt;bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"&gt; &lt;property name="dataSource" ref="dataSource" /&gt; &lt;!-- 指定sqlMapConfig总配置文件，订制的environment在spring容器中不在生效 --&gt; &lt;property name="configLocation" value="classpath:mybatis-config.xml" /&gt; &lt;/bean&gt; &lt;!-- Mybatis 映射文件路径 用逗号隔开 --&gt; &lt;bean class="org.mybatis.spring.mapper.MapperScannerConfigurer"&gt; &lt;property name="basePackage" value="dolphin.dao" /&gt; &lt;property name="sqlSessionFactoryBeanName" value="sqlSessionFactory" /&gt; &lt;/bean&gt;&lt;/beans&gt; 新建MyBatis配置文件123456789101112131415161718192021222324252627&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.1//EN""http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;configuration&gt; &lt;!--设置--&gt; &lt;settings&gt; &lt;!-- 全局映射器启用缓存，不建议使用mybatis自己的缓存--&gt; &lt;setting name="cacheEnabled" value="false" /&gt; &lt;!-- 查询时，关闭关联对象即时加载以提高性能 --&gt; &lt;setting name="lazyLoadingEnabled" value="true" /&gt; &lt;!-- 设置关联对象加载的形态，此处为按需加载字段(加载字段由SQL指定)，不会加载关联表的所有字段，以提高性能 --&gt; &lt;setting name="aggressiveLazyLoading" value="false" /&gt; &lt;!-- 对于未知的SQL查询，允许返回不同的结果集以达到通用的效果 --&gt; &lt;setting name="multipleResultSetsEnabled" value="true" /&gt; &lt;!-- 允许使用列标签代替列名 --&gt; &lt;setting name="useColumnLabel" value="true" /&gt; &lt;!-- 允许使用自定义的主键值(比如由程序生成的UUID 32位编码作为键值)，数据表的PK生成策略将被覆盖 --&gt; &lt;setting name="useGeneratedKeys" value="true" /&gt; &lt;!-- 给予被嵌套的resultMap以字段-属性的映射支持 --&gt; &lt;setting name="autoMappingBehavior" value="FULL" /&gt; &lt;!-- 对于批量更新操作缓存SQL以提高性能 但是返回id有问题--&gt; &lt;setting name="defaultExecutorType" value="SIMPLE" /&gt; &lt;!-- 数据库超过36000秒仍未响应则超时 --&gt; &lt;setting name="defaultStatementTimeout" value="36000" /&gt; &lt;setting name="mapUnderscoreToCamelCase" value="true"/&gt; &lt;/settings&gt;&lt;/configuration&gt; 数据库链接配置1234567#=========================================# DataSource#=========================================jdbc.driverClass = com.mysql.jdbc.Driverjdbc.url = jdbc:mysql://127.0.0.1:3306/testjdbc.username = rootjdbc.password =123456 新建POJO12345678910111213141516171819202122public class Country &#123; private Integer id; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; private String countryName; public String getCountryName() &#123; return countryName; &#125; public void setCountryName(String countryName) &#123; this.countryName = countryName; &#125;&#125; 新建DAO123public interface CountryDao &#123; Country getCountry(int id);&#125; 新建Mapper123456789101112&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.3//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;mapper namespace="dolphin.dao.CountryDao"&gt; &lt;select id="getCountry" parameterType="int" resultType="dolphin.mode.Country"&gt; SELECT * FROM country WHERE id = #&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; 新建Service123public interface CountryService &#123; Country getCountyById(int id);&#125; 实现接口123456789@Servicepublic class CountryServiceImpl implements CountryService&#123; @Autowired private CountryDao countryDao; public Country getCountyById(int id) &#123; return countryDao.getCountry(id); &#125;&#125; 新建测试类1234@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration("classpath:spring-config/spring-base.xml")public class BasicTest &#123;&#125; 12345678910111213141516public class PageMapperTest extends BasicTest &#123; @Autowired private CountryService countryService; @Test public void test() &#123; try &#123; Country country = countryService.getCountyById(1); System.out.print(country.getId()); &#125; catch (Exception e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; 新建数据库表12345678910111213141516USE `test`;## Table structure for table country#DROP TABLE IF EXISTS `country`;CREATE TABLE `country` ( `Id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键', `countryname` varchar(255) DEFAULT NULL COMMENT '名称', `countrycode` varchar(255) DEFAULT NULL COMMENT '代码', PRIMARY KEY (`Id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='国家信息';INSERT INTO `country` VALUES (1,'Angola','AO');INSERT INTO `country` VALUES (2,'Afghanistan','AF'); 启动测试结果如下如所示。 引用文章： 详解 Spring 3.0 基于 Annotation 的依赖注入实现]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[IntelliJ Idea快捷键技巧]]></title>
      <url>%2F2016%2F09%2F26%2Fintllij-idea-encoding%2F</url>
      <content type="text"><![CDATA[输入乱码在控制台输入中文时显示的不是输入的汉字，而是显示正方形符号，此时输入的汉字显示乱码，如图所示。 解决此问题，可使用支持中文较好的字体，覆盖默认字体，在Intellig Idea中的File-Settings-Appearance&amp;Behavior-Appearance中(Ctrl + Alt + S)。如下图所示。 在Debug控制台中Tomcat中输出乱码，可以打开Run/Debug Configuration,选择你的tomcat。在Server &gt; VM options设置为-Dfile.encoding=UTF-8 ，重启tomcat，其中参数-D代表Default，设置默认的文件编码是UTF-8。 智能提示敏感调整在智能提示时默认大小写敏感，比如String的智能提示就必须首字母大写才能够准确定位，经常大小写切换，不是特别方便。在settings-Editor-General-Code Completion里设置后，可以省去大小写切换操作。 Live TemplatesLive templates帮助你快速、高效、精准的输入经常使用的或者自定义的代码片段(Live templates let you insert frequently-used or custom code constructs into your source code file quickly, efficiently, and accurately).相当于一个代码块的缩写，输入Ctrl + J会显示当前上下文中所有可用的模板。快速输入foreach，在方法内(一定要在方法里敲)输入iter + TAB。 快速进行空判断输入ifn，按TAB即可。 123if (args == null) &#123;&#125; 快速输入Main方法输入psvm,按TAB键即可。 123public static void main(String[] args) &#123;&#125; Surround with用于快速添加try catch等代码块，The Surround with feature allows you easily put expressions or statements within blocks or language constructs.快捷键为：Ctrl + Alt + T。 重构(Reflactor)重命名(Rename)Shift + F6,可以重命名你的类、方法、变量等等。 主题(Theme)更换默认主题在settings(Ctrl + Alt + S) –&gt; editor –&gt; colors &amp; fonts –&gt; general，如下图所示，如果需要修改主题的字体等是默认是不允许的，需要复制一个新的主题，再自定义修改相应的参数。 Maven配置使用Maven最头疼的就是从中央库下载jar包超级慢，等待下载jar包的感觉用重庆话讲：肚肠子把把逗紧了。解决办法就是使用私服地址，私服可以使用本地配置的Maven(不是Intellij Idea里面的boundle Maven)，修改Intellj Maven采用本机安装Maven在设置(Ctrl + Alt + S)中输入Maven，修改地址(例如：D:\Source\zwnewplatform\javasoftware\runtime\apache-maven-3.3.9)即可，如下图所示。 常用快捷键Intellij Idea可以完全丢掉键盘工作，有点早期Vim和Emacs编辑器的味道。善用Intellij Idea编辑器可以大幅提高工作效率，用快捷键有一种装逼的感觉，装逼人士必备。 快捷键(Short Keys) 作用 Ctrl + Shift + F12 编辑区全屏 Ctrl + F4 关闭当前编辑文件 Alt + 1 打开项目树视图,并将光标定位到当前编辑文件在项目树上的位置 Alt + Home 定位到导航条 Ctrl + Shift + N 定位到某一个文件(定位到指定文件) Shift + Esc 隐藏底部(如：Debug、TODO等)窗口 F12 把焦点从编辑器移到最近使用的工具窗口，可用于显示底部(如：Debug、TODO等)对应窗口 Alt + 5 激活Debug窗口 Alt + 7 打开Structure窗口(Structure前面有一个带有下划线的5是快捷键提示) Ctrl + N 根据输入的类名查找类文件 Ctrl + Shift + N 查找文件 Alt + Insert 生成Getter、Setter Ctrl + F12 在当前编辑的文件中快速导航 Alt + F3(Search/Incremental Search ) 在编辑器中实现快速查查找功能 Ctrl + J 如果记不住Live Template的缩写，使用此快捷键可以弹出所有Live Template的缩写 Ctrl + X 删除行，剪切(Cut) Ctrl + Alt + F12 显示当前项目树结构文件/文件夹所在目录，可以快速打开文件夹 Ctrl + Shift + T(Test) 选中类名，按下快捷键,创建一个新的测试Case Ctrl + Shift + 数字键(NumPad)+ 展开所有 Ctrl + Shift + 数字键(NumPad)- 折叠所有 Ctrl + 数字键(NumPad)- 在项目树上应用此快捷键可以折叠所有展开的文件夹 Ctrl + F12 打开Intellij中嵌入的终端(Terminal) Ctrl + B 跳转到实现处 Ctrl + F8 设置断点和取消设置断点 Alt + Up/Down 跳转到下一个方法或者属性 Ctrl + Tab 编辑窗口切换(切换Debug视图、编辑区类文件等) Shift + F2/F2 跳到上/下一个错误处 Alt + F1 选中目标，可以定位到文件等各种对象，Alt + F1弹出的界面中选择需要定位的对象，也可以理解成对象导航 Shift + F6 重构、重命名 Ctrl + Shift + F10 运行测试，注意运行测试时界面需要切换到测试类的界面 Alt + F8 计算变量值 Alt + Left/Right 按左/右方向切换当前已打开的文件视图 Ctrl + F10 更新资源和类文件，热部署(需要配置，部署时选择Exploded模式) Ctrl + Shift + T 新建测试类，在测试类与被测试类之间跳转 Ctrl + Shift +F 全文查找，类似于Eclipse的Ctrl + H Ctrl + Shift + Alt + N 全文搜索，包含Maven引用的jar包里面的内容，在搜索界面中，需要勾选Include non-project symbols Ctrl + Alt + Left/Right 到上一次/下一次编辑的位置 Ctrl + Y 删除光标所在行或删除选中的行 Ctrl + P(Parameter) 方法参数提示显示,当调用方法时未出现参数的智能提示时，可以手动显示方法的提示 Ctrl + Q 光标所在的变量/类名/方法名等上面（也可以在提示补充的时候按），显示文档内容 Ctrl + Shift + Up/Down 代码行上下移动 Ctrl + Shift + U 变量转换为大写 Ctrl + W 选中光标所在位置的单词(Words),递进式选择代码块。可选中光标所在的单词或段落，连续按会在原有选中的基础上再扩展选中范围 Ctrl + Alt + O 优化(Optimization)类的Import Ctrl + H 显示类的层次结构(Hierarchy) Alt + F12 激活Terminal Ctrl + Shift + F10 运行所有测试(不是特别好用) Ctrl + Delete 删除行 Ctrl + Alt + O（Optimize） 优化包导入 进入退出全屏使用快捷键Alt + V打开View菜单，移动上下键选择Enter Full Screen即可。也可以自定义快捷键。 properties显示中文使用IntelliJ Idea打开属性文件时，如果包含中文，显示为原始的编码，正确显示中文需要做如下设置，到设置(settings)中,可以使用快捷键Ctrl + Alt + S打开设置界面，找到File Encoding选项卡，选中Default encoding for properties file即可，如下图所示. 限制编辑器Tab页个数有时我们并不习惯在编辑器中打开太多的Tab页，如果不小心点开了7个以上的Tab页，在Tab页面中切换也蛮让人晕眩的，设置Tab页最大个数在Settings(Ctrl + Alt + S),Editor-General-Editor Tabs中，如下图如所示。 类文件注释通过File–》Setting–》File and Code Template中来设置类文件注释，如下图所示： 格式化设置有时在按下格式化按钮后，希望同时优化导入的包，自动移除未使用的包引用，在Intellij Idea中，可以在文件夹上右键，选择Reformate Code(Ctrl + Alt + L)，在弹出的界面中勾选Optimize Import即可。这样在每次按下格式化按钮时，会自动优化导入的包，比如自动移除未使用的包引用、自动排序等等。 参考： Java开发必装的IntelliJ IDEA插件 Surrounding Blocks of Code with Language Constructs]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring step by step(一)--依赖注入(Dependency Injection)]]></title>
      <url>%2F2016%2F09%2F25%2Fspring-ioc%2F</url>
      <content type="text"><![CDATA[为什么要依赖注入依赖注入(Dependency Injection)所要达到的目标是实现程序间的松耦合。将服务的调用者和服务的提供者分离。DI提供一种机制，在运行时绑定服务的提供者和调用者。 新建项目新建一个Web项目，目录结构如下。 引入包引入servlet-api.jar包，目录结构如下。 注意引入servlet-api.jar包后，多了src/main/java目录和src/test/java目录。servlet-api.jar包中，实现了Sevlet规范，在Apache Tomcat 8.0.37中实现的是Servlet Specification 3.1，JSP Specification 2.3。版本之间详细的对应关系可以看Apache Tomcat Versions。 添加Spring依赖包在Maven的POM文件中添加spring-context依赖。 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt;&lt;/dependency&gt; 此处虽然只添加了一个jar包，Maven会自动下载此jar包相关的依赖包。在Maven的Dependency Hierarchy中查看包与包之间的依赖关系如下图所示。 添加类123public interface MessageService &#123; String getMessage();&#125; 1234567891011121314@Componentpublic class MessagePrinter &#123; final private MessageService service; @Autowired public MessagePrinter(MessageService service) &#123; this.service = service; &#125; public void printMessage() &#123; System.out.println(this.service.getMessage()); &#125;&#125; 带有 @Configuration 的注解类表示这个类可以使用 Spring IoC 容器作为 bean 定义的来源。@Bean 注解告诉 Spring，一个带有 @Bean 的注解方法将返回一个对象，该对象应该被注册为在 Spring 应用程序上下文中的 bean。 123456789101112131415161718@Configuration@ComponentScanpublic class Application &#123; @Bean MessageService mockMessageService() &#123; return new MessageService() &#123; public String getMessage() &#123; return "Hello World!"; &#125; &#125;; &#125; public static void main(String[] args) &#123; ApplicationContext context = new AnnotationConfigApplicationContext(Application.class); MessagePrinter printer = context.getBean(MessagePrinter.class); printer.printMessage(); &#125;&#125; 上面的代码(排除Main函数)等同于: 123&lt;beans&gt; &lt;bean id="mockMessageService" class="dolphin.MessageService" /&gt;&lt;/beans&gt; 带有 @Bean 注解的方法名称作为 bean 的 ID，它创建并返回实际的 bean。配置类可以声明多个 @Bean。一旦定义了配置类，你就可以使用 AnnotationConfigApplicationContext 来加载并把他们提供给 Spring 容器(Main方法中即是)。 运行结果如图所示。 来自： spring-framework-quick-start]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Eclipse断点类型]]></title>
      <url>%2F2016%2F09%2F24%2Feclipse-breakpoint-type%2F</url>
      <content type="text"><![CDATA[本文的Eclipse版本为:Eclipse Java EE IDE for Web Developers. Version: Neon Release (4.6.0)Build id: 20160613-1800 Line BreakpointLine Breakpoint是最简单的Eclipse断点，只要双击某行代码对应的左侧栏，就对该行设置上断点。断点的颜色为一个蓝色的实心点。 Watchpoint关注某个关键变量的变化或使用。此时，就可以为该变量设置一种特殊的断点–Watchpoint。 Method Breakpoint关注程序对某个方法的调用情况，即，可以设置Method Breakpoint。 断点上的左右小箭头代表进入和退出方法时命中。 Exception Breakpoint某个特定异常发生时程序能够被中断，以方便查看当时程序所处的状态。 Class Load Breakpoint当某个类被加载时，通过该断点可以中断程序。 来自： 详解Eclipse断点]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Phoenix常用操作]]></title>
      <url>%2F2016%2F09%2F23%2Fphoenix-tutorial%2F</url>
      <content type="text"><![CDATA[简介Phoenix本意是凤凰的意思，Apache Phoenix是构建在HBase之上的关系型数据库层，作为内嵌的客户端JDBC驱动用以对HBase中的数据进行低延迟访问。Apache Phoenix会将用户编写的sql查询编译为一系列的scan操作，最终产生通用的JDBC结果集返回给客户端。数据表的元数据存储在HBase的表中被会标记版本号，所以进行查询的时候会自动选择正确的schema。直接使用HBase的API，结合协处理器（coprocessor）和自定义的过滤器的话，小范围的查询在毫秒级响应，千万数据的话响应速度为秒级。 特性Phoenix可以用SQL语句来查询Hbase，且只能查Hbase，别的类型比如查询文本文件等都不支持！如果要查文本文件等，可以使用Hive和Impala，Phoenix在Hbase上查询的性能较Hive和Impala具有优势。 常用操作登录。 1./sqlline.py localhost 查看HBase所有表。 1!tables table schema为system的是系统表。 查看HBase中表名为test的所有列。 1!columns test 表名不用加引号。 查询表数据。 123select * from test;select name from test;//查询指定列select * from test where name = 'jiangxiaoqiang';//条件查询 删除表数据。 12delete from test where condition;//语法delete from test where name is null;//例子 condition是过滤条件，注意结尾需要有分号，表示SQL语句已经结束，可以提交给引擎执行。 清除表的重复数据。 1delete from tablename where vtime in ( select vtime from tablename group by vtime having count(*)&gt;1) and id not in (select max(id) from tablename group by vtime having count(*)&gt;1 ); 清除同一时间重复的数据（同一时刻只能有1条数据）。 连接查询。 1SELECT P.id as id, vtime, A.description FROM 表P P left join 表A A on P.id=A.positional_Id WHERE P.vehicle_id='cae21196-cb66-4256-88a6-7cdfb23e2c78' and P.vtime &gt;= '2016-10-10 00:00:00' and P.vtime &lt;= '2016-10-13 23:59:59' and P.alarm is not null and P.alarm != '0' order by P.vtime; 注意在做MyBatis映射时，P.id列需要一个别名，对应定义的实体字段。 函数(Function)substr函数用于截取字符串。 1select substr(time,0,11) from test group by substr(time,0,11); time为需要截取的字符串，从第0位开始截取11位长度。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Tomcat 8.0 HTTP请求流程]]></title>
      <url>%2F2016%2F09%2F22%2Ftomcat-http%2F</url>
      <content type="text"><![CDATA[假设来自浏览器的请求为：http://localhost:8080/test/index.jsp Connector获取请求请求被发送到Apache Tomcat服务器端口8080，被在那里侦听的Coyote HTTP/1.1 Connector(org.apache.tomcat.util.net.NioEndpoint)获得。Tomcat源码中与connector相关的类位于org.apache.coyote包中，Connector分为以下几类： Http Connector 基于HTTP协议，负责建立HTTP连接。它又分为BIO Http Connector与NIO Http Connector(org.apache.tomcat.util.net.NioEndpoint)两种。BIO(blocking I/O)，顾名思义，即阻塞式I/O操作，表示Tomcat使用的是传统的Java I/O操作(即java.io包 及其子包)。一般而言，bio模式是三种运行模式中性能最低的一种。NIO为Network IO，后者提供非阻塞IO与长连接Comet支持。NIO(new I/O)，是Java SE 1.4及后续版本提供的一种新的I/O操作方式(即java.nio包及其子包)。Java nio是一个基于缓冲区、并能提供非阻塞I/O操作的Java API，因此nio也被看成是non-blocking I/O的缩写。它拥有比传统I/O操作(bio)更好的并发运行性能。 AJP Connector 基于AJP协议，AJP是专门设计用来为tomcat与http服务器之间通信专门定制的协议，能提供较高的通信速度和效率。如与Apache服务器集成时，采用这个协议。AJP(Apache JServ Protocol)协议：目前正在使用的AJP协议的版本是通过JK和JK2连接器提供支持的AJP13，它基于二进制的格式在Web服务器和Tomcat之间传输数据，而此前的版本AJP10和AJP11则使用文本格式传输数据。 APR HTTP Connector org.apache.tomcat.util.net.AprEndpoint，用C实现，通过JNI(Java Native Interface)调用的。主要提升对静态资源(如HTML、图片、CSS、JS等)的访问性能。现在这个库已独立出来可用在任何项目中。Tomcat在配置APR之后性能非常强劲。APR(Apache Portable Runtime/Apache可移植运行时)，是Apache HTTP服务器的支持库。你可以简单地理解为，Tomcat将以JNI的形式调用Apache HTTP服务器的核心动态链接库来处理文件读取或网络传输操作，从而大大地提高Tomcat对静态文件的处理性能。Tomcat apr也是在Tomcat上运行高并发应用的首选模式。与配置nio运行模式一样，也需要将对应的Connector节点的protocol属性值改为org.apache.coyote.http11.Http11AprProtocol。 Tomcat7和Tomcat8默认设置都是http1.1，Tomcat7默认使用BIO，Tomcat8根据情况自动选择BIO还是NIO，甚至NIO2.当前调试版本是Tomcat 8，默认进入的是NioEndpoint(其实这里已经到AbstractEndpoint的Processor了，在NioEndpoint的断点始终没有命中，不知何故)，如下图所示。 Engine处理请求Connector把该请求交给它所在的Service的Engine(StandardEngine)来处理，并等待Engine的回应。Container 是容器的父接口，所有子容器都必须实现这个接口，Container 容器的设计用的是典型的责任链的设计模式，它有四个子容器组件构成，分别是：Engine、Host、Context、Wrapper，这四个组件不是平行的，而是父子关系，Engine 包含 Host,Host 包含 Context，Context 包含 Wrapper。通常一个 Servlet class 对应一个 Wrapper，如果有多个 Servlet 就可以定义多个 Wrapper，如果有多个 Wrapper 就要定义一个更高的Container。详细的关系在server.xml可以看出来： 1234567891011&lt;Server&gt;&lt;!--顶层元素，代表一个服务器--&gt; &lt;Service&gt;&lt;!--顶层元素，是Connector的集合，只有一个Engine--&gt; &lt;Connectior/&gt;&lt;!--连接器类元素，代表通信接口--&gt; &lt;Engine&gt;&lt;!--容器类元素，为特定的Service组件处理所有客户请求，可包含多个Host--&gt; &lt;Host&gt;&lt;!--为特定的虚拟主机处理所有客户请求--&gt; &lt;Context&gt;&lt;!--为特定的WEB应用处理所有客户请求--&gt; &lt;/Context&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Service&gt;&lt;/Server&gt; 匹配HostEngine获得请求localhost:8080/test/index.jsp，匹配它所有虚拟主机HostEngine匹配到名为localhost的Host（即使匹配不到也把请求交给该Host处理，因为该Host被定义为该Engine的默认主机） 匹配Contextlocalhost Host获得请求/test/index.jsp，匹配它所拥有的所有ContextHost匹配到路径为/test的Context（如果匹配不到就把该请求交给路径名为””的Context去处理） 匹配Servletpath=”/test”的Context获得请求/index.jsp，在它的mapping table中寻找对应的servletContext匹配到URL PATTERN为*.jsp的servlet，对应于JspServlet类构造HttpServletRequest对象和HttpServletResponse对象，作为参数调用JspServlet的doGet或doPost方法 返回响应Context把执行完了之后的HttpServletResponse对象返回给HostHost把HttpServletResponse对象返回给EngineEngine把HttpServletResponse对象返回给ConnectorConnector把HttpServletResponse对象返回给客户browser 请求流程图(Graphiz绘制)：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在Eclipse中启动和调试Tomcat（二）]]></title>
      <url>%2F2016%2F09%2F20%2Flaunching-and-debugging-tomcat-extra%2F</url>
      <content type="text"><![CDATA[在Eclipse中启动和调试Tomcat（一）中，断点只能在自定义Servlet中命中，如果想观察详细的Tomcat运行过程，还需要从Tomcat的入口开始进行调试。详细步骤如下(未完全通过，暂勿参考)： 引入Tomcat8.0.37jar包由于Tomcat的入口类JIoEndPoint实现在tomcat-coyote.jar包中，入口org.apache.tomcat.util.net.JIoEndpoint，该类用来处理传递进来的TCP连接，它实现了一个简单的服务器模式：一个监听线程用来接收socket以及为每个进来的连接创建一个worker来处理。更加高级的功能会涉及到线程重用，如队列等。所以需要引入Tomcat8.0.37中的tomcat-coyote.jar包，在Tomcat8项目的lib目录中。 附加源码附加源码如下图所示。 调试在JIoEndPoint类中的processSocket方法上新建断点，在浏览器中请求地址即可命中断点，单步跟踪调试即可。Tomcat中支持两种协议的连接器：HTTP/1.1与AJP/1.3。HTTP/1.1协议负责建立HTTP连接，web应用通过浏览器访问tomcat服务器用的就是这个连接器，默认监听的是8080端口；AJP/1.3协议负责和其他HTTP服务器建立连接，监听的是8009端口，比如tomcat和apache或者iis集成时需要用到这个连接器。协议上有三种不同的实现方式：JIO、APR、NIO。 JIO(java.io)：用java.io纯JAVA编写的TCP模块，这是tomcat默认连接器实现方法； APR(Apache Portable Runtime)：有C语言和JAVA两种语言实现，连接Apache httpd Web服务器的类库是在C中实现的，同时用APR进行网络通信； NIO(java.nio)：这是用纯Java编写的连接器(Conector)的一种可选方法。该实现用java.nio核心Java网络类以提供非阻塞的TCP包特性。ProtocolHandler接口是对这些协议的抽象]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在Eclipse中启动和调试Tomcat（一）]]></title>
      <url>%2F2016%2F09%2F20%2Flaunching-and-debugging-tomcat%2F</url>
      <content type="text"><![CDATA[在Eclipse中调试Tomcat可以分析HTTP请求从Tomcat捕获到Servlet的过程。这个是调试Tomcat的目的。调试的环境是： IDE：Eclipse Java EE IDE for Web Developers.Version: Neon Release (4.6.0) Build id: 20160613-1800 Tomcat 8.0（8.0.37） JDK 1.8 OS：Windows 7 Ultimate x64 创建项目Tomcat8创建一个新的Java类型项目，项目名称为“Tomcat8”，如下如所示。 下载Tomcat 8.0.37二进制包下载Tomcat 8.0.37对应的二进制文件。将之拷贝到项目Tomcat8项目的根目录。拷贝之后Tomcat项目的目录结构如下图所示。 创建另一个项目test创建一个Maven的webapp项目,名字为“test”。 新建自定义Servlet在test项目下新建一个自定义Servlet,名字为“TestServlet”，如图所示： 由于TestServlet实现HttpServlet接口，HttpServlet接口的定义在servlet-api.jar包中，所以在test项目的Library中需要引用Tomcat8项目目录下的servlet-api.jar包。新建自定义Servlet完成后在Tomcat8项目中配置servlet映射(/webapps/examples/WEB-INF)： 1234&lt;servlet-mapping&gt; &lt;servlet-name&gt;test&lt;/servlet-name&gt; &lt;url-pattern&gt;/demo/test&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 以上映射说明从/demo/test发送的请求由名为test的Servlet来处理。在Tomcat8项目中配置servlet(/webapps/examples/WEB-INF)： 1234&lt;servlet&gt; &lt;servlet-name&gt;test&lt;/servlet-name&gt; &lt;servlet-class&gt;demo.TestServlet&lt;/servlet-class&gt;&lt;/servlet&gt; 以上配置指明名为test的Servlet处理的类的完整路径为：demo.TestServlet。 配置调试参数配置test项目，创建调试配置，配置Main Class为Tomcat的启动类： 1org.apache.catalina.startup.Bootstrap 配置test项目的工作空间为Tomcat8的工作空间，如图所示： 调试在自定义的Servlet的doGet方法上打断点，打开浏览器访问链接即可命中自定义Servlet。从这里可以加深理解HTTP请求到自定义Servlet处理的过程，可以理解Spring MVC和Tomcat是如何联系起来的。 源自： Launching and Debugging Tomcat from Eclipse without complex plugins]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java中String非空判断]]></title>
      <url>%2F2016%2F09%2F19%2Fjava-string-not-null%2F</url>
      <content type="text"><![CDATA[如果使用的Java运行时版本在Java SE 1.6之前，可以这样判断字符串非空： 1if(str != null &amp;&amp; str.length() != 0) 如果使用的Java运行时版本是Java SE 1.6及之后版本，可以这样判断字符串非空： 1if(str != null &amp;&amp; !str.isEmpty()) 使用org.apache.commons.lang.StringUtilsApache commons-lang来完成： 12345import org.apache.commons.lang.StringUtils;if (StringUtils.isNotBlank(str)) &#123;&#125; 如下方式也可： 12345import com.google.common.base.Strings;if (!Strings.isNullOrEmpty(myString)) &#123; return myString;&#125; 非空默认值有时在值为null时，会给一个默认值，有如下方式。 1234567String alarmType = vehicleMessageHandler.parseAlarm(kafkaRecievedLocationMessageBody.getAlarm());//使用三元运算符positional.setAlarm(StringUtils.isBlank(alarmType) ? "0" : alarmType);//com.google.common.baseObjects.firstNonNull(alarmType, "0");//java.utilOptional.ofNullable(alarmType).orElse("0"); 源自： Java, check whether a string is not null and not empty? How to convert a possible null-value to a default value using Guava?]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring MVC源码调试]]></title>
      <url>%2F2016%2F09%2F17%2Fspringmvc-sourcecode-debugging%2F</url>
      <content type="text"><![CDATA[这里调试Spring MVC的环境是： Windows 7 Eclipse Java EE IDE for Web Developers,Version: Neon Release (4.6.0).Build id: 20160613-1800 JDK 1.8 Spring MVC 4.2.3 Apache Tomcat 8.0 想了解平时学习的理论知识在实际的代码实现中是什么情况，比较好的方式是阅读源码，如果能在阅读过程中根据疑问动手调试源码验证猜想和疑问，那就更加完美了。这里想看Spring MVC一个HTTP请求从开始到结束到底是怎么运行的，Spring MVC怎么处理，选取了4.2.3版本的源码进行调试。 首先从GitHub上下载4.2.3版本的源码，在项目的Maven Dependencies找到名为spring-webmvc-4.2.3.REALEASE的jar包，单击右键build-path–&gt;Configure build path,在Libraries中找到对应的jar包，选中source attachment–&gt;Edit. 在spring-webmvc-4.2.3.REALEASE.jar中找到dispatchservlet类，打断点即可进入调试。]]></content>
    </entry>

    
  
  
</search>
